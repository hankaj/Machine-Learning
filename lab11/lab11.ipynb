{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48bde695-6504-4b53-9baf-90dac1da8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e75ac71-ac43-45b2-a501-84dfdbdcc8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00a7a4e-7158-4d1a-aa93-a1516f416a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "def build_model(n_hidden, n_neurons, optimizer, learning_rate, momentum=0):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=X_train.shape[1:]))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    elif optimizer == \"nesterov\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate, nesterov=True)\n",
    "    if optimizer == \"momentum\":\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)\n",
    "    if optimizer == \"adam\":\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"mse\", metrics=[\"mae\"], optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d53edefb-3b60-4123-8611-382d43a51edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
    "\n",
    "def get_dir(name, value):\n",
    "    ts = int(time.time())\n",
    "    run_id = f\"{ts}_{name}_{value}\"\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760c4b03-c495-410e-b120-156a95ba4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_cb = tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4a2dc6e-18fd-4e11-bf23-afea09748479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c074d9d0-dc27-486c-bb25-c4105c2bf769",
   "metadata": {},
   "source": [
    "First experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c87a6ae-54f7-45d7-8d7d-23f176a7ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 19ms/step - loss: 3401.3970 - mae: 33.4821 - val_loss: 311.0516 - val_mae: 14.4932\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 254.7935 - mae: 12.1135 - val_loss: 137.1021 - val_mae: 9.5869\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 206.4626 - mae: 11.0221 - val_loss: 111.4679 - val_mae: 8.4720\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 190.3381 - mae: 10.6453 - val_loss: 103.1058 - val_mae: 8.1271\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 181.7432 - mae: 10.3951 - val_loss: 92.0520 - val_mae: 7.5028\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 167.9646 - mae: 10.0638 - val_loss: 91.0381 - val_mae: 7.4938\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 160.2945 - mae: 9.7564 - val_loss: 86.1156 - val_mae: 7.1749\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 151.9255 - mae: 9.5025 - val_loss: 88.3485 - val_mae: 7.3268\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 150.7776 - mae: 9.3999 - val_loss: 87.0422 - val_mae: 7.4143\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 145.0160 - mae: 9.1961 - val_loss: 84.2994 - val_mae: 7.2079\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 138.8707 - mae: 8.9406 - val_loss: 70.5830 - val_mae: 6.4961\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 132.7805 - mae: 8.7084 - val_loss: 69.0946 - val_mae: 6.3470\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 133.2398 - mae: 8.5558 - val_loss: 66.8998 - val_mae: 6.3894\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 125.9727 - mae: 8.3752 - val_loss: 71.9403 - val_mae: 6.7465\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 124.4008 - mae: 8.4025 - val_loss: 64.5811 - val_mae: 6.3022\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 123.2611 - mae: 8.1595 - val_loss: 70.6555 - val_mae: 6.7305\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 114.9031 - mae: 7.9083 - val_loss: 137.5748 - val_mae: 9.4083\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 122.5685 - mae: 8.3105 - val_loss: 60.5539 - val_mae: 6.1907\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 113.0878 - mae: 7.6942 - val_loss: 57.8925 - val_mae: 5.8584\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 111.8553 - mae: 7.8361 - val_loss: 64.2592 - val_mae: 6.3630\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 111.1639 - mae: 7.7264 - val_loss: 138.4926 - val_mae: 9.4621\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 114.6501 - mae: 8.0196 - val_loss: 58.2692 - val_mae: 5.9551\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 106.9936 - mae: 7.5453 - val_loss: 75.4888 - val_mae: 6.9974\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 116.1018 - mae: 7.8289 - val_loss: 54.4399 - val_mae: 5.7547\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 108.5194 - mae: 7.6550 - val_loss: 59.5131 - val_mae: 6.0584\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 101.0658 - mae: 7.3122 - val_loss: 50.7790 - val_mae: 5.6436\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 101.2246 - mae: 7.1912 - val_loss: 60.6714 - val_mae: 6.2802\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 102.9807 - mae: 7.1726 - val_loss: 48.1730 - val_mae: 5.5107\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 101.6642 - mae: 7.3467 - val_loss: 48.2767 - val_mae: 5.5517\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 99.4551 - mae: 7.1899 - val_loss: 55.2115 - val_mae: 5.8217\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 103.4502 - mae: 7.3502 - val_loss: 46.9110 - val_mae: 5.4202\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 97.0031 - mae: 7.0112 - val_loss: 59.7139 - val_mae: 6.0803\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 102.2468 - mae: 7.2458 - val_loss: 45.9055 - val_mae: 5.3687\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 97.6612 - mae: 7.0722 - val_loss: 62.1914 - val_mae: 6.2669\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 98.9197 - mae: 7.1644 - val_loss: 47.1045 - val_mae: 5.3795\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 96.0855 - mae: 7.1472 - val_loss: 49.4812 - val_mae: 5.6463\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 96.1311 - mae: 6.9835 - val_loss: 44.1941 - val_mae: 5.3567\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 98.0588 - mae: 7.0623 - val_loss: 54.1608 - val_mae: 5.8911\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 97.8571 - mae: 7.0061 - val_loss: 56.3712 - val_mae: 5.9660\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 100.0383 - mae: 7.1676 - val_loss: 62.6311 - val_mae: 6.3414\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 98.9366 - mae: 7.0468 - val_loss: 63.3991 - val_mae: 6.3584\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 95.4558 - mae: 7.0321 - val_loss: 55.4809 - val_mae: 5.9054\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 93.7780 - mae: 6.9104 - val_loss: 42.8653 - val_mae: 5.2911\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 93.9628 - mae: 6.8705 - val_loss: 42.7624 - val_mae: 5.2879\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 93.4555 - mae: 6.8447 - val_loss: 41.6445 - val_mae: 5.2394\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 92.6519 - mae: 6.8894 - val_loss: 41.2638 - val_mae: 5.1923\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 92.0201 - mae: 6.9324 - val_loss: 44.2374 - val_mae: 5.2750\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 92.4091 - mae: 6.8595 - val_loss: 42.5194 - val_mae: 5.3196\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 96.5793 - mae: 6.7951 - val_loss: 46.5188 - val_mae: 5.5370\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 95.8612 - mae: 6.9129 - val_loss: 49.6743 - val_mae: 5.5656\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 91.6675 - mae: 6.7761 - val_loss: 58.4179 - val_mae: 6.0261\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 92.2882 - mae: 6.7415 - val_loss: 41.8829 - val_mae: 5.1458\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 91.5379 - mae: 6.7669 - val_loss: 55.1788 - val_mae: 5.8893\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 95.2156 - mae: 6.8166 - val_loss: 39.7503 - val_mae: 5.0695\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 88.8349 - mae: 6.7236 - val_loss: 42.7737 - val_mae: 5.1800\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 93.2427 - mae: 6.8062 - val_loss: 40.0094 - val_mae: 5.0528\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 89.3869 - mae: 6.7063 - val_loss: 39.0827 - val_mae: 5.0586\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 89.8468 - mae: 6.6300 - val_loss: 40.3927 - val_mae: 5.2236\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 90.4636 - mae: 6.7015 - val_loss: 40.7437 - val_mae: 5.1042\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 89.3262 - mae: 6.7390 - val_loss: 44.5987 - val_mae: 5.2784\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 91.6262 - mae: 6.7124 - val_loss: 38.8031 - val_mae: 5.0497\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 87.9664 - mae: 6.5568 - val_loss: 41.2607 - val_mae: 5.2947\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 89.0625 - mae: 6.5883 - val_loss: 66.3276 - val_mae: 6.4112\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 89.7302 - mae: 6.8523 - val_loss: 39.0048 - val_mae: 5.0041\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 75.7833 - mae: 6.4953\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 1976.4631 - mae: 24.9624 - val_loss: 92.0227 - val_mae: 7.7692\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 111.3110 - mae: 7.6819 - val_loss: 38.3741 - val_mae: 4.5534\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 87.0192 - mae: 6.5419 - val_loss: 31.7850 - val_mae: 4.6930\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 83.0455 - mae: 6.5096 - val_loss: 30.8344 - val_mae: 4.6713\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 82.4864 - mae: 6.4415 - val_loss: 31.4124 - val_mae: 4.8137\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 80.1411 - mae: 6.5360 - val_loss: 29.9031 - val_mae: 4.5418\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 80.9173 - mae: 6.3912 - val_loss: 30.3189 - val_mae: 4.6978\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 79.4935 - mae: 6.3390 - val_loss: 37.7037 - val_mae: 5.4374\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 82.0350 - mae: 6.5326 - val_loss: 29.2614 - val_mae: 4.5477\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 78.9287 - mae: 6.2680 - val_loss: 34.5033 - val_mae: 5.1616\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 80.5781 - mae: 6.5108 - val_loss: 32.0883 - val_mae: 4.9524\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 79.9088 - mae: 6.3714 - val_loss: 38.1970 - val_mae: 5.4811\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 80.3287 - mae: 6.3712 - val_loss: 32.6121 - val_mae: 5.0095\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 78.9394 - mae: 6.3860 - val_loss: 30.0855 - val_mae: 4.7426\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 77.8264 - mae: 6.3397 - val_loss: 28.9605 - val_mae: 4.5890\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 78.5452 - mae: 6.3133 - val_loss: 28.4151 - val_mae: 4.4244\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 77.0681 - mae: 6.0639 - val_loss: 68.0476 - val_mae: 7.2430\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 79.5498 - mae: 6.6186 - val_loss: 28.0641 - val_mae: 4.3907\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 78.0146 - mae: 6.1898 - val_loss: 36.7233 - val_mae: 5.3336\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 77.0698 - mae: 6.3658 - val_loss: 29.3982 - val_mae: 4.7000\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 76.6914 - mae: 6.2153 - val_loss: 47.3790 - val_mae: 6.0967\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 77.1919 - mae: 6.3097 - val_loss: 40.8570 - val_mae: 5.6469\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 77.0124 - mae: 6.3530 - val_loss: 27.7698 - val_mae: 4.3153\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 77.5965 - mae: 6.1591 - val_loss: 32.9371 - val_mae: 5.0513\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 76.9863 - mae: 6.2906 - val_loss: 31.6053 - val_mae: 4.9367\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 75.0251 - mae: 6.2000 - val_loss: 27.4938 - val_mae: 4.4206\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 66.4898 - mae: 5.9466\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 239776064.0000 - mae: 4555.7354 - val_loss: 169.3525 - val_mae: 11.4142\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 238.5111 - mae: 12.4207 - val_loss: 168.7057 - val_mae: 11.3868\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 237.8095 - mae: 12.3940 - val_loss: 168.0618 - val_mae: 11.3595\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 237.1109 - mae: 12.3676 - val_loss: 167.4087 - val_mae: 11.3317\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 236.4025 - mae: 12.3413 - val_loss: 166.7556 - val_mae: 11.3038\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 235.6939 - mae: 12.3140 - val_loss: 166.1159 - val_mae: 11.2770\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 234.9994 - mae: 12.2883 - val_loss: 165.4717 - val_mae: 11.2508\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 234.2999 - mae: 12.2624 - val_loss: 164.8259 - val_mae: 11.2244\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 233.5996 - mae: 12.2361 - val_loss: 164.2016 - val_mae: 11.1989\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 232.9205 - mae: 12.2103 - val_loss: 163.5594 - val_mae: 11.1726\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 232.2234 - mae: 12.1839 - val_loss: 162.9289 - val_mae: 11.1467\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 231.5387 - mae: 12.1586 - val_loss: 162.3039 - val_mae: 11.1209\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 230.8595 - mae: 12.1329 - val_loss: 161.6866 - val_mae: 11.0954\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 230.1877 - mae: 12.1076 - val_loss: 161.0638 - val_mae: 11.0696\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 229.5105 - mae: 12.0821 - val_loss: 160.4461 - val_mae: 11.0440\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 228.8397 - mae: 12.0568 - val_loss: 159.8383 - val_mae: 11.0187\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 228.1774 - mae: 12.0315 - val_loss: 159.2083 - val_mae: 10.9924\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 227.4935 - mae: 12.0058 - val_loss: 158.6108 - val_mae: 10.9674\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 226.8423 - mae: 11.9808 - val_loss: 157.9984 - val_mae: 10.9417\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 226.1759 - mae: 11.9551 - val_loss: 157.4015 - val_mae: 10.9166\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 225.5258 - mae: 11.9305 - val_loss: 156.7846 - val_mae: 10.8905\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 224.8546 - mae: 11.9054 - val_loss: 156.1864 - val_mae: 10.8652\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 224.2034 - mae: 11.8803 - val_loss: 155.5984 - val_mae: 10.8403\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 223.5626 - mae: 11.8554 - val_loss: 155.0054 - val_mae: 10.8151\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 222.9168 - mae: 11.8309 - val_loss: 154.4153 - val_mae: 10.7899\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 222.2731 - mae: 11.8065 - val_loss: 153.8310 - val_mae: 10.7649\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 221.6367 - mae: 11.7820 - val_loss: 153.2578 - val_mae: 10.7404\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 221.0112 - mae: 11.7588 - val_loss: 152.6716 - val_mae: 10.7152\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 220.3726 - mae: 11.7337 - val_loss: 152.0980 - val_mae: 10.6905\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 219.7471 - mae: 11.7106 - val_loss: 151.5307 - val_mae: 10.6660\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 219.1276 - mae: 11.6868 - val_loss: 150.9542 - val_mae: 10.6410\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 218.4983 - mae: 11.6627 - val_loss: 150.3879 - val_mae: 10.6164\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 217.8804 - mae: 11.6389 - val_loss: 149.8232 - val_mae: 10.5918\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 217.2633 - mae: 11.6148 - val_loss: 149.2501 - val_mae: 10.5668\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 216.6379 - mae: 11.5908 - val_loss: 148.6870 - val_mae: 10.5422\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 216.0234 - mae: 11.5667 - val_loss: 148.1367 - val_mae: 10.5180\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 215.4215 - mae: 11.5435 - val_loss: 147.5794 - val_mae: 10.4935\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 214.8132 - mae: 11.5197 - val_loss: 147.0361 - val_mae: 10.4695\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 214.2187 - mae: 11.4970 - val_loss: 146.4861 - val_mae: 10.4452\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 213.6180 - mae: 11.4734 - val_loss: 145.9524 - val_mae: 10.4216\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 213.0339 - mae: 11.4504 - val_loss: 145.4003 - val_mae: 10.3970\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 212.4302 - mae: 11.4266 - val_loss: 144.8618 - val_mae: 10.3730\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 211.8410 - mae: 11.4040 - val_loss: 144.3243 - val_mae: 10.3490\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 211.2531 - mae: 11.3812 - val_loss: 143.7933 - val_mae: 10.3252\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 210.6718 - mae: 11.3581 - val_loss: 143.2633 - val_mae: 10.3014\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 210.0920 - mae: 11.3348 - val_loss: 142.7435 - val_mae: 10.2780\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 209.5225 - mae: 11.3123 - val_loss: 142.2129 - val_mae: 10.2540\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 208.9415 - mae: 11.2897 - val_loss: 141.7029 - val_mae: 10.2309\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 208.3827 - mae: 11.2678 - val_loss: 141.1869 - val_mae: 10.2075\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 207.8175 - mae: 11.2460 - val_loss: 140.6694 - val_mae: 10.1839\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 207.2497 - mae: 11.2227 - val_loss: 140.1508 - val_mae: 10.1602\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 206.6814 - mae: 11.2008 - val_loss: 139.6387 - val_mae: 10.1368\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 206.1202 - mae: 11.1778 - val_loss: 139.1279 - val_mae: 10.1134\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 205.5599 - mae: 11.1561 - val_loss: 138.6135 - val_mae: 10.0897\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 204.9950 - mae: 11.1333 - val_loss: 138.1070 - val_mae: 10.0663\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 204.4397 - mae: 11.1112 - val_loss: 137.6065 - val_mae: 10.0432\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 203.8898 - mae: 11.0896 - val_loss: 137.1006 - val_mae: 10.0198\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 203.3346 - mae: 11.0675 - val_loss: 136.6062 - val_mae: 9.9981\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 202.7918 - mae: 11.0459 - val_loss: 136.1079 - val_mae: 9.9761\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 202.2441 - mae: 11.0242 - val_loss: 135.6128 - val_mae: 9.9542\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 201.7007 - mae: 11.0027 - val_loss: 135.1210 - val_mae: 9.9324\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 201.1600 - mae: 10.9812 - val_loss: 134.6387 - val_mae: 9.9110\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 200.6300 - mae: 10.9604 - val_loss: 134.1487 - val_mae: 9.8892\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 200.0915 - mae: 10.9388 - val_loss: 133.6700 - val_mae: 9.8678\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 199.5653 - mae: 10.9182 - val_loss: 133.1960 - val_mae: 9.8465\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 199.0430 - mae: 10.8974 - val_loss: 132.7111 - val_mae: 9.8247\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 198.5098 - mae: 10.8765 - val_loss: 132.2338 - val_mae: 9.8032\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 197.9844 - mae: 10.8551 - val_loss: 131.7529 - val_mae: 9.7815\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 197.4547 - mae: 10.8338 - val_loss: 131.2772 - val_mae: 9.7600\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 196.9313 - mae: 10.8135 - val_loss: 130.8067 - val_mae: 9.7386\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 196.4131 - mae: 10.7925 - val_loss: 130.3472 - val_mae: 9.7177\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 195.9070 - mae: 10.7720 - val_loss: 129.8894 - val_mae: 9.6968\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 195.4026 - mae: 10.7513 - val_loss: 129.4267 - val_mae: 9.6756\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 194.8926 - mae: 10.7310 - val_loss: 128.9699 - val_mae: 9.6547\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 194.3891 - mae: 10.7107 - val_loss: 128.5172 - val_mae: 9.6338\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 193.8894 - mae: 10.6901 - val_loss: 128.0594 - val_mae: 9.6127\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 193.3848 - mae: 10.6697 - val_loss: 127.6106 - val_mae: 9.5920\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 192.8895 - mae: 10.6498 - val_loss: 127.1580 - val_mae: 9.5710\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 192.3902 - mae: 10.6300 - val_loss: 126.7167 - val_mae: 9.5504\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 191.9026 - mae: 10.6102 - val_loss: 126.2702 - val_mae: 9.5296\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 191.4097 - mae: 10.5903 - val_loss: 125.8212 - val_mae: 9.5086\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 190.9143 - mae: 10.5699 - val_loss: 125.3800 - val_mae: 9.4880\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 190.4265 - mae: 10.5495 - val_loss: 124.9431 - val_mae: 9.4674\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 189.9437 - mae: 10.5303 - val_loss: 124.5061 - val_mae: 9.4468\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 189.4607 - mae: 10.5100 - val_loss: 124.0670 - val_mae: 9.4261\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 188.9761 - mae: 10.4898 - val_loss: 123.6424 - val_mae: 9.4060\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 188.5059 - mae: 10.4712 - val_loss: 123.2130 - val_mae: 9.3856\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 188.0307 - mae: 10.4513 - val_loss: 122.7853 - val_mae: 9.3652\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 187.5576 - mae: 10.4320 - val_loss: 122.3637 - val_mae: 9.3451\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 187.0914 - mae: 10.4127 - val_loss: 121.9475 - val_mae: 9.3259\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 186.6301 - mae: 10.3935 - val_loss: 121.5247 - val_mae: 9.3068\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 186.1618 - mae: 10.3738 - val_loss: 121.0996 - val_mae: 9.2875\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 185.6911 - mae: 10.3543 - val_loss: 120.6817 - val_mae: 9.2685\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 185.2288 - mae: 10.3356 - val_loss: 120.2793 - val_mae: 9.2501\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 184.7821 - mae: 10.3165 - val_loss: 119.8672 - val_mae: 9.2313\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 184.3257 - mae: 10.2983 - val_loss: 119.4578 - val_mae: 9.2125\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 183.8717 - mae: 10.2796 - val_loss: 119.0453 - val_mae: 9.1935\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 183.4139 - mae: 10.2602 - val_loss: 118.6285 - val_mae: 9.1743\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 182.9519 - mae: 10.2417 - val_loss: 118.2280 - val_mae: 9.1558\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 182.5070 - mae: 10.2233 - val_loss: 117.8242 - val_mae: 9.1371\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 187.8460 - mae: 11.1542\n"
     ]
    }
   ],
   "source": [
    "lr = [10**(-6), 10**(-5), 10**(-4)]\n",
    "name = \"lr\"\n",
    "results_lr = []\n",
    "for value in lr:\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_dir(name, value))\n",
    "    model = build_model(1, 25, \"sgd\", value)\n",
    "    model.fit(X_train, y_train, epochs=100, validation_split=0.1, callbacks=[tensorboard_cb, es_cb])\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    results_lr.append((value, score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4981015-f22e-4fb1-9180-ac0936dfc9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-06, 75.78328704833984, 6.495291709899902),\n",
       " (1e-05, 66.48982238769531, 5.946558952331543),\n",
       " (0.0001, 187.84597778320312, 11.154184341430664)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8038d740-38f7-4018-ba5b-044f95e12658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('lr.pkl', 'wb') as fp:\n",
    "    pickle.dump(results_lr, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d887a2-40b5-41f0-b4b0-8ca45fa54033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6480), started 4:31:37 ago. (Use '!kill 6480' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3aaf562224470e0e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3aaf562224470e0e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "564e21b4-3c68-4587-9a0f-8ecdc320d8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 17ms/step - loss: 21292295559556628480.0000 - mae: 1202941952.0000 - val_loss: 21449109297692739108864.0000 - val_mae: 143597600768.0000\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: inf - mae: 539821520172613632.0000 - val_loss: inf - val_mae: 67373058777090621440.0000\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: inf - mae: 276100940946972912849518592.0000 - val_loss: inf - val_mae: 26818146889840090426529284096.0000\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: inf - mae: 109274026944394145150861008902815744.0000 - val_loss: inf - val_mae: inf\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 19ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "4/4 [==============================] - 0s 2ms/step - loss: nan - mae: nan\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 20ms/step - loss: 204742.5938 - mae: 171.3230 - val_loss: 504.5232 - val_mae: 21.4905\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 593.8337 - mae: 22.4627 - val_loss: 504.2938 - val_mae: 21.4852\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 593.5941 - mae: 22.4573 - val_loss: 504.0646 - val_mae: 21.4799\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 593.3544 - mae: 22.4520 - val_loss: 503.8331 - val_mae: 21.4745\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 593.1124 - mae: 22.4466 - val_loss: 503.6010 - val_mae: 21.4691\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 592.8698 - mae: 22.4411 - val_loss: 503.3710 - val_mae: 21.4637\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 592.6295 - mae: 22.4358 - val_loss: 503.1396 - val_mae: 21.4583\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 592.3876 - mae: 22.4305 - val_loss: 502.9074 - val_mae: 21.4529\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 592.1450 - mae: 22.4251 - val_loss: 502.6789 - val_mae: 21.4476\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 591.9060 - mae: 22.4197 - val_loss: 502.4464 - val_mae: 21.4422\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 591.6631 - mae: 22.4143 - val_loss: 502.2157 - val_mae: 21.4368\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 591.4219 - mae: 22.4089 - val_loss: 501.9855 - val_mae: 21.4314\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 591.1815 - mae: 22.4035 - val_loss: 501.7563 - val_mae: 21.4261\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 590.9418 - mae: 22.3982 - val_loss: 501.5256 - val_mae: 21.4207\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 590.7006 - mae: 22.3928 - val_loss: 501.2954 - val_mae: 21.4153\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 590.4601 - mae: 22.3874 - val_loss: 501.0667 - val_mae: 21.4100\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 590.2207 - mae: 22.3821 - val_loss: 500.8330 - val_mae: 21.4045\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 589.9767 - mae: 22.3766 - val_loss: 500.6053 - val_mae: 21.3992\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 589.7385 - mae: 22.3713 - val_loss: 500.3741 - val_mae: 21.3938\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 589.4969 - mae: 22.3659 - val_loss: 500.1456 - val_mae: 21.3885\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 589.2580 - mae: 22.3606 - val_loss: 499.9125 - val_mae: 21.3830\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 589.0144 - mae: 22.3552 - val_loss: 499.6827 - val_mae: 21.3776\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 588.7742 - mae: 22.3498 - val_loss: 499.4545 - val_mae: 21.3723\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 588.5356 - mae: 22.3444 - val_loss: 499.2248 - val_mae: 21.3669\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 588.2955 - mae: 22.3390 - val_loss: 498.9951 - val_mae: 21.3615\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 588.0553 - mae: 22.3336 - val_loss: 498.7661 - val_mae: 21.3562\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 587.8161 - mae: 22.3283 - val_loss: 498.5389 - val_mae: 21.3509\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 587.5784 - mae: 22.3230 - val_loss: 498.3086 - val_mae: 21.3455\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 587.3378 - mae: 22.3176 - val_loss: 498.0804 - val_mae: 21.3401\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 587.0993 - mae: 22.3123 - val_loss: 497.8530 - val_mae: 21.3348\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 586.8613 - mae: 22.3070 - val_loss: 497.6232 - val_mae: 21.3294\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 586.6212 - mae: 22.3016 - val_loss: 497.3951 - val_mae: 21.3241\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 586.3827 - mae: 22.2962 - val_loss: 497.1668 - val_mae: 21.3187\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 586.1439 - mae: 22.2908 - val_loss: 496.9364 - val_mae: 21.3133\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 585.9030 - mae: 22.2854 - val_loss: 496.7075 - val_mae: 21.3079\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 585.6638 - mae: 22.2800 - val_loss: 496.4807 - val_mae: 21.3026\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 585.4266 - mae: 22.2747 - val_loss: 496.2520 - val_mae: 21.2972\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 585.1877 - mae: 22.2694 - val_loss: 496.0258 - val_mae: 21.2919\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 584.9510 - mae: 22.2641 - val_loss: 495.7978 - val_mae: 21.2866\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 584.7127 - mae: 22.2587 - val_loss: 495.5727 - val_mae: 21.2813\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 584.4772 - mae: 22.2534 - val_loss: 495.3433 - val_mae: 21.2759\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 584.2374 - mae: 22.2480 - val_loss: 495.1162 - val_mae: 21.2706\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 584.0001 - mae: 22.2427 - val_loss: 494.8889 - val_mae: 21.2652\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 583.7623 - mae: 22.2374 - val_loss: 494.6625 - val_mae: 21.2599\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 583.5256 - mae: 22.2320 - val_loss: 494.4359 - val_mae: 21.2546\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 583.2888 - mae: 22.2267 - val_loss: 494.2110 - val_mae: 21.2493\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 583.0535 - mae: 22.2214 - val_loss: 493.9834 - val_mae: 21.2439\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 582.8156 - mae: 22.2161 - val_loss: 493.7597 - val_mae: 21.2386\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 582.5817 - mae: 22.2107 - val_loss: 493.5342 - val_mae: 21.2333\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 582.3459 - mae: 22.2055 - val_loss: 493.3081 - val_mae: 21.2280\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 582.1093 - mae: 22.2001 - val_loss: 493.0813 - val_mae: 21.2227\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 581.8721 - mae: 22.1948 - val_loss: 492.8553 - val_mae: 21.2173\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 581.6359 - mae: 22.1894 - val_loss: 492.6292 - val_mae: 21.2120\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 581.3995 - mae: 22.1841 - val_loss: 492.4020 - val_mae: 21.2067\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 581.1617 - mae: 22.1787 - val_loss: 492.1759 - val_mae: 21.2013\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 580.9254 - mae: 22.1734 - val_loss: 491.9507 - val_mae: 21.1960\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 580.6899 - mae: 22.1681 - val_loss: 491.7239 - val_mae: 21.1907\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 580.4528 - mae: 22.1628 - val_loss: 491.4991 - val_mae: 21.1854\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 580.2178 - mae: 22.1575 - val_loss: 491.2732 - val_mae: 21.1800\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 579.9814 - mae: 22.1521 - val_loss: 491.0474 - val_mae: 21.1747\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 579.7453 - mae: 22.1468 - val_loss: 490.8219 - val_mae: 21.1694\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 579.5095 - mae: 22.1415 - val_loss: 490.5981 - val_mae: 21.1641\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 579.2754 - mae: 22.1362 - val_loss: 490.3722 - val_mae: 21.1587\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 579.0391 - mae: 22.1308 - val_loss: 490.1483 - val_mae: 21.1535\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 578.8051 - mae: 22.1255 - val_loss: 489.9250 - val_mae: 21.1482\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 578.5715 - mae: 22.1203 - val_loss: 489.6990 - val_mae: 21.1428\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 578.3351 - mae: 22.1150 - val_loss: 489.4741 - val_mae: 21.1375\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 578.0999 - mae: 22.1096 - val_loss: 489.2480 - val_mae: 21.1322\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 577.8634 - mae: 22.1043 - val_loss: 489.0228 - val_mae: 21.1268\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 577.6277 - mae: 22.0990 - val_loss: 488.7981 - val_mae: 21.1215\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 577.3929 - mae: 22.0937 - val_loss: 488.5756 - val_mae: 21.1162\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 577.1601 - mae: 22.0884 - val_loss: 488.3531 - val_mae: 21.1110\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 576.9273 - mae: 22.0831 - val_loss: 488.1289 - val_mae: 21.1057\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 576.6929 - mae: 22.0778 - val_loss: 487.9058 - val_mae: 21.1004\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 576.4595 - mae: 22.0725 - val_loss: 487.6831 - val_mae: 21.0951\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 576.2266 - mae: 22.0672 - val_loss: 487.4588 - val_mae: 21.0898\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 575.9921 - mae: 22.0619 - val_loss: 487.2363 - val_mae: 21.0845\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 575.7593 - mae: 22.0566 - val_loss: 487.0124 - val_mae: 21.0792\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 575.5251 - mae: 22.0514 - val_loss: 486.7907 - val_mae: 21.0739\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 575.2932 - mae: 22.0461 - val_loss: 486.5674 - val_mae: 21.0686\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 575.0595 - mae: 22.0408 - val_loss: 486.3432 - val_mae: 21.0633\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 574.8250 - mae: 22.0355 - val_loss: 486.1203 - val_mae: 21.0580\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 574.5919 - mae: 22.0302 - val_loss: 485.8980 - val_mae: 21.0528\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 574.3593 - mae: 22.0249 - val_loss: 485.6753 - val_mae: 21.0475\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 574.1263 - mae: 22.0196 - val_loss: 485.4517 - val_mae: 21.0421\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 573.8926 - mae: 22.0143 - val_loss: 485.2310 - val_mae: 21.0369\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 573.6617 - mae: 22.0091 - val_loss: 485.0089 - val_mae: 21.0316\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 573.4292 - mae: 22.0038 - val_loss: 484.7867 - val_mae: 21.0263\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 573.1968 - mae: 21.9985 - val_loss: 484.5656 - val_mae: 21.0211\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 572.9656 - mae: 21.9933 - val_loss: 484.3453 - val_mae: 21.0158\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 572.7351 - mae: 21.9880 - val_loss: 484.1232 - val_mae: 21.0106\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 572.5026 - mae: 21.9827 - val_loss: 483.9000 - val_mae: 21.0052\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 572.2692 - mae: 21.9774 - val_loss: 483.6783 - val_mae: 21.0000\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 572.0373 - mae: 21.9721 - val_loss: 483.4597 - val_mae: 20.9948\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 571.8085 - mae: 21.9669 - val_loss: 483.2385 - val_mae: 20.9895\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 571.5771 - mae: 21.9617 - val_loss: 483.0176 - val_mae: 20.9842\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 571.3460 - mae: 21.9564 - val_loss: 482.7955 - val_mae: 20.9789\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 571.1136 - mae: 21.9511 - val_loss: 482.5719 - val_mae: 20.9736\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 570.8798 - mae: 21.9458 - val_loss: 482.3521 - val_mae: 20.9684\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 570.6496 - mae: 21.9405 - val_loss: 482.1309 - val_mae: 20.9631\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 590.4849 - mae: 22.5220\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 15ms/step - loss: 10451.7344 - mae: 55.0229 - val_loss: 37.0298 - val_mae: 4.5211\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 77.4038 - mae: 6.3702 - val_loss: 30.1728 - val_mae: 4.1678\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 76.1124 - mae: 6.2977 - val_loss: 25.8057 - val_mae: 3.9760\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 71.3566 - mae: 6.0009 - val_loss: 24.8020 - val_mae: 4.0901\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 69.3486 - mae: 5.9240 - val_loss: 24.9105 - val_mae: 3.9482\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 67.9424 - mae: 5.9773 - val_loss: 26.8034 - val_mae: 3.9969\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 67.3855 - mae: 5.7923 - val_loss: 24.9927 - val_mae: 3.9424\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 66.9752 - mae: 5.7817 - val_loss: 26.5719 - val_mae: 4.1289\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 67.3831 - mae: 5.8608 - val_loss: 26.7228 - val_mae: 4.0198\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 65.3906 - mae: 5.6526 - val_loss: 31.1534 - val_mae: 4.4789\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 69.2393 - mae: 5.9271 - val_loss: 27.3591 - val_mae: 4.2476\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 66.1692 - mae: 5.7870 - val_loss: 32.4732 - val_mae: 4.6433\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 65.3678 - mae: 5.7176 - val_loss: 27.0642 - val_mae: 4.2171\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 65.7863 - mae: 5.7605 - val_loss: 26.6272 - val_mae: 4.1394\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 59.9959 - mae: 5.6438\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 17ms/step - loss: 189.6759 - mae: 10.5787 - val_loss: 54.4695 - val_mae: 6.0197\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 78.2698 - mae: 6.4026 - val_loss: 30.8724 - val_mae: 4.4841\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 80.0451 - mae: 6.4366 - val_loss: 35.2113 - val_mae: 4.7114\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 74.3996 - mae: 6.2357 - val_loss: 23.9522 - val_mae: 3.9225\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 70.4535 - mae: 5.9819 - val_loss: 23.2053 - val_mae: 3.9255\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 68.7713 - mae: 6.0079 - val_loss: 27.4187 - val_mae: 4.2490\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 70.4131 - mae: 6.0423 - val_loss: 24.4802 - val_mae: 4.0042\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 68.0375 - mae: 5.9387 - val_loss: 26.9257 - val_mae: 4.1547\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 70.3559 - mae: 6.0879 - val_loss: 34.2899 - val_mae: 4.7020\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 67.9324 - mae: 5.8612 - val_loss: 32.5389 - val_mae: 4.5511\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 73.3582 - mae: 6.1154 - val_loss: 29.3013 - val_mae: 4.3551\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 69.3025 - mae: 5.9347 - val_loss: 31.0171 - val_mae: 4.4402\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 67.4670 - mae: 5.8166 - val_loss: 21.0188 - val_mae: 3.7994\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 68.0232 - mae: 5.8435 - val_loss: 22.5948 - val_mae: 3.8538\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 65.8923 - mae: 5.7408 - val_loss: 20.9606 - val_mae: 3.7758\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 66.7781 - mae: 5.7001 - val_loss: 37.9089 - val_mae: 5.0420\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 64.5357 - mae: 5.5038 - val_loss: 100.3580 - val_mae: 8.9161\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 77.5306 - mae: 6.4262 - val_loss: 28.7649 - val_mae: 4.3211\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 67.4061 - mae: 5.6487 - val_loss: 24.3522 - val_mae: 3.9598\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 63.6591 - mae: 5.6585 - val_loss: 20.3242 - val_mae: 3.6957\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 62.3091 - mae: 5.5159 - val_loss: 63.3191 - val_mae: 6.7657\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 67.5762 - mae: 5.9394 - val_loss: 30.3975 - val_mae: 4.4184\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 64.2378 - mae: 5.6423 - val_loss: 19.4921 - val_mae: 3.6460\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 68.5293 - mae: 5.8134 - val_loss: 28.0166 - val_mae: 4.2901\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 67.6187 - mae: 5.9518 - val_loss: 21.5793 - val_mae: 3.7640\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 61.5142 - mae: 5.4816 - val_loss: 23.6801 - val_mae: 3.9544\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 61.5734 - mae: 5.4283 - val_loss: 34.2871 - val_mae: 4.7948\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 62.8415 - mae: 5.3793 - val_loss: 20.2897 - val_mae: 3.6641\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 63.3905 - mae: 5.6040 - val_loss: 26.6308 - val_mae: 4.1844\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 62.0615 - mae: 5.4734 - val_loss: 22.2205 - val_mae: 3.9116\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 64.1802 - mae: 5.5858 - val_loss: 19.3681 - val_mae: 3.6494\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 61.3741 - mae: 5.4102 - val_loss: 19.7060 - val_mae: 3.6598\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 63.5418 - mae: 5.5270 - val_loss: 19.1604 - val_mae: 3.5996\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 63.3084 - mae: 5.9308\n"
     ]
    }
   ],
   "source": [
    "hl = list(range(4))\n",
    "name = \"hl\"\n",
    "results_hl = []\n",
    "for value in hl:\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_dir(name, value))\n",
    "    model = build_model(value, 25, \"sgd\", 10**(-5))\n",
    "    model.fit(X_train, y_train, epochs=100, validation_split=0.1, callbacks=[tensorboard_cb, es_cb])\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    results_hl.append((value, score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0acb20b-5c06-4a18-a3ad-2ee4ebc20dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, nan, nan),\n",
       " (1, 590.48486328125, 22.52201271057129),\n",
       " (2, 59.99591064453125, 5.643824100494385),\n",
       " (3, 63.30836486816406, 5.930777072906494)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('hl.pkl', 'wb') as fp:\n",
    "    pickle.dump(results_hl, fp)\n",
    "    \n",
    "results_hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b30bf2df-1df0-48e1-a85f-6ec84a61a97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6480), started 4:31:53 ago. (Use '!kill 6480' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-afc1e927e08f478b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-afc1e927e08f478b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6f71f2-6739-4f83-80b2-dd83bcd84732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 23ms/step - loss: 14378.0225 - mae: 54.8116 - val_loss: 505.2232 - val_mae: 21.5068\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 594.5654 - mae: 22.4790 - val_loss: 504.9935 - val_mae: 21.5015\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 594.3253 - mae: 22.4736 - val_loss: 504.7639 - val_mae: 21.4961\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 594.0853 - mae: 22.4682 - val_loss: 504.5320 - val_mae: 21.4908\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 593.8430 - mae: 22.4629 - val_loss: 504.2996 - val_mae: 21.4853\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 593.6001 - mae: 22.4574 - val_loss: 504.0693 - val_mae: 21.4800\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 593.3593 - mae: 22.4521 - val_loss: 503.8376 - val_mae: 21.4746\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 593.1171 - mae: 22.4467 - val_loss: 503.6050 - val_mae: 21.4692\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 592.8742 - mae: 22.4413 - val_loss: 503.3761 - val_mae: 21.4638\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 592.6348 - mae: 22.4360 - val_loss: 503.1433 - val_mae: 21.4584\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 592.3916 - mae: 22.4305 - val_loss: 502.9123 - val_mae: 21.4530\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 592.1501 - mae: 22.4251 - val_loss: 502.6817 - val_mae: 21.4477\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 591.9091 - mae: 22.4198 - val_loss: 502.4523 - val_mae: 21.4423\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 591.6692 - mae: 22.4144 - val_loss: 502.2212 - val_mae: 21.4369\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 591.4277 - mae: 22.4090 - val_loss: 501.9906 - val_mae: 21.4315\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 591.1868 - mae: 22.4037 - val_loss: 501.7615 - val_mae: 21.4262\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 590.9471 - mae: 22.3984 - val_loss: 501.5275 - val_mae: 21.4207\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 590.7028 - mae: 22.3929 - val_loss: 501.2995 - val_mae: 21.4154\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 590.4642 - mae: 22.3875 - val_loss: 501.0680 - val_mae: 21.4100\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 590.2224 - mae: 22.3821 - val_loss: 500.8390 - val_mae: 21.4047\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 589.9829 - mae: 22.3768 - val_loss: 500.6057 - val_mae: 21.3992\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 589.7390 - mae: 22.3714 - val_loss: 500.3756 - val_mae: 21.3938\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 589.4985 - mae: 22.3660 - val_loss: 500.1469 - val_mae: 21.3885\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 589.2595 - mae: 22.3606 - val_loss: 499.9169 - val_mae: 21.3831\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 589.0190 - mae: 22.3552 - val_loss: 499.6868 - val_mae: 21.3777\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 588.7786 - mae: 22.3498 - val_loss: 499.4576 - val_mae: 21.3724\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 588.5389 - mae: 22.3445 - val_loss: 499.2301 - val_mae: 21.3670\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 588.3009 - mae: 22.3392 - val_loss: 498.9994 - val_mae: 21.3616\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 588.0599 - mae: 22.3337 - val_loss: 498.7709 - val_mae: 21.3563\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 587.8210 - mae: 22.3285 - val_loss: 498.5431 - val_mae: 21.3510\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 587.5829 - mae: 22.3231 - val_loss: 498.3131 - val_mae: 21.3456\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 587.3423 - mae: 22.3177 - val_loss: 498.0846 - val_mae: 21.3402\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 587.1035 - mae: 22.3124 - val_loss: 497.8560 - val_mae: 21.3349\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 586.8643 - mae: 22.3070 - val_loss: 497.6251 - val_mae: 21.3295\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 586.6231 - mae: 22.3016 - val_loss: 497.3958 - val_mae: 21.3241\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 586.3835 - mae: 22.2962 - val_loss: 497.1688 - val_mae: 21.3188\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 586.1460 - mae: 22.2909 - val_loss: 496.9398 - val_mae: 21.3134\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 585.9067 - mae: 22.2855 - val_loss: 496.7132 - val_mae: 21.3081\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 585.6697 - mae: 22.2802 - val_loss: 496.4849 - val_mae: 21.3027\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 585.4310 - mae: 22.2748 - val_loss: 496.2594 - val_mae: 21.2974\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 585.1953 - mae: 22.2695 - val_loss: 496.0296 - val_mae: 21.2920\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 584.9551 - mae: 22.2641 - val_loss: 495.8023 - val_mae: 21.2867\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 584.7173 - mae: 22.2588 - val_loss: 495.5746 - val_mae: 21.2813\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 584.4794 - mae: 22.2535 - val_loss: 495.3479 - val_mae: 21.2760\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 584.2422 - mae: 22.2481 - val_loss: 495.1210 - val_mae: 21.2707\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 584.0051 - mae: 22.2428 - val_loss: 494.8958 - val_mae: 21.2654\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 583.7695 - mae: 22.2375 - val_loss: 494.6678 - val_mae: 21.2600\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 583.5312 - mae: 22.2322 - val_loss: 494.4438 - val_mae: 21.2547\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 583.2969 - mae: 22.2268 - val_loss: 494.2180 - val_mae: 21.2494\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 583.0609 - mae: 22.2216 - val_loss: 493.9915 - val_mae: 21.2441\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 582.8240 - mae: 22.2162 - val_loss: 493.7643 - val_mae: 21.2388\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 582.5864 - mae: 22.2109 - val_loss: 493.5380 - val_mae: 21.2334\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 582.3499 - mae: 22.2055 - val_loss: 493.3116 - val_mae: 21.2281\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 582.1130 - mae: 22.2002 - val_loss: 493.0840 - val_mae: 21.2227\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 581.8749 - mae: 22.1948 - val_loss: 492.8576 - val_mae: 21.2174\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 581.6384 - mae: 22.1895 - val_loss: 492.6320 - val_mae: 21.2121\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 581.4025 - mae: 22.1842 - val_loss: 492.4049 - val_mae: 21.2067\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 581.1649 - mae: 22.1788 - val_loss: 492.1799 - val_mae: 21.2014\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 580.9296 - mae: 22.1736 - val_loss: 491.9536 - val_mae: 21.1961\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 580.6929 - mae: 22.1682 - val_loss: 491.7275 - val_mae: 21.1907\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 580.4565 - mae: 22.1629 - val_loss: 491.5017 - val_mae: 21.1854\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 580.2203 - mae: 22.1575 - val_loss: 491.2775 - val_mae: 21.1801\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 579.9858 - mae: 22.1523 - val_loss: 491.0513 - val_mae: 21.1748\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 579.7493 - mae: 22.1469 - val_loss: 490.8270 - val_mae: 21.1695\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 579.5150 - mae: 22.1416 - val_loss: 490.6035 - val_mae: 21.1642\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 579.2809 - mae: 22.1363 - val_loss: 490.3770 - val_mae: 21.1589\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 579.0442 - mae: 22.1310 - val_loss: 490.1519 - val_mae: 21.1535\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 578.8087 - mae: 22.1257 - val_loss: 489.9255 - val_mae: 21.1482\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 578.5719 - mae: 22.1203 - val_loss: 489.6998 - val_mae: 21.1429\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 578.3358 - mae: 22.1151 - val_loss: 489.4749 - val_mae: 21.1375\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 578.1006 - mae: 22.1097 - val_loss: 489.2520 - val_mae: 21.1323\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 577.8676 - mae: 22.1044 - val_loss: 489.0291 - val_mae: 21.1270\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 577.6344 - mae: 22.0991 - val_loss: 488.8047 - val_mae: 21.1217\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 577.3998 - mae: 22.0938 - val_loss: 488.5812 - val_mae: 21.1164\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 577.1660 - mae: 22.0885 - val_loss: 488.3581 - val_mae: 21.1111\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 576.9326 - mae: 22.0832 - val_loss: 488.1336 - val_mae: 21.1058\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 576.6978 - mae: 22.0779 - val_loss: 487.9106 - val_mae: 21.1005\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 576.4646 - mae: 22.0726 - val_loss: 487.6865 - val_mae: 21.0952\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 576.2302 - mae: 22.0673 - val_loss: 487.4644 - val_mae: 21.0899\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 575.9979 - mae: 22.0621 - val_loss: 487.2409 - val_mae: 21.0846\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 575.7640 - mae: 22.0568 - val_loss: 487.0163 - val_mae: 21.0793\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 575.5291 - mae: 22.0514 - val_loss: 486.7931 - val_mae: 21.0740\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 575.2955 - mae: 22.0461 - val_loss: 486.5704 - val_mae: 21.0687\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 575.0627 - mae: 22.0409 - val_loss: 486.3474 - val_mae: 21.0634\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 574.8293 - mae: 22.0356 - val_loss: 486.1234 - val_mae: 21.0581\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 574.5953 - mae: 22.0302 - val_loss: 485.9024 - val_mae: 21.0529\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 574.3640 - mae: 22.0250 - val_loss: 485.6800 - val_mae: 21.0476\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 574.1313 - mae: 22.0197 - val_loss: 485.4575 - val_mae: 21.0423\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 573.8986 - mae: 22.0144 - val_loss: 485.2361 - val_mae: 21.0370\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 573.6669 - mae: 22.0092 - val_loss: 485.0154 - val_mae: 21.0318\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 573.4361 - mae: 22.0039 - val_loss: 484.7930 - val_mae: 21.0265\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 573.2034 - mae: 21.9986 - val_loss: 484.5695 - val_mae: 21.0212\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 572.9696 - mae: 21.9933 - val_loss: 484.3474 - val_mae: 21.0159\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 572.7374 - mae: 21.9880 - val_loss: 484.1285 - val_mae: 21.0107\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 572.5082 - mae: 21.9828 - val_loss: 483.9070 - val_mae: 21.0054\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 572.2766 - mae: 21.9776 - val_loss: 483.6857 - val_mae: 21.0001\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 572.0450 - mae: 21.9723 - val_loss: 483.4633 - val_mae: 20.9948\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 571.8123 - mae: 21.9670 - val_loss: 483.2395 - val_mae: 20.9895\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 571.5782 - mae: 21.9617 - val_loss: 483.0192 - val_mae: 20.9843\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 571.3476 - mae: 21.9564 - val_loss: 482.7977 - val_mae: 20.9790\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 591.2012 - mae: 22.5379\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 37ms/step - loss: 65602.4844 - mae: 103.3234 - val_loss: 521.5793 - val_mae: 21.9880\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 594.4525 - mae: 22.3787 - val_loss: 494.8153 - val_mae: 20.8818\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 583.6967 - mae: 21.8656 - val_loss: 493.4088 - val_mae: 20.8325\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 581.9642 - mae: 21.7574 - val_loss: 493.5463 - val_mae: 20.8848\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 580.5348 - mae: 21.7168 - val_loss: 492.3494 - val_mae: 20.8896\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 536.4098 - mae: 20.6837 - val_loss: 266.4342 - val_mae: 15.1352\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 149.3236 - mae: 9.0622 - val_loss: 55.5709 - val_mae: 5.9832\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 90.3694 - mae: 6.7722 - val_loss: 47.7153 - val_mae: 5.5034\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 87.1531 - mae: 6.6802 - val_loss: 44.2742 - val_mae: 5.4600\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 83.7188 - mae: 6.5125 - val_loss: 42.2315 - val_mae: 5.1895\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 81.5282 - mae: 6.5628 - val_loss: 40.3977 - val_mae: 5.1587\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 81.3238 - mae: 6.4242 - val_loss: 40.7784 - val_mae: 5.1007\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 79.4864 - mae: 6.3088 - val_loss: 39.0810 - val_mae: 5.0164\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 77.7789 - mae: 6.3418 - val_loss: 36.2479 - val_mae: 4.9447\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 76.8972 - mae: 6.2958 - val_loss: 34.9814 - val_mae: 4.8957\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 77.2816 - mae: 6.2215 - val_loss: 35.1466 - val_mae: 4.9550\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 75.5610 - mae: 6.0758 - val_loss: 64.9633 - val_mae: 6.3897\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 79.4885 - mae: 6.5741 - val_loss: 34.3854 - val_mae: 4.8801\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 76.3177 - mae: 6.1500 - val_loss: 35.0127 - val_mae: 4.7440\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 75.5678 - mae: 6.2455 - val_loss: 32.9823 - val_mae: 4.7746\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 75.2878 - mae: 6.1715 - val_loss: 52.5305 - val_mae: 5.7045\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 76.8925 - mae: 6.3247 - val_loss: 37.2677 - val_mae: 4.8772\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 75.8389 - mae: 6.2701 - val_loss: 32.9374 - val_mae: 4.7824\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 76.7538 - mae: 6.1660 - val_loss: 35.3860 - val_mae: 4.8022\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 76.4545 - mae: 6.3001 - val_loss: 34.5264 - val_mae: 4.7417\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 73.9755 - mae: 6.1504 - val_loss: 32.5627 - val_mae: 4.7327\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 74.0861 - mae: 6.0850 - val_loss: 34.5766 - val_mae: 4.9377\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 75.2408 - mae: 6.0078 - val_loss: 32.9665 - val_mae: 4.6451\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 74.7298 - mae: 6.1880 - val_loss: 32.8539 - val_mae: 4.7933\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 73.5693 - mae: 5.9919 - val_loss: 37.2550 - val_mae: 4.8840\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 70.5078 - mae: 6.3895\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 22ms/step - loss: 61060.6602 - mae: 86.8091 - val_loss: 46.3825 - val_mae: 5.7533\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 91.2325 - mae: 6.9976 - val_loss: 32.3076 - val_mae: 4.6495\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 82.2084 - mae: 6.6464 - val_loss: 28.7135 - val_mae: 4.3619\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 78.5201 - mae: 6.4371 - val_loss: 27.5152 - val_mae: 4.3684\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 76.5098 - mae: 6.3399 - val_loss: 26.9382 - val_mae: 4.3133\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 73.5886 - mae: 6.2697 - val_loss: 27.4861 - val_mae: 4.1327\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 73.7507 - mae: 6.1602 - val_loss: 25.9180 - val_mae: 4.1220\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 72.4669 - mae: 6.1189 - val_loss: 31.1851 - val_mae: 4.7259\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 74.2973 - mae: 6.2583 - val_loss: 26.2370 - val_mae: 4.0588\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 71.6075 - mae: 6.0381 - val_loss: 30.6987 - val_mae: 4.6635\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 73.2692 - mae: 6.1879 - val_loss: 25.6982 - val_mae: 4.1828\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 72.4437 - mae: 6.0566 - val_loss: 29.2650 - val_mae: 4.5934\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 71.4426 - mae: 6.0285 - val_loss: 26.2955 - val_mae: 4.3035\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 70.0820 - mae: 6.0299 - val_loss: 24.9772 - val_mae: 4.1248\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 69.4959 - mae: 5.9445 - val_loss: 24.7007 - val_mae: 4.0826\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 70.0229 - mae: 5.8945 - val_loss: 26.2951 - val_mae: 4.0384\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 68.2925 - mae: 5.7506 - val_loss: 52.8729 - val_mae: 6.1145\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 71.9506 - mae: 6.2197 - val_loss: 25.6232 - val_mae: 4.0204\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 68.5924 - mae: 5.7788 - val_loss: 27.3474 - val_mae: 4.3932\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 67.8002 - mae: 5.8920 - val_loss: 24.2981 - val_mae: 4.0852\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 66.6554 - mae: 5.7207 - val_loss: 43.8502 - val_mae: 5.5036\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 68.4198 - mae: 5.8758 - val_loss: 34.3890 - val_mae: 4.8647\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 67.9783 - mae: 5.8908 - val_loss: 24.3041 - val_mae: 4.0421\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 68.2548 - mae: 5.7915 - val_loss: 27.1612 - val_mae: 4.3099\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 69.3807 - mae: 5.9008 - val_loss: 24.8866 - val_mae: 4.1825\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 62.2796 - mae: 5.8079\n"
     ]
    }
   ],
   "source": [
    "nn = [5, 25, 125]\n",
    "name = \"nn\"\n",
    "results_nn = []\n",
    "for value in nn:\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_dir(name, value))\n",
    "    model = build_model(1, value, \"sgd\", 10**(-5))\n",
    "    model.fit(X_train, y_train, epochs=100, validation_split=0.1, callbacks=[tensorboard_cb, es_cb])\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    results_nn.append((value, score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c310c71c-04c1-4bd3-838e-6d0344213cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 591.2012329101562, 22.53790855407715),\n",
       " (25, 70.50782775878906, 6.389472484588623),\n",
       " (125, 62.27962875366211, 5.80792236328125)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('nn.pkl', 'wb') as fp:\n",
    "    pickle.dump(results_nn, fp)\n",
    "    \n",
    "results_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43077282-2fe3-4b3d-acda-4d293a3ffa45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6480), started 4:32:10 ago. (Use '!kill 6480' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c7b28ad11db4ed53\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c7b28ad11db4ed53\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4194789-b374-47c9-84d4-8824e2f18b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 25ms/step - loss: 13018.1240 - mae: 65.2370 - val_loss: 529.5540 - val_mae: 22.2617\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 487.2162 - mae: 19.8275 - val_loss: 99.9038 - val_mae: 7.6155\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 156.7772 - mae: 9.7153 - val_loss: 71.1782 - val_mae: 6.6300\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 23ms/step - loss: 110.3769 - mae: 8.0015 - val_loss: 51.0345 - val_mae: 5.5003\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 91.7442 - mae: 7.1407 - val_loss: 45.3149 - val_mae: 5.1759\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 84.6806 - mae: 6.8575 - val_loss: 43.8818 - val_mae: 5.1191\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 84.0727 - mae: 6.7035 - val_loss: 40.4575 - val_mae: 4.9183\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 80.6442 - mae: 6.5487 - val_loss: 40.7982 - val_mae: 4.9633\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 80.8577 - mae: 6.6084 - val_loss: 41.7841 - val_mae: 5.1747\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 79.4864 - mae: 6.4776 - val_loss: 37.4794 - val_mae: 4.7463\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 78.3832 - mae: 6.5697 - val_loss: 35.9269 - val_mae: 4.6463\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 79.0558 - mae: 6.4278 - val_loss: 38.1377 - val_mae: 4.7033\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 76.8900 - mae: 6.3869 - val_loss: 33.1186 - val_mae: 4.4910\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 75.7476 - mae: 6.3008 - val_loss: 31.9604 - val_mae: 4.4336\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 73.7723 - mae: 6.2287 - val_loss: 31.6782 - val_mae: 4.4854\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 73.6981 - mae: 6.1243 - val_loss: 32.1609 - val_mae: 4.5832\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 71.6736 - mae: 5.8704 - val_loss: 66.4377 - val_mae: 6.7413\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 76.4634 - mae: 6.3727 - val_loss: 29.1869 - val_mae: 4.2975\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 72.1771 - mae: 5.9155 - val_loss: 27.3263 - val_mae: 4.0512\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 70.1609 - mae: 5.9080 - val_loss: 26.9893 - val_mae: 4.0936\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 69.4122 - mae: 5.8375 - val_loss: 55.0537 - val_mae: 6.0673\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 72.8021 - mae: 6.0660 - val_loss: 28.8595 - val_mae: 4.1339\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 70.5308 - mae: 5.9116 - val_loss: 25.6861 - val_mae: 3.9385\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 72.3748 - mae: 5.9409 - val_loss: 26.3118 - val_mae: 3.9774\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 71.2693 - mae: 5.9831 - val_loss: 26.0453 - val_mae: 3.9339\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 68.2547 - mae: 5.7707 - val_loss: 25.5907 - val_mae: 3.9587\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 68.4245 - mae: 5.7622 - val_loss: 30.7366 - val_mae: 4.4757\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 69.8502 - mae: 5.6837 - val_loss: 25.2856 - val_mae: 3.8830\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 69.6017 - mae: 5.8558 - val_loss: 28.3763 - val_mae: 4.2775\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 68.5381 - mae: 5.6770 - val_loss: 30.7493 - val_mae: 4.3426\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 70.3386 - mae: 5.9057 - val_loss: 24.9494 - val_mae: 3.8603\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 67.8228 - mae: 5.6864 - val_loss: 25.7724 - val_mae: 3.9328\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 70.0904 - mae: 5.8785 - val_loss: 24.6333 - val_mae: 3.8583\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 69.2902 - mae: 5.7718 - val_loss: 29.9284 - val_mae: 4.2960\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 69.2176 - mae: 5.8126 - val_loss: 26.1919 - val_mae: 3.9824\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 68.6567 - mae: 5.8956 - val_loss: 25.1319 - val_mae: 3.9646\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 68.2105 - mae: 5.7543 - val_loss: 24.7876 - val_mae: 3.8919\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 68.6603 - mae: 5.8101 - val_loss: 26.5482 - val_mae: 4.1110\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 68.7486 - mae: 5.7030 - val_loss: 29.7967 - val_mae: 4.2884\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 70.0754 - mae: 5.9919 - val_loss: 29.6892 - val_mae: 4.3835\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 69.7752 - mae: 5.7849 - val_loss: 30.5019 - val_mae: 4.3527\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 68.1297 - mae: 5.8190 - val_loss: 26.7805 - val_mae: 4.0293\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 68.2814 - mae: 5.7659 - val_loss: 24.5913 - val_mae: 3.9059\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 70.9285 - mae: 6.2844\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 33ms/step - loss: 32089.0625 - mae: 88.1216 - val_loss: 510.2418 - val_mae: 21.5672\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 593.0580 - mae: 22.3832 - val_loss: 501.9547 - val_mae: 21.4167\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 587.0963 - mae: 22.1971 - val_loss: 491.6836 - val_mae: 21.1693\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 564.8829 - mae: 21.6183 - val_loss: 397.6703 - val_mae: 18.9549\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 279.8954 - mae: 13.3626 - val_loss: 47.1274 - val_mae: 5.5879\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 100.2007 - mae: 7.6085 - val_loss: 44.3420 - val_mae: 5.3958\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 98.1558 - mae: 7.4775 - val_loss: 41.0893 - val_mae: 5.0439\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 96.5786 - mae: 7.4485 - val_loss: 43.5359 - val_mae: 5.2334\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 97.2220 - mae: 7.4290 - val_loss: 40.0317 - val_mae: 5.0482\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 95.1915 - mae: 7.3497 - val_loss: 41.1183 - val_mae: 5.0845\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 94.8157 - mae: 7.4141 - val_loss: 38.7239 - val_mae: 4.9605\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 94.6761 - mae: 7.2509 - val_loss: 39.7804 - val_mae: 5.0032\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 93.8130 - mae: 7.2114 - val_loss: 37.9967 - val_mae: 4.8435\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 92.4231 - mae: 7.2342 - val_loss: 37.4985 - val_mae: 4.8718\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 91.6775 - mae: 7.1799 - val_loss: 37.1244 - val_mae: 4.8210\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 91.6444 - mae: 7.1447 - val_loss: 39.4595 - val_mae: 5.0180\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 90.0831 - mae: 6.9850 - val_loss: 64.1859 - val_mae: 6.5621\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 93.4830 - mae: 7.3760 - val_loss: 37.3693 - val_mae: 4.9043\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 90.6416 - mae: 6.9745 - val_loss: 38.2308 - val_mae: 4.9322\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 89.1432 - mae: 7.0877 - val_loss: 35.8035 - val_mae: 4.7439\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 88.9091 - mae: 6.9790 - val_loss: 49.0056 - val_mae: 5.6513\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 89.5053 - mae: 7.0532 - val_loss: 41.6665 - val_mae: 5.1111\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 89.4455 - mae: 7.0550 - val_loss: 35.1771 - val_mae: 4.7312\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 89.9970 - mae: 6.9983 - val_loss: 37.7492 - val_mae: 4.8982\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 89.3201 - mae: 7.0853 - val_loss: 36.5827 - val_mae: 4.8330\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 87.0183 - mae: 6.9828 - val_loss: 34.7973 - val_mae: 4.7214\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 86.5420 - mae: 6.8850 - val_loss: 35.9599 - val_mae: 4.7989\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 87.5012 - mae: 6.8058 - val_loss: 35.1776 - val_mae: 4.7380\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 87.6848 - mae: 6.9813 - val_loss: 33.8240 - val_mae: 4.6127\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 85.6221 - mae: 6.8029 - val_loss: 35.7454 - val_mae: 4.7844\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 86.9797 - mae: 6.8672 - val_loss: 35.4187 - val_mae: 4.7582\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 85.5980 - mae: 6.8414 - val_loss: 35.8382 - val_mae: 4.7804\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 87.0004 - mae: 6.9352 - val_loss: 34.6672 - val_mae: 4.6928\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 85.5723 - mae: 6.8625 - val_loss: 39.9248 - val_mae: 5.0503\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 85.4729 - mae: 6.8260 - val_loss: 37.5608 - val_mae: 4.8857\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 85.5457 - mae: 6.9526 - val_loss: 32.7939 - val_mae: 4.5491\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 84.1719 - mae: 6.7854 - val_loss: 35.5242 - val_mae: 4.7508\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 84.6489 - mae: 6.8625 - val_loss: 36.9840 - val_mae: 4.8614\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 85.2500 - mae: 6.7227 - val_loss: 34.5516 - val_mae: 4.7026\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 84.8067 - mae: 6.8460 - val_loss: 35.6238 - val_mae: 4.7781\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 85.3749 - mae: 6.7742 - val_loss: 39.8312 - val_mae: 5.0999\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 83.6784 - mae: 6.8633 - val_loss: 32.2076 - val_mae: 4.5234\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 83.7518 - mae: 6.7274 - val_loss: 32.2781 - val_mae: 4.5253\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 83.6779 - mae: 6.7799 - val_loss: 32.0745 - val_mae: 4.5102\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 83.5514 - mae: 6.7468 - val_loss: 35.4127 - val_mae: 4.7802\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 83.4118 - mae: 6.7592 - val_loss: 32.0927 - val_mae: 4.5411\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 80.8257 - mae: 6.6872\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 26ms/step - loss: 50716.0039 - mae: 97.5665 - val_loss: 306.5926 - val_mae: 16.6801\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 146.5288 - mae: 9.6927 - val_loss: 46.2526 - val_mae: 5.2871\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 99.4261 - mae: 7.4123 - val_loss: 45.9547 - val_mae: 5.3950\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 98.7221 - mae: 7.4109 - val_loss: 46.6665 - val_mae: 5.6665\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 98.5741 - mae: 7.4207 - val_loss: 45.7213 - val_mae: 5.5225\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 95.6300 - mae: 7.4040 - val_loss: 47.7194 - val_mae: 5.3666\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 95.7217 - mae: 7.2898 - val_loss: 45.1374 - val_mae: 5.3182\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 95.8206 - mae: 7.2525 - val_loss: 48.1144 - val_mae: 5.9002\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 97.6992 - mae: 7.4170 - val_loss: 44.8827 - val_mae: 5.2737\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 94.9657 - mae: 7.1662 - val_loss: 46.8568 - val_mae: 5.7751\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 95.5527 - mae: 7.3768 - val_loss: 45.3028 - val_mae: 5.2648\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 95.5539 - mae: 7.2205 - val_loss: 47.6762 - val_mae: 5.8768\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 96.8713 - mae: 7.3062 - val_loss: 48.9158 - val_mae: 5.9882\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 93.7420 - mae: 7.2857 - val_loss: 44.1564 - val_mae: 5.2632\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 93.1795 - mae: 7.2294 - val_loss: 44.3155 - val_mae: 5.2636\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 93.8944 - mae: 7.2045 - val_loss: 44.7462 - val_mae: 5.2411\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 93.2757 - mae: 7.0249 - val_loss: 79.8923 - val_mae: 7.7610\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 100.2417 - mae: 8.4729\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 18ms/step - loss: 8103.4463 - mae: 82.2083 - val_loss: 7692.1426 - val_mae: 80.8122\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 8021.6895 - mae: 81.7462 - val_loss: 7615.7769 - val_mae: 80.3870\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 7941.6763 - mae: 81.2919 - val_loss: 7539.9175 - val_mae: 79.9617\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 7861.2666 - mae: 80.8249 - val_loss: 7465.3149 - val_mae: 79.5409\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7783.0469 - mae: 80.3674 - val_loss: 7389.9663 - val_mae: 79.1134\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 7703.9326 - mae: 79.9026 - val_loss: 7315.4229 - val_mae: 78.6876\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 7625.7202 - mae: 79.4476 - val_loss: 7242.2734 - val_mae: 78.2668\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 7548.6431 - mae: 78.9937 - val_loss: 7169.9785 - val_mae: 77.8481\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 7472.1479 - mae: 78.5385 - val_loss: 7098.2827 - val_mae: 77.4301\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 7397.0151 - mae: 78.0876 - val_loss: 7027.4609 - val_mae: 77.0144\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 7322.5854 - mae: 77.6384 - val_loss: 6956.5703 - val_mae: 76.5954\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 7249.4512 - mae: 77.1895 - val_loss: 6886.2349 - val_mae: 76.1768\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 7175.5415 - mae: 76.7459 - val_loss: 6818.6538 - val_mae: 75.7715\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 7104.1191 - mae: 76.3100 - val_loss: 6751.5269 - val_mae: 75.3664\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 7032.7046 - mae: 75.8662 - val_loss: 6684.2065 - val_mae: 74.9576\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 6961.4409 - mae: 75.4331 - val_loss: 6617.6714 - val_mae: 74.5508\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 6891.4814 - mae: 74.9948 - val_loss: 6551.1904 - val_mae: 74.1416\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 6822.3677 - mae: 74.5628 - val_loss: 6485.4834 - val_mae: 73.7344\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 6753.3506 - mae: 74.1256 - val_loss: 6421.2637 - val_mae: 73.3338\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 6685.6592 - mae: 73.6964 - val_loss: 6357.0527 - val_mae: 72.9304\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 6618.8320 - mae: 73.2762 - val_loss: 6293.2729 - val_mae: 72.5270\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 6550.9897 - mae: 72.8381 - val_loss: 6230.9106 - val_mae: 72.1297\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 6485.5640 - mae: 72.4177 - val_loss: 6169.3154 - val_mae: 71.7350\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 6421.4932 - mae: 71.9924 - val_loss: 6107.4209 - val_mae: 71.3360\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 6356.4233 - mae: 71.5779 - val_loss: 6047.0303 - val_mae: 70.9438\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 6292.7871 - mae: 71.1582 - val_loss: 5986.8462 - val_mae: 70.5502\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 6229.2954 - mae: 70.7442 - val_loss: 5926.8179 - val_mae: 70.1551\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 6166.3374 - mae: 70.3243 - val_loss: 5867.7754 - val_mae: 69.7639\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 6104.3242 - mae: 69.9138 - val_loss: 5808.8076 - val_mae: 69.3706\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 6042.7363 - mae: 69.5049 - val_loss: 5750.6104 - val_mae: 68.9794\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 5981.8418 - mae: 69.0928 - val_loss: 5694.5898 - val_mae: 68.6146\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5922.8730 - mae: 68.6928 - val_loss: 5637.3521 - val_mae: 68.2433\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 5862.9106 - mae: 68.2803 - val_loss: 5581.8325 - val_mae: 67.8800\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 5805.0171 - mae: 67.8776 - val_loss: 5526.3154 - val_mae: 67.5144\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 5746.5767 - mae: 67.4773 - val_loss: 5471.4092 - val_mae: 67.1502\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 5689.0181 - mae: 67.0835 - val_loss: 5417.2734 - val_mae: 66.7884\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5632.3486 - mae: 66.6876 - val_loss: 5363.8774 - val_mae: 66.4290\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5576.3877 - mae: 66.2876 - val_loss: 5310.4800 - val_mae: 66.0672\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5520.1353 - mae: 65.8967 - val_loss: 5258.0532 - val_mae: 65.7093\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5464.8340 - mae: 65.5073 - val_loss: 5206.6860 - val_mae: 65.3558\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5411.5615 - mae: 65.1314 - val_loss: 5155.2373 - val_mae: 64.9994\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5358.0903 - mae: 64.7425 - val_loss: 5104.1113 - val_mae: 64.6428\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 5304.3896 - mae: 64.3595 - val_loss: 5054.6724 - val_mae: 64.2951\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 5251.9517 - mae: 63.9775 - val_loss: 5005.0928 - val_mae: 63.9442\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5200.9316 - mae: 63.6039 - val_loss: 4955.5356 - val_mae: 63.5908\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 5149.4146 - mae: 63.2257 - val_loss: 4907.6069 - val_mae: 63.2465\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 5099.1807 - mae: 62.8621 - val_loss: 4860.5532 - val_mae: 62.9059\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 5049.4980 - mae: 62.4852 - val_loss: 4813.2036 - val_mae: 62.5612\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 5000.6943 - mae: 62.1242 - val_loss: 4766.9380 - val_mae: 62.2216\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4951.7471 - mae: 61.7653 - val_loss: 4721.0444 - val_mae: 61.8821\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 4903.7119 - mae: 61.4071 - val_loss: 4675.5830 - val_mae: 61.5435\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 4856.6387 - mae: 61.0535 - val_loss: 4629.6660 - val_mae: 61.1991\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4808.8442 - mae: 60.6978 - val_loss: 4584.7856 - val_mae: 60.8601\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 4762.0439 - mae: 60.3465 - val_loss: 4540.3394 - val_mae: 60.5217\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4714.9248 - mae: 59.9880 - val_loss: 4496.1743 - val_mae: 60.1834\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4669.4141 - mae: 59.6381 - val_loss: 4452.0425 - val_mae: 59.8428\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4623.4922 - mae: 59.2959 - val_loss: 4409.2852 - val_mae: 59.5099\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4578.5483 - mae: 58.9529 - val_loss: 4365.8545 - val_mae: 59.1701\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4533.3975 - mae: 58.6126 - val_loss: 4323.7622 - val_mae: 58.8601\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 4489.7495 - mae: 58.2875 - val_loss: 4281.4971 - val_mae: 58.5497\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4445.6323 - mae: 57.9594 - val_loss: 4240.2266 - val_mae: 58.2444\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4403.0566 - mae: 57.6525 - val_loss: 4199.3184 - val_mae: 57.9394\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4360.3252 - mae: 57.3472 - val_loss: 4159.4795 - val_mae: 57.6399\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 4319.0425 - mae: 57.0482 - val_loss: 4119.8008 - val_mae: 57.3396\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 4278.1948 - mae: 56.7572 - val_loss: 4080.3994 - val_mae: 57.0392\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4237.2402 - mae: 56.4625 - val_loss: 4041.6631 - val_mae: 56.7419\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 4196.4736 - mae: 56.1744 - val_loss: 4003.0774 - val_mae: 56.4534\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4156.4946 - mae: 55.8889 - val_loss: 3964.5352 - val_mae: 56.1701\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4116.3638 - mae: 55.6049 - val_loss: 3926.3259 - val_mae: 55.8869\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 4076.4419 - mae: 55.3338 - val_loss: 3889.5325 - val_mae: 55.6206\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 4038.3667 - mae: 55.0652 - val_loss: 3852.4048 - val_mae: 55.3617\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 4000.7993 - mae: 54.8034 - val_loss: 3815.7500 - val_mae: 55.1041\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3963.0671 - mae: 54.5388 - val_loss: 3780.0359 - val_mae: 54.8514\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3925.8650 - mae: 54.2789 - val_loss: 3745.1943 - val_mae: 54.6025\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3889.3330 - mae: 54.0222 - val_loss: 3710.2019 - val_mae: 54.3508\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3853.0920 - mae: 53.7643 - val_loss: 3675.5850 - val_mae: 54.1002\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3817.5740 - mae: 53.5193 - val_loss: 3641.2173 - val_mae: 53.8488\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3781.8647 - mae: 53.2766 - val_loss: 3607.0342 - val_mae: 53.6125\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3746.2937 - mae: 53.0313 - val_loss: 3573.8711 - val_mae: 53.3856\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3711.8496 - mae: 52.7957 - val_loss: 3540.9497 - val_mae: 53.1577\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3677.6687 - mae: 52.5653 - val_loss: 3508.1921 - val_mae: 52.9296\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3643.7834 - mae: 52.3214 - val_loss: 3475.4707 - val_mae: 52.6999\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3609.9009 - mae: 52.0911 - val_loss: 3443.3281 - val_mae: 52.4731\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3576.6440 - mae: 51.8607 - val_loss: 3411.3503 - val_mae: 52.2453\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3543.4211 - mae: 51.6357 - val_loss: 3379.6516 - val_mae: 52.0169\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3511.0117 - mae: 51.4012 - val_loss: 3348.3225 - val_mae: 51.7894\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3478.7634 - mae: 51.1813 - val_loss: 3317.7522 - val_mae: 51.5666\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3447.0896 - mae: 50.9517 - val_loss: 3287.1936 - val_mae: 51.3411\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3415.3364 - mae: 50.7319 - val_loss: 3256.8330 - val_mae: 51.1161\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3383.8826 - mae: 50.5052 - val_loss: 3227.3357 - val_mae: 50.8942\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3353.7087 - mae: 50.2936 - val_loss: 3197.6514 - val_mae: 50.6702\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3323.2117 - mae: 50.0694 - val_loss: 3168.3777 - val_mae: 50.4467\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3293.4822 - mae: 49.8699 - val_loss: 3139.6333 - val_mae: 50.2434\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3263.7346 - mae: 49.6634 - val_loss: 3111.2480 - val_mae: 50.0416\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3234.9380 - mae: 49.4602 - val_loss: 3083.7222 - val_mae: 49.8425\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3205.8127 - mae: 49.2577 - val_loss: 3056.7603 - val_mae: 49.6463\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 3178.4348 - mae: 49.0670 - val_loss: 3028.4463 - val_mae: 49.4395\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 3148.8120 - mae: 48.8585 - val_loss: 3001.1235 - val_mae: 49.2386\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3120.3718 - mae: 48.6634 - val_loss: 2974.2764 - val_mae: 49.0392\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 3092.6135 - mae: 48.4636 - val_loss: 2947.6189 - val_mae: 48.8385\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 3027.0105 - mae: 46.6371\n"
     ]
    }
   ],
   "source": [
    "opt = [\"sgd\", \"nesterov\", \"momentum\", \"adam\"]\n",
    "name = \"opt\"\n",
    "results_opt= []\n",
    "for value in opt:\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_dir(name, value))\n",
    "    model = build_model(1, 25, value, 10**(-5), 0.5)\n",
    "    model.fit(X_train, y_train, epochs=100, validation_split=0.1, callbacks=[tensorboard_cb, es_cb])\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    results_opt.append((value, score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae5ccc4-8df3-4659-996c-50d8be44c115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sgd', 70.92853546142578, 6.284383296966553),\n",
       " ('nesterov', 80.82572937011719, 6.687215805053711),\n",
       " ('momentum', 100.24166107177734, 8.472890853881836),\n",
       " ('adam', 3027.010498046875, 46.63709259033203)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('opt.pkl', 'wb') as fp:\n",
    "    pickle.dump(results_opt, fp)\n",
    "    \n",
    "results_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd13cd83-2963-4b81-8c08-7bfb3291c2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6480), started 4:32:35 ago. (Use '!kill 6480' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2a7b30b7729e926d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2a7b30b7729e926d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40925363-07e7-4d22-8804-d20e9288b8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 26ms/step - loss: 12535.2217 - mae: 63.7885 - val_loss: 497.5861 - val_mae: 21.2019\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 588.1895 - mae: 22.0781 - val_loss: 495.5449 - val_mae: 21.0787\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 586.9178 - mae: 21.9877 - val_loss: 494.1761 - val_mae: 20.9802\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 586.0182 - mae: 21.9478 - val_loss: 492.1589 - val_mae: 20.8182\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 584.9926 - mae: 21.9250 - val_loss: 490.8977 - val_mae: 20.6986\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 584.4433 - mae: 21.9150 - val_loss: 490.0975 - val_mae: 20.6194\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 583.9919 - mae: 21.9117 - val_loss: 489.3862 - val_mae: 20.5756\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 583.6206 - mae: 21.9126 - val_loss: 489.0661 - val_mae: 20.5702\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 583.3164 - mae: 21.9055 - val_loss: 488.8165 - val_mae: 20.5650\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 583.0252 - mae: 21.8991 - val_loss: 488.5768 - val_mae: 20.5596\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 582.7800 - mae: 21.8912 - val_loss: 488.3112 - val_mae: 20.5543\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 582.4779 - mae: 21.8864 - val_loss: 488.0701 - val_mae: 20.5489\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 582.1805 - mae: 21.8724 - val_loss: 487.7772 - val_mae: 20.5437\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 581.9386 - mae: 21.8724 - val_loss: 487.5370 - val_mae: 20.5384\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 581.6143 - mae: 21.8594 - val_loss: 487.2927 - val_mae: 20.5330\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 581.3373 - mae: 21.8530 - val_loss: 487.0591 - val_mae: 20.5277\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 581.0862 - mae: 21.8455 - val_loss: 486.8179 - val_mae: 20.5224\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 580.7765 - mae: 21.8355 - val_loss: 486.5709 - val_mae: 20.5171\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 580.5133 - mae: 21.8279 - val_loss: 486.3240 - val_mae: 20.5118\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 580.2316 - mae: 21.8231 - val_loss: 486.1488 - val_mae: 20.5065\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 579.9555 - mae: 21.8091 - val_loss: 485.8401 - val_mae: 20.5012\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 579.7058 - mae: 21.8094 - val_loss: 485.5688 - val_mae: 20.4959\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 579.4471 - mae: 21.8038 - val_loss: 485.3392 - val_mae: 20.4907\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 579.1616 - mae: 21.7942 - val_loss: 485.1086 - val_mae: 20.4854\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 578.8954 - mae: 21.7880 - val_loss: 484.8727 - val_mae: 20.4801\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 578.6202 - mae: 21.7808 - val_loss: 484.6518 - val_mae: 20.4748\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 578.3392 - mae: 21.7705 - val_loss: 484.3945 - val_mae: 20.4696\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 578.0878 - mae: 21.7647 - val_loss: 484.1519 - val_mae: 20.4643\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 577.8286 - mae: 21.7608 - val_loss: 483.8993 - val_mae: 20.4591\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 577.5251 - mae: 21.7507 - val_loss: 483.7188 - val_mae: 20.4538\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 577.2834 - mae: 21.7418 - val_loss: 483.4576 - val_mae: 20.4485\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 577.0042 - mae: 21.7371 - val_loss: 483.2124 - val_mae: 20.4433\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 576.7387 - mae: 21.7278 - val_loss: 482.9197 - val_mae: 20.4381\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 576.4683 - mae: 21.7256 - val_loss: 482.6648 - val_mae: 20.4328\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 576.1595 - mae: 21.7174 - val_loss: 482.4203 - val_mae: 20.4275\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 575.8799 - mae: 21.7089 - val_loss: 482.1736 - val_mae: 20.4223\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 575.6096 - mae: 21.7035 - val_loss: 481.9457 - val_mae: 20.4170\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 575.3182 - mae: 21.6962 - val_loss: 481.7228 - val_mae: 20.4118\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 575.0386 - mae: 21.6866 - val_loss: 481.5273 - val_mae: 20.4065\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 574.7797 - mae: 21.6779 - val_loss: 481.3174 - val_mae: 20.4013\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 574.4854 - mae: 21.6671 - val_loss: 481.0723 - val_mae: 20.3960\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 574.1994 - mae: 21.6589 - val_loss: 480.8347 - val_mae: 20.3908\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 573.9174 - mae: 21.6540 - val_loss: 480.5864 - val_mae: 20.3856\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 573.6403 - mae: 21.6458 - val_loss: 480.3485 - val_mae: 20.3803\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 573.3481 - mae: 21.6381 - val_loss: 480.1037 - val_mae: 20.3749\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 573.0374 - mae: 21.6320 - val_loss: 479.9063 - val_mae: 20.3694\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 572.7501 - mae: 21.6174 - val_loss: 479.5759 - val_mae: 20.3607\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 572.4710 - mae: 21.6125 - val_loss: 479.2185 - val_mae: 20.3515\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 572.1807 - mae: 21.6023 - val_loss: 478.8216 - val_mae: 20.3413\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 571.9167 - mae: 21.5962 - val_loss: 478.5246 - val_mae: 20.3335\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 571.6229 - mae: 21.5854 - val_loss: 478.0954 - val_mae: 20.3224\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 571.3475 - mae: 21.5790 - val_loss: 477.7367 - val_mae: 20.3129\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 571.0719 - mae: 21.5734 - val_loss: 477.3683 - val_mae: 20.3027\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 570.7458 - mae: 21.5632 - val_loss: 476.8150 - val_mae: 20.2867\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 570.3820 - mae: 21.5526 - val_loss: 476.3622 - val_mae: 20.2734\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 570.0508 - mae: 21.5427 - val_loss: 475.9709 - val_mae: 20.2619\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 569.6978 - mae: 21.5380 - val_loss: 475.6024 - val_mae: 20.2511\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 569.3560 - mae: 21.5254 - val_loss: 475.1180 - val_mae: 20.2353\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 568.9520 - mae: 21.5100 - val_loss: 474.7372 - val_mae: 20.2234\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 568.3798 - mae: 21.4934 - val_loss: 473.8056 - val_mae: 20.1902\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 567.3350 - mae: 21.4731 - val_loss: 471.9432 - val_mae: 20.1610\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 557.2168 - mae: 21.1856 - val_loss: 456.5605 - val_mae: 19.7712\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 531.1776 - mae: 20.3726 - val_loss: 439.5139 - val_mae: 19.0380\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 466.3387 - mae: 18.6652 - val_loss: 318.9495 - val_mae: 15.7501\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 303.8751 - mae: 14.6451 - val_loss: 216.7819 - val_mae: 12.9158\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 237.9765 - mae: 12.8263 - val_loss: 175.7146 - val_mae: 11.3646\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 205.8527 - mae: 11.6429 - val_loss: 146.4790 - val_mae: 10.4219\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 192.6983 - mae: 11.1060 - val_loss: 149.5749 - val_mae: 10.1261\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 173.7790 - mae: 10.4503 - val_loss: 115.8122 - val_mae: 9.2762\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 158.3434 - mae: 9.8209 - val_loss: 106.5710 - val_mae: 8.8670\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 156.4245 - mae: 9.7601 - val_loss: 120.2124 - val_mae: 9.2567\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 147.1992 - mae: 9.2698 - val_loss: 102.9628 - val_mae: 8.5465\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 144.4227 - mae: 9.1488 - val_loss: 82.1630 - val_mae: 7.7926\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 130.7257 - mae: 8.6466 - val_loss: 76.2168 - val_mae: 7.4800\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 126.7928 - mae: 8.4615 - val_loss: 70.1481 - val_mae: 7.1032\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 120.5346 - mae: 8.1117 - val_loss: 79.0474 - val_mae: 6.9199\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 114.1851 - mae: 7.8747 - val_loss: 59.8211 - val_mae: 6.5603\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 107.5872 - mae: 7.5409 - val_loss: 55.2688 - val_mae: 6.2806\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 104.2399 - mae: 7.3814 - val_loss: 50.9036 - val_mae: 6.0292\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 99.3725 - mae: 7.1589 - val_loss: 47.9095 - val_mae: 5.7019\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 95.6070 - mae: 6.9018 - val_loss: 53.0328 - val_mae: 5.6881\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 92.4760 - mae: 6.8300 - val_loss: 41.4531 - val_mae: 5.2966\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 89.5897 - mae: 6.6844 - val_loss: 39.9910 - val_mae: 5.2268\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 14ms/step - loss: 86.2588 - mae: 6.5960 - val_loss: 37.9037 - val_mae: 4.9356\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 84.9211 - mae: 6.5881 - val_loss: 39.8124 - val_mae: 4.9563\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 84.1543 - mae: 6.7249 - val_loss: 41.9012 - val_mae: 5.2222\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 21ms/step - loss: 82.1230 - mae: 6.5809 - val_loss: 35.8639 - val_mae: 4.7166\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 81.1991 - mae: 6.6147 - val_loss: 36.8196 - val_mae: 4.8404\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 80.5484 - mae: 6.4534 - val_loss: 36.0530 - val_mae: 4.6686\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 80.8944 - mae: 6.5618 - val_loss: 35.6971 - val_mae: 4.6916\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 79.4794 - mae: 6.4100 - val_loss: 39.1367 - val_mae: 4.8328\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 80.1814 - mae: 6.5591 - val_loss: 41.3232 - val_mae: 4.9671\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 80.1810 - mae: 6.7072 - val_loss: 35.7782 - val_mae: 4.7091\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 80.3425 - mae: 6.5464 - val_loss: 36.4649 - val_mae: 4.7948\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 79.9283 - mae: 6.4885 - val_loss: 36.6424 - val_mae: 4.6712\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 81.3066 - mae: 6.6215 - val_loss: 35.6301 - val_mae: 4.6714\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 80.1264 - mae: 6.5520 - val_loss: 35.7622 - val_mae: 4.6670\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 78.1202 - mae: 6.5607\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 24ms/step - loss: 16307.6113 - mae: 67.4206 - val_loss: 498.4810 - val_mae: 21.1578\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 12ms/step - loss: 588.4062 - mae: 22.1588 - val_loss: 490.8185 - val_mae: 20.7409\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 16ms/step - loss: 584.8276 - mae: 21.9960 - val_loss: 489.8786 - val_mae: 20.6871\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 583.3917 - mae: 21.9197 - val_loss: 489.4650 - val_mae: 20.6530\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 582.3431 - mae: 21.8660 - val_loss: 489.1844 - val_mae: 20.6465\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 581.5167 - mae: 21.8268 - val_loss: 488.7043 - val_mae: 20.6422\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 580.5702 - mae: 21.7937 - val_loss: 487.6899 - val_mae: 20.6112\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 579.7556 - mae: 21.7629 - val_loss: 486.7077 - val_mae: 20.5794\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 578.7714 - mae: 21.7203 - val_loss: 485.7128 - val_mae: 20.5796\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 571.8472 - mae: 21.4899 - val_loss: 461.0735 - val_mae: 19.8008\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 470.3949 - mae: 18.7072 - val_loss: 241.2137 - val_mae: 13.7536\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 251.3752 - mae: 12.8999 - val_loss: 158.9714 - val_mae: 10.9192\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 194.4498 - mae: 11.1576 - val_loss: 121.9680 - val_mae: 9.6092\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 160.8724 - mae: 9.9527 - val_loss: 99.6697 - val_mae: 8.5252\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 142.6926 - mae: 9.3330 - val_loss: 85.4998 - val_mae: 7.8362\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 133.0579 - mae: 8.6926 - val_loss: 73.1433 - val_mae: 7.1970\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 117.0784 - mae: 8.0141 - val_loss: 112.6020 - val_mae: 8.2218\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 119.4189 - mae: 8.2664 - val_loss: 68.7646 - val_mae: 6.8122\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 107.2552 - mae: 7.5121 - val_loss: 49.9244 - val_mae: 5.7304\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 97.4586 - mae: 7.1777 - val_loss: 46.3706 - val_mae: 5.5224\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 95.9188 - mae: 7.1858 - val_loss: 53.8775 - val_mae: 5.8332\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 95.2747 - mae: 7.2853 - val_loss: 45.3915 - val_mae: 5.2922\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 94.1375 - mae: 7.2335 - val_loss: 48.6144 - val_mae: 5.6199\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 17ms/step - loss: 94.0530 - mae: 7.1410 - val_loss: 44.6923 - val_mae: 5.1588\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 92.6533 - mae: 7.2199 - val_loss: 44.0443 - val_mae: 5.1598\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 89.1032 - mae: 7.0772 - val_loss: 42.8601 - val_mae: 5.1983\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 90.1145 - mae: 7.1668 - val_loss: 45.1491 - val_mae: 5.3668\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 91.5647 - mae: 7.0635 - val_loss: 43.5167 - val_mae: 5.1347\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 89.8798 - mae: 7.1568 - val_loss: 43.5305 - val_mae: 5.2398\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 88.4406 - mae: 7.0166 - val_loss: 42.4144 - val_mae: 5.1841\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 90.2831 - mae: 7.0776 - val_loss: 42.7052 - val_mae: 5.1229\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 88.2761 - mae: 7.0564 - val_loss: 42.2226 - val_mae: 5.1362\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 89.3449 - mae: 7.0916 - val_loss: 42.3232 - val_mae: 5.1139\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 88.1943 - mae: 7.0240 - val_loss: 43.4666 - val_mae: 5.0775\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 88.3228 - mae: 6.9718 - val_loss: 48.0849 - val_mae: 5.2987\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 90.6267 - mae: 7.2780 - val_loss: 43.7630 - val_mae: 5.2548\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 88.1410 - mae: 6.9525\n",
      "Epoch 1/100\n",
      "12/12 [==============================] - 1s 17ms/step - loss: 79541.0938 - mae: 149.9240 - val_loss: 501.0468 - val_mae: 21.4095\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 588.7161 - mae: 22.3491 - val_loss: 497.8142 - val_mae: 21.3339\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 585.7469 - mae: 22.2820 - val_loss: 495.2763 - val_mae: 21.2743\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 583.2094 - mae: 22.2249 - val_loss: 492.9492 - val_mae: 21.2196\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 580.8026 - mae: 22.1708 - val_loss: 490.6732 - val_mae: 21.1659\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 578.4484 - mae: 22.1170 - val_loss: 488.3997 - val_mae: 21.1121\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 576.0736 - mae: 22.0639 - val_loss: 486.1793 - val_mae: 21.0594\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 573.7408 - mae: 22.0112 - val_loss: 483.9621 - val_mae: 21.0067\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 571.4161 - mae: 21.9584 - val_loss: 481.7459 - val_mae: 20.9539\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 569.1174 - mae: 21.9057 - val_loss: 479.5481 - val_mae: 20.9014\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 566.8244 - mae: 21.8529 - val_loss: 477.3343 - val_mae: 20.8484\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 564.5182 - mae: 21.8001 - val_loss: 475.1476 - val_mae: 20.7959\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 562.2251 - mae: 21.7478 - val_loss: 472.9900 - val_mae: 20.7439\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 559.9802 - mae: 21.6958 - val_loss: 470.8297 - val_mae: 20.6918\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 557.7203 - mae: 21.6438 - val_loss: 468.6777 - val_mae: 20.6397\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 555.4662 - mae: 21.5919 - val_loss: 466.5443 - val_mae: 20.5880\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 553.2267 - mae: 21.5403 - val_loss: 464.4335 - val_mae: 20.5366\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 551.0083 - mae: 21.4885 - val_loss: 462.2902 - val_mae: 20.4844\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 548.7897 - mae: 21.4365 - val_loss: 460.1823 - val_mae: 20.4329\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 546.5816 - mae: 21.3849 - val_loss: 458.0754 - val_mae: 20.3813\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 544.3861 - mae: 21.3335 - val_loss: 455.9882 - val_mae: 20.3300\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 542.1744 - mae: 21.2823 - val_loss: 453.9049 - val_mae: 20.2787\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 540.0046 - mae: 21.2310 - val_loss: 451.8263 - val_mae: 20.2274\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 537.8505 - mae: 21.1796 - val_loss: 449.7601 - val_mae: 20.1762\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 535.6852 - mae: 21.1287 - val_loss: 447.7121 - val_mae: 20.1254\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 533.5449 - mae: 21.0779 - val_loss: 445.6673 - val_mae: 20.0746\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 531.4046 - mae: 21.0273 - val_loss: 443.6435 - val_mae: 20.0241\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 529.2903 - mae: 20.9769 - val_loss: 441.6370 - val_mae: 19.9739\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 527.1882 - mae: 20.9264 - val_loss: 439.6161 - val_mae: 19.9233\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 525.0653 - mae: 20.8763 - val_loss: 437.6335 - val_mae: 19.8734\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 522.9816 - mae: 20.8265 - val_loss: 435.6619 - val_mae: 19.8238\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 520.9122 - mae: 20.7766 - val_loss: 433.6743 - val_mae: 19.7736\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 518.8342 - mae: 20.7265 - val_loss: 431.7054 - val_mae: 19.7237\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 516.7787 - mae: 20.6766 - val_loss: 429.7379 - val_mae: 19.6738\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 514.7118 - mae: 20.6265 - val_loss: 427.7728 - val_mae: 19.6238\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 512.6651 - mae: 20.5767 - val_loss: 425.8162 - val_mae: 19.5739\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 510.6184 - mae: 20.5271 - val_loss: 423.8956 - val_mae: 19.5247\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 508.6059 - mae: 20.4778 - val_loss: 421.9672 - val_mae: 19.4753\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 506.5856 - mae: 20.4287 - val_loss: 420.0702 - val_mae: 19.4265\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 26ms/step - loss: 504.5936 - mae: 20.3798 - val_loss: 418.1683 - val_mae: 19.3775\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 502.6172 - mae: 20.3308 - val_loss: 416.2791 - val_mae: 19.3287\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 500.6374 - mae: 20.2817 - val_loss: 414.3762 - val_mae: 19.2794\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 498.6353 - mae: 20.2331 - val_loss: 412.5137 - val_mae: 19.2311\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 496.6658 - mae: 20.1848 - val_loss: 410.6635 - val_mae: 19.1829\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 494.7430 - mae: 20.1362 - val_loss: 408.7953 - val_mae: 19.1341\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 492.7908 - mae: 20.0877 - val_loss: 406.9476 - val_mae: 19.0858\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 490.8618 - mae: 20.0395 - val_loss: 405.1182 - val_mae: 19.0378\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 488.9225 - mae: 19.9917 - val_loss: 403.3033 - val_mae: 18.9901\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 487.0329 - mae: 19.9439 - val_loss: 401.4946 - val_mae: 18.9424\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 485.1200 - mae: 19.8965 - val_loss: 399.7082 - val_mae: 18.8952\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 483.2416 - mae: 19.8490 - val_loss: 397.9081 - val_mae: 18.8475\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 481.3475 - mae: 19.8014 - val_loss: 396.1146 - val_mae: 18.7999\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 479.4782 - mae: 19.7536 - val_loss: 394.3197 - val_mae: 18.7521\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 477.5933 - mae: 19.7061 - val_loss: 392.5447 - val_mae: 18.7047\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 475.7316 - mae: 19.6584 - val_loss: 390.7639 - val_mae: 18.6570\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 473.8659 - mae: 19.6109 - val_loss: 389.0022 - val_mae: 18.6097\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 472.0152 - mae: 19.5639 - val_loss: 387.2577 - val_mae: 18.5628\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 470.1808 - mae: 19.5168 - val_loss: 385.5089 - val_mae: 18.5156\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 468.3383 - mae: 19.4700 - val_loss: 383.7897 - val_mae: 18.4692\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 466.5322 - mae: 19.4233 - val_loss: 382.0584 - val_mae: 18.4222\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 464.7113 - mae: 19.3765 - val_loss: 380.3418 - val_mae: 18.3756\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 0s 15ms/step - loss: 462.9096 - mae: 19.3298 - val_loss: 378.6317 - val_mae: 18.3290\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 461.1136 - mae: 19.2836 - val_loss: 376.9428 - val_mae: 18.2829\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 459.3428 - mae: 19.2371 - val_loss: 375.2397 - val_mae: 18.2362\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 457.5638 - mae: 19.1907 - val_loss: 373.5581 - val_mae: 18.1901\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 455.7918 - mae: 19.1449 - val_loss: 371.9007 - val_mae: 18.1444\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 454.0243 - mae: 19.0992 - val_loss: 370.2454 - val_mae: 18.0988\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 452.2900 - mae: 19.0533 - val_loss: 368.5766 - val_mae: 18.0526\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 450.5355 - mae: 19.0071 - val_loss: 366.9133 - val_mae: 18.0065\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 448.7733 - mae: 18.9614 - val_loss: 365.2780 - val_mae: 17.9610\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 447.0584 - mae: 18.9157 - val_loss: 363.6368 - val_mae: 17.9153\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 445.3510 - mae: 18.8699 - val_loss: 362.0089 - val_mae: 17.8698\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 443.6466 - mae: 18.8247 - val_loss: 360.3960 - val_mae: 17.8246\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 441.9467 - mae: 18.7795 - val_loss: 358.7880 - val_mae: 17.7794\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 440.2539 - mae: 18.7345 - val_loss: 357.1934 - val_mae: 17.7345\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 438.5736 - mae: 18.6897 - val_loss: 355.6093 - val_mae: 17.6898\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 436.9077 - mae: 18.6448 - val_loss: 354.0166 - val_mae: 17.6447\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 435.2356 - mae: 18.5998 - val_loss: 352.4421 - val_mae: 17.6001\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 433.5590 - mae: 18.5554 - val_loss: 350.8875 - val_mae: 17.5559\n",
      "Epoch 80/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 431.9244 - mae: 18.5112 - val_loss: 349.3339 - val_mae: 17.5115\n",
      "Epoch 81/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 430.2803 - mae: 18.4669 - val_loss: 347.7838 - val_mae: 17.4672\n",
      "Epoch 82/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 428.6430 - mae: 18.4223 - val_loss: 346.2306 - val_mae: 17.4227\n",
      "Epoch 83/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 427.0163 - mae: 18.3779 - val_loss: 344.6833 - val_mae: 17.3783\n",
      "Epoch 84/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 425.3864 - mae: 18.3337 - val_loss: 343.1564 - val_mae: 17.3343\n",
      "Epoch 85/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 423.7760 - mae: 18.2897 - val_loss: 341.6322 - val_mae: 17.2902\n",
      "Epoch 86/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 422.1743 - mae: 18.2455 - val_loss: 340.1041 - val_mae: 17.2460\n",
      "Epoch 87/100\n",
      "12/12 [==============================] - 0s 13ms/step - loss: 420.5659 - mae: 18.2018 - val_loss: 338.6100 - val_mae: 17.2026\n",
      "Epoch 88/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 418.9892 - mae: 18.1582 - val_loss: 337.1096 - val_mae: 17.1590\n",
      "Epoch 89/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 417.4036 - mae: 18.1146 - val_loss: 335.6208 - val_mae: 17.1155\n",
      "Epoch 90/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 415.8278 - mae: 18.0715 - val_loss: 334.1492 - val_mae: 17.0725\n",
      "Epoch 91/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 414.2799 - mae: 18.0283 - val_loss: 332.6784 - val_mae: 17.0294\n",
      "Epoch 92/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 412.7261 - mae: 17.9850 - val_loss: 331.2046 - val_mae: 16.9860\n",
      "Epoch 93/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 411.1767 - mae: 17.9415 - val_loss: 329.7238 - val_mae: 16.9424\n",
      "Epoch 94/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 409.6146 - mae: 17.8983 - val_loss: 328.2690 - val_mae: 16.8994\n",
      "Epoch 95/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 408.0894 - mae: 17.8554 - val_loss: 326.8336 - val_mae: 16.8569\n",
      "Epoch 96/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 406.5615 - mae: 17.8130 - val_loss: 325.4042 - val_mae: 16.8144\n",
      "Epoch 97/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 405.0558 - mae: 17.7703 - val_loss: 323.9662 - val_mae: 16.7716\n",
      "Epoch 98/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 403.5304 - mae: 17.7276 - val_loss: 322.5357 - val_mae: 16.7289\n",
      "Epoch 99/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 402.0036 - mae: 17.6849 - val_loss: 321.1118 - val_mae: 16.6863\n",
      "Epoch 100/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 400.5088 - mae: 17.6424 - val_loss: 319.6970 - val_mae: 16.6438\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 414.5843 - mae: 18.2028\n"
     ]
    }
   ],
   "source": [
    "mom = [0.1, 0.5, 0.9]\n",
    "name = \"mom\"\n",
    "results_mom = []\n",
    "for value in mom:\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(get_dir(name, value))\n",
    "    model = build_model(1, 25, \"momentum\", 10**(-5), value)\n",
    "    model.fit(X_train, y_train, epochs=100, validation_split=0.1, callbacks=[tensorboard_cb, es_cb])\n",
    "    score = model.evaluate(X_test, y_test)\n",
    "    results_mom.append((value, score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fcef011-2b1d-431b-911a-7981b37ae46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1, 78.12017822265625, 6.560732364654541),\n",
       " (0.5, 88.1409912109375, 6.952456474304199),\n",
       " (0.9, 414.5842590332031, 18.202756881713867)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('mom.pkl', 'wb') as fp:\n",
    "    pickle.dump(results_mom, fp)\n",
    "    \n",
    "results_mom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a69af6d-fadf-45a4-819d-b589047e6c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6480), started 4:33:02 ago. (Use '!kill 6480' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2fa7420ecc5e30c8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2fa7420ecc5e30c8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf01c84c-5578-4881-91fd-e29e3c579f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "    \"model__n_hidden\": [0, 1, 2, 3],\n",
    "    \"model__n_neurons\": [5, 25, 125],\n",
    "    \"model__learning_rate\": [10**-6, 10**-5, 10**-4],\n",
    "    \"model__optimizer\": [\"sgd\", \"nesterov\", \"momentum\", \"adam\"],\n",
    "    \"model__momentum\": [0.1, 0.5, 0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0340af0f-133f-4dc5-99c0-1deb7470ddd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikeras\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=10, min_delta=1.0, verbose=1)\n",
    "\n",
    "keras_reg = KerasRegressor(build_model, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca7e3df8-37ae-4828-bef3-c1c55ce8e108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 32ms/step - loss: 20131.1855 - mae: 72.9395 - val_loss: 445.7802 - val_mae: 20.1908\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 592.9150 - mae: 22.5755 - val_loss: 441.5637 - val_mae: 20.0861\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 588.0016 - mae: 22.4655 - val_loss: 437.0992 - val_mae: 19.9746\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 583.0701 - mae: 22.3559 - val_loss: 432.8120 - val_mae: 19.8670\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 578.1187 - mae: 22.2449 - val_loss: 428.2100 - val_mae: 19.7509\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 572.9350 - mae: 22.1268 - val_loss: 423.8127 - val_mae: 19.6392\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 567.9205 - mae: 22.0140 - val_loss: 419.0535 - val_mae: 19.5177\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 562.4857 - mae: 21.8916 - val_loss: 414.2456 - val_mae: 19.3941\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 557.1525 - mae: 21.7686 - val_loss: 409.2201 - val_mae: 19.2641\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 551.2503 - mae: 21.6321 - val_loss: 403.8775 - val_mae: 19.1250\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 545.3131 - mae: 21.4956 - val_loss: 398.5969 - val_mae: 18.9864\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 539.0320 - mae: 21.3487 - val_loss: 392.7108 - val_mae: 18.8308\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 532.3489 - mae: 21.1921 - val_loss: 386.8061 - val_mae: 18.6733\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 525.3893 - mae: 21.0261 - val_loss: 380.2532 - val_mae: 18.4970\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 517.8921 - mae: 20.8467 - val_loss: 373.5998 - val_mae: 18.3163\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 510.0500 - mae: 20.6564 - val_loss: 366.2466 - val_mae: 18.1145\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 501.6808 - mae: 20.4560 - val_loss: 358.7336 - val_mae: 17.9059\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 492.7094 - mae: 20.2356 - val_loss: 350.3540 - val_mae: 17.6703\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 482.9151 - mae: 19.9958 - val_loss: 341.2065 - val_mae: 17.4096\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 472.5258 - mae: 19.7316 - val_loss: 331.6724 - val_mae: 17.1336\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 460.9453 - mae: 19.4334 - val_loss: 321.3820 - val_mae: 16.8306\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 448.7924 - mae: 19.1188 - val_loss: 310.0309 - val_mae: 16.4899\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 435.2712 - mae: 18.7665 - val_loss: 297.9505 - val_mae: 16.1195\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 420.9125 - mae: 18.3773 - val_loss: 284.4465 - val_mae: 15.6950\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 404.8930 - mae: 17.9306 - val_loss: 270.2316 - val_mae: 15.2354\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 387.8791 - mae: 17.4550 - val_loss: 254.6559 - val_mae: 14.7154\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 369.4156 - mae: 16.9347 - val_loss: 238.4114 - val_mae: 14.1527\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 349.6540 - mae: 16.3426 - val_loss: 220.7299 - val_mae: 13.5136\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 328.5627 - mae: 15.7017 - val_loss: 202.3463 - val_mae: 12.8496\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 306.1494 - mae: 14.9771 - val_loss: 183.2092 - val_mae: 12.1369\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 283.1190 - mae: 14.2210 - val_loss: 164.0756 - val_mae: 11.3755\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 259.6402 - mae: 13.4370 - val_loss: 144.1327 - val_mae: 10.5174\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 235.3455 - mae: 12.5537 - val_loss: 125.0056 - val_mae: 9.6559\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 211.6387 - mae: 11.6752 - val_loss: 106.6943 - val_mae: 8.7843\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 188.9802 - mae: 10.7743 - val_loss: 90.2651 - val_mae: 7.9938\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 168.1542 - mae: 9.8986 - val_loss: 76.0753 - val_mae: 7.3316\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 149.8564 - mae: 9.1326 - val_loss: 64.3287 - val_mae: 6.7532\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 134.1149 - mae: 8.4662 - val_loss: 55.1432 - val_mae: 6.2013\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 121.1749 - mae: 7.8442 - val_loss: 48.0488 - val_mae: 5.7136\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 110.4801 - mae: 7.3386 - val_loss: 43.1307 - val_mae: 5.3791\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 102.1354 - mae: 6.9217 - val_loss: 40.2259 - val_mae: 5.2289\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 96.1892 - mae: 6.6313 - val_loss: 38.7020 - val_mae: 5.1263\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 92.0235 - mae: 6.4527 - val_loss: 38.1522 - val_mae: 5.1533\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 89.1146 - mae: 6.3499 - val_loss: 38.2141 - val_mae: 5.2107\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 87.1056 - mae: 6.3112 - val_loss: 38.6542 - val_mae: 5.2571\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 85.6893 - mae: 6.2985 - val_loss: 39.2216 - val_mae: 5.2923\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 85.0027 - mae: 6.2951 - val_loss: 39.8277 - val_mae: 5.3208\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 84.3470 - mae: 6.2958 - val_loss: 40.5978 - val_mae: 5.3505\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 83.8862 - mae: 6.3135 - val_loss: 41.2428 - val_mae: 5.3719\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 83.6513 - mae: 6.3265 - val_loss: 41.7530 - val_mae: 5.3936\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 83.5125 - mae: 6.3385 - val_loss: 42.3221 - val_mae: 5.4203\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 83.5026 - mae: 6.3641 - val_loss: 42.6949 - val_mae: 5.4367\n",
      "Epoch 52: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   6.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 36ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   1.7s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 30ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   1.8s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 25ms/step - loss: 1774.2852 - mae: 40.6237 - val_loss: 1679.4150 - val_mae: 40.2672\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1774.0135 - mae: 40.6205 - val_loss: 1679.1373 - val_mae: 40.2639\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1773.7402 - mae: 40.6172 - val_loss: 1678.8593 - val_mae: 40.2606\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1773.4664 - mae: 40.6140 - val_loss: 1678.5830 - val_mae: 40.2573\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1773.1968 - mae: 40.6109 - val_loss: 1678.3060 - val_mae: 40.2540\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1772.9227 - mae: 40.6076 - val_loss: 1678.0302 - val_mae: 40.2507\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1772.6511 - mae: 40.6045 - val_loss: 1677.7522 - val_mae: 40.2474\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1772.3757 - mae: 40.6013 - val_loss: 1677.4744 - val_mae: 40.2441\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1772.1031 - mae: 40.5980 - val_loss: 1677.1954 - val_mae: 40.2408\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1771.8291 - mae: 40.5948 - val_loss: 1676.9177 - val_mae: 40.2374\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1771.5581 - mae: 40.5916 - val_loss: 1676.6390 - val_mae: 40.2341\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1771.2844 - mae: 40.5884 - val_loss: 1676.3641 - val_mae: 40.2308\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1771.0112 - mae: 40.5852 - val_loss: 1676.0900 - val_mae: 40.2276\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1770.7455 - mae: 40.5821 - val_loss: 1675.8103 - val_mae: 40.2242\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1770.4683 - mae: 40.5788 - val_loss: 1675.5353 - val_mae: 40.2209\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1770.1960 - mae: 40.5756 - val_loss: 1675.2581 - val_mae: 40.2176\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1769.9269 - mae: 40.5725 - val_loss: 1674.9796 - val_mae: 40.2143\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1769.6545 - mae: 40.5693 - val_loss: 1674.7014 - val_mae: 40.2110\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1769.3781 - mae: 40.5661 - val_loss: 1674.4297 - val_mae: 40.2077\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1769.1085 - mae: 40.5629 - val_loss: 1674.1544 - val_mae: 40.2044\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1768.8379 - mae: 40.5597 - val_loss: 1673.8748 - val_mae: 40.2011\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1768.5647 - mae: 40.5565 - val_loss: 1673.5983 - val_mae: 40.1978\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1768.2954 - mae: 40.5533 - val_loss: 1673.3192 - val_mae: 40.1945\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1768.0210 - mae: 40.5501 - val_loss: 1673.0443 - val_mae: 40.1912\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1767.7521 - mae: 40.5469 - val_loss: 1672.7672 - val_mae: 40.1879\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1767.4784 - mae: 40.5437 - val_loss: 1672.4930 - val_mae: 40.1846\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1767.2076 - mae: 40.5405 - val_loss: 1672.2167 - val_mae: 40.1813\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1766.9384 - mae: 40.5373 - val_loss: 1671.9424 - val_mae: 40.1780\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1766.6698 - mae: 40.5342 - val_loss: 1671.6666 - val_mae: 40.1747\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1766.3966 - mae: 40.5310 - val_loss: 1671.3914 - val_mae: 40.1714\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1766.1268 - mae: 40.5278 - val_loss: 1671.1166 - val_mae: 40.1681\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1765.8553 - mae: 40.5246 - val_loss: 1670.8417 - val_mae: 40.1649\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1765.5883 - mae: 40.5214 - val_loss: 1670.5634 - val_mae: 40.1615\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1765.3132 - mae: 40.5182 - val_loss: 1670.2920 - val_mae: 40.1583\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1765.0461 - mae: 40.5151 - val_loss: 1670.0195 - val_mae: 40.1550\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1764.7732 - mae: 40.5119 - val_loss: 1669.7484 - val_mae: 40.1518\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1764.5083 - mae: 40.5087 - val_loss: 1669.4696 - val_mae: 40.1484\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1764.2356 - mae: 40.5055 - val_loss: 1669.1923 - val_mae: 40.1451\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1763.9639 - mae: 40.5023 - val_loss: 1668.9174 - val_mae: 40.1418\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1763.6924 - mae: 40.4992 - val_loss: 1668.6417 - val_mae: 40.1385\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1763.4229 - mae: 40.4960 - val_loss: 1668.3663 - val_mae: 40.1353\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1763.1511 - mae: 40.4928 - val_loss: 1668.0923 - val_mae: 40.1320\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1762.8804 - mae: 40.4896 - val_loss: 1667.8182 - val_mae: 40.1287\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1762.6145 - mae: 40.4864 - val_loss: 1667.5402 - val_mae: 40.1254\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1762.3394 - mae: 40.4832 - val_loss: 1667.2675 - val_mae: 40.1221\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1762.0726 - mae: 40.4801 - val_loss: 1666.9933 - val_mae: 40.1188\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1761.8024 - mae: 40.4769 - val_loss: 1666.7218 - val_mae: 40.1156\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1761.5341 - mae: 40.4737 - val_loss: 1666.4482 - val_mae: 40.1123\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1761.2638 - mae: 40.4706 - val_loss: 1666.1744 - val_mae: 40.1090\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1760.9938 - mae: 40.4674 - val_loss: 1665.8987 - val_mae: 40.1057\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1760.7228 - mae: 40.4642 - val_loss: 1665.6240 - val_mae: 40.1024\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1760.4521 - mae: 40.4610 - val_loss: 1665.3488 - val_mae: 40.0991\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1760.1807 - mae: 40.4578 - val_loss: 1665.0726 - val_mae: 40.0958\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1759.9115 - mae: 40.4546 - val_loss: 1664.7955 - val_mae: 40.0925\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1759.6405 - mae: 40.4514 - val_loss: 1664.5189 - val_mae: 40.0892\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1759.3665 - mae: 40.4482 - val_loss: 1664.2444 - val_mae: 40.0859\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1759.0973 - mae: 40.4450 - val_loss: 1663.9701 - val_mae: 40.0826\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1758.8289 - mae: 40.4419 - val_loss: 1663.6930 - val_mae: 40.0793\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1758.5549 - mae: 40.4386 - val_loss: 1663.4198 - val_mae: 40.0760\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1758.2899 - mae: 40.4355 - val_loss: 1663.1438 - val_mae: 40.0727\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1758.0153 - mae: 40.4323 - val_loss: 1662.8711 - val_mae: 40.0694\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1757.7484 - mae: 40.4291 - val_loss: 1662.5955 - val_mae: 40.0661\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1757.4773 - mae: 40.4259 - val_loss: 1662.3226 - val_mae: 40.0629\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1757.2081 - mae: 40.4228 - val_loss: 1662.0475 - val_mae: 40.0596\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1756.9369 - mae: 40.4196 - val_loss: 1661.7745 - val_mae: 40.0563\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1756.6703 - mae: 40.4164 - val_loss: 1661.4972 - val_mae: 40.0530\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1756.3976 - mae: 40.4132 - val_loss: 1661.2211 - val_mae: 40.0497\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1756.1277 - mae: 40.4100 - val_loss: 1660.9432 - val_mae: 40.0463\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1755.8564 - mae: 40.4068 - val_loss: 1660.6686 - val_mae: 40.0430\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1755.5852 - mae: 40.4036 - val_loss: 1660.3951 - val_mae: 40.0397\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1755.3171 - mae: 40.4004 - val_loss: 1660.1221 - val_mae: 40.0365\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1755.0476 - mae: 40.3973 - val_loss: 1659.8469 - val_mae: 40.0332\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1754.7784 - mae: 40.3941 - val_loss: 1659.5752 - val_mae: 40.0299\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1754.5101 - mae: 40.3909 - val_loss: 1659.3018 - val_mae: 40.0266\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1754.2434 - mae: 40.3878 - val_loss: 1659.0256 - val_mae: 40.0233\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1753.9708 - mae: 40.3846 - val_loss: 1658.7521 - val_mae: 40.0200\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1753.7025 - mae: 40.3814 - val_loss: 1658.4773 - val_mae: 40.0167\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1753.4325 - mae: 40.3782 - val_loss: 1658.2036 - val_mae: 40.0135\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1753.1633 - mae: 40.3750 - val_loss: 1657.9290 - val_mae: 40.0102\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1752.8918 - mae: 40.3718 - val_loss: 1657.6576 - val_mae: 40.0069\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1752.6268 - mae: 40.3687 - val_loss: 1657.3818 - val_mae: 40.0036\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1752.3524 - mae: 40.3654 - val_loss: 1657.1085 - val_mae: 40.0003\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1752.0853 - mae: 40.3623 - val_loss: 1656.8325 - val_mae: 39.9970\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1751.8162 - mae: 40.3591 - val_loss: 1656.5591 - val_mae: 39.9937\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1751.5464 - mae: 40.3559 - val_loss: 1656.2874 - val_mae: 39.9905\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1751.2784 - mae: 40.3528 - val_loss: 1656.0164 - val_mae: 39.9872\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1751.0112 - mae: 40.3496 - val_loss: 1655.7454 - val_mae: 39.9839\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1750.7472 - mae: 40.3465 - val_loss: 1655.4702 - val_mae: 39.9806\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1750.4764 - mae: 40.3433 - val_loss: 1655.1968 - val_mae: 39.9774\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1750.2081 - mae: 40.3401 - val_loss: 1654.9252 - val_mae: 39.9741\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1749.9413 - mae: 40.3369 - val_loss: 1654.6525 - val_mae: 39.9708\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1749.6718 - mae: 40.3338 - val_loss: 1654.3805 - val_mae: 39.9675\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1749.4019 - mae: 40.3306 - val_loss: 1654.1094 - val_mae: 39.9643\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1749.1361 - mae: 40.3274 - val_loss: 1653.8353 - val_mae: 39.9610\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1748.8674 - mae: 40.3243 - val_loss: 1653.5638 - val_mae: 39.9577\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1748.5985 - mae: 40.3211 - val_loss: 1653.2911 - val_mae: 39.9544\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1748.3323 - mae: 40.3179 - val_loss: 1653.0159 - val_mae: 39.9511\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1748.0613 - mae: 40.3147 - val_loss: 1652.7435 - val_mae: 39.9479\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1747.7925 - mae: 40.3115 - val_loss: 1652.4694 - val_mae: 39.9446\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1747.5236 - mae: 40.3084 - val_loss: 1652.1960 - val_mae: 39.9413\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=adam; total time=  10.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 34ms/step - loss: 567.9770 - mae: 20.1863 - val_loss: 542.2874 - val_mae: 20.5147\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 567.8565 - mae: 20.1850 - val_loss: 542.1152 - val_mae: 20.5120\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 567.7366 - mae: 20.1837 - val_loss: 541.9376 - val_mae: 20.5092\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 567.6135 - mae: 20.1825 - val_loss: 541.7687 - val_mae: 20.5066\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 567.4985 - mae: 20.1813 - val_loss: 541.6028 - val_mae: 20.5039\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 567.3821 - mae: 20.1800 - val_loss: 541.4457 - val_mae: 20.5014\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 567.2759 - mae: 20.1788 - val_loss: 541.2832 - val_mae: 20.4989\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 567.1740 - mae: 20.1779 - val_loss: 541.1089 - val_mae: 20.4962\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 567.0482 - mae: 20.1765 - val_loss: 540.9561 - val_mae: 20.4937\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 566.9388 - mae: 20.1753 - val_loss: 540.8014 - val_mae: 20.4913\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 566.8317 - mae: 20.1741 - val_loss: 540.6378 - val_mae: 20.4887\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 566.7175 - mae: 20.1729 - val_loss: 540.4803 - val_mae: 20.4862\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 566.6078 - mae: 20.1718 - val_loss: 540.3207 - val_mae: 20.4837\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 566.5071 - mae: 20.1708 - val_loss: 540.1495 - val_mae: 20.4810\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 566.3891 - mae: 20.1695 - val_loss: 539.9865 - val_mae: 20.4784\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 566.2756 - mae: 20.1683 - val_loss: 539.8368 - val_mae: 20.4760\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 566.1714 - mae: 20.1672 - val_loss: 539.6760 - val_mae: 20.4735\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 566.0536 - mae: 20.1658 - val_loss: 539.5222 - val_mae: 20.4710\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 565.9493 - mae: 20.1648 - val_loss: 539.3547 - val_mae: 20.4684\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 565.8344 - mae: 20.1634 - val_loss: 539.1923 - val_mae: 20.4658\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 565.7219 - mae: 20.1622 - val_loss: 539.0298 - val_mae: 20.4632\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 565.6052 - mae: 20.1610 - val_loss: 538.8699 - val_mae: 20.4607\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 565.4928 - mae: 20.1597 - val_loss: 538.7083 - val_mae: 20.4581\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 565.3853 - mae: 20.1586 - val_loss: 538.5409 - val_mae: 20.4554\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 565.2677 - mae: 20.1573 - val_loss: 538.3897 - val_mae: 20.4530\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 565.1632 - mae: 20.1562 - val_loss: 538.2242 - val_mae: 20.4504\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 565.0454 - mae: 20.1549 - val_loss: 538.0682 - val_mae: 20.4479\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 564.9479 - mae: 20.1540 - val_loss: 537.8892 - val_mae: 20.4451\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 564.8199 - mae: 20.1526 - val_loss: 537.7336 - val_mae: 20.4426\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 564.7098 - mae: 20.1514 - val_loss: 537.5720 - val_mae: 20.4400\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 564.6007 - mae: 20.1503 - val_loss: 537.4146 - val_mae: 20.4375\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 564.4897 - mae: 20.1490 - val_loss: 537.2586 - val_mae: 20.4350\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 564.3790 - mae: 20.1477 - val_loss: 537.1076 - val_mae: 20.4326\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 564.2695 - mae: 20.1465 - val_loss: 536.9510 - val_mae: 20.4301\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 564.1723 - mae: 20.1456 - val_loss: 536.7708 - val_mae: 20.4272\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 564.0423 - mae: 20.1440 - val_loss: 536.6275 - val_mae: 20.4249\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 563.9385 - mae: 20.1429 - val_loss: 536.4703 - val_mae: 20.4224\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 563.8275 - mae: 20.1417 - val_loss: 536.3102 - val_mae: 20.4198\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 563.7150 - mae: 20.1405 - val_loss: 536.1485 - val_mae: 20.4173\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 563.6068 - mae: 20.1394 - val_loss: 535.9784 - val_mae: 20.4146\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 563.4911 - mae: 20.1381 - val_loss: 535.8132 - val_mae: 20.4119\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 563.3769 - mae: 20.1368 - val_loss: 535.6529 - val_mae: 20.4093\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 563.2689 - mae: 20.1357 - val_loss: 535.4917 - val_mae: 20.4068\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 563.1655 - mae: 20.1346 - val_loss: 535.3256 - val_mae: 20.4041\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 563.0446 - mae: 20.1332 - val_loss: 535.1823 - val_mae: 20.4018\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 562.9389 - mae: 20.1320 - val_loss: 535.0271 - val_mae: 20.3993\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 562.8263 - mae: 20.1307 - val_loss: 534.8767 - val_mae: 20.3969\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 562.7234 - mae: 20.1296 - val_loss: 534.7067 - val_mae: 20.3942\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 562.6115 - mae: 20.1285 - val_loss: 534.5436 - val_mae: 20.3915\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 562.4993 - mae: 20.1272 - val_loss: 534.3819 - val_mae: 20.3890\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 562.3846 - mae: 20.1259 - val_loss: 534.2177 - val_mae: 20.3863\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 562.2697 - mae: 20.1248 - val_loss: 534.0515 - val_mae: 20.3837\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 562.1616 - mae: 20.1237 - val_loss: 533.8806 - val_mae: 20.3809\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 562.0412 - mae: 20.1223 - val_loss: 533.7245 - val_mae: 20.3784\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 561.9303 - mae: 20.1212 - val_loss: 533.5628 - val_mae: 20.3759\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 561.8229 - mae: 20.1200 - val_loss: 533.4003 - val_mae: 20.3732\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 561.7141 - mae: 20.1189 - val_loss: 533.2433 - val_mae: 20.3707\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 561.6050 - mae: 20.1177 - val_loss: 533.0847 - val_mae: 20.3682\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 561.4948 - mae: 20.1164 - val_loss: 532.9261 - val_mae: 20.3656\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 561.3849 - mae: 20.1152 - val_loss: 532.7787 - val_mae: 20.3632\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 561.2792 - mae: 20.1139 - val_loss: 532.6316 - val_mae: 20.3608\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 561.1744 - mae: 20.1126 - val_loss: 532.4769 - val_mae: 20.3583\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 561.0655 - mae: 20.1114 - val_loss: 532.3194 - val_mae: 20.3557\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 560.9517 - mae: 20.1101 - val_loss: 532.1702 - val_mae: 20.3533\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 560.8470 - mae: 20.1090 - val_loss: 532.0065 - val_mae: 20.3507\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 560.7443 - mae: 20.1081 - val_loss: 531.8308 - val_mae: 20.3479\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 560.6204 - mae: 20.1067 - val_loss: 531.6825 - val_mae: 20.3455\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 560.5181 - mae: 20.1056 - val_loss: 531.5185 - val_mae: 20.3428\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 560.4053 - mae: 20.1043 - val_loss: 531.3681 - val_mae: 20.3404\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 560.3003 - mae: 20.1031 - val_loss: 531.2131 - val_mae: 20.3379\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 560.1932 - mae: 20.1019 - val_loss: 531.0575 - val_mae: 20.3353\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 560.0906 - mae: 20.1007 - val_loss: 530.8921 - val_mae: 20.3327\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 559.9713 - mae: 20.0994 - val_loss: 530.7488 - val_mae: 20.3303\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 559.8690 - mae: 20.0982 - val_loss: 530.5955 - val_mae: 20.3278\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 559.7587 - mae: 20.0969 - val_loss: 530.4449 - val_mae: 20.3254\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 559.6574 - mae: 20.0959 - val_loss: 530.2820 - val_mae: 20.3228\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 559.5507 - mae: 20.0947 - val_loss: 530.1171 - val_mae: 20.3201\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 559.4298 - mae: 20.0932 - val_loss: 529.9803 - val_mae: 20.3178\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 559.3328 - mae: 20.0922 - val_loss: 529.8143 - val_mae: 20.3151\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 559.2192 - mae: 20.0909 - val_loss: 529.6646 - val_mae: 20.3127\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 559.1172 - mae: 20.0897 - val_loss: 529.5101 - val_mae: 20.3102\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 559.0107 - mae: 20.0886 - val_loss: 529.3513 - val_mae: 20.3076\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 558.8977 - mae: 20.0873 - val_loss: 529.1948 - val_mae: 20.3051\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 558.7882 - mae: 20.0860 - val_loss: 529.0465 - val_mae: 20.3026\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 558.6803 - mae: 20.0848 - val_loss: 528.8914 - val_mae: 20.3001\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 558.5743 - mae: 20.0837 - val_loss: 528.7327 - val_mae: 20.2975\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 558.4736 - mae: 20.0825 - val_loss: 528.5679 - val_mae: 20.2949\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 558.3598 - mae: 20.0812 - val_loss: 528.4196 - val_mae: 20.2924\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 558.2635 - mae: 20.0801 - val_loss: 528.2628 - val_mae: 20.2899\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 558.1456 - mae: 20.0787 - val_loss: 528.1255 - val_mae: 20.2876\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 558.0457 - mae: 20.0776 - val_loss: 527.9702 - val_mae: 20.2851\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 557.9341 - mae: 20.0762 - val_loss: 527.8241 - val_mae: 20.2826\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 557.8410 - mae: 20.0752 - val_loss: 527.6644 - val_mae: 20.2800\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 557.7205 - mae: 20.0738 - val_loss: 527.5288 - val_mae: 20.2778\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 557.6151 - mae: 20.0725 - val_loss: 527.3874 - val_mae: 20.2754\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 557.5209 - mae: 20.0714 - val_loss: 527.2198 - val_mae: 20.2727\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 557.4052 - mae: 20.0700 - val_loss: 527.0689 - val_mae: 20.2702\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 557.2938 - mae: 20.0687 - val_loss: 526.9255 - val_mae: 20.2679\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 557.1923 - mae: 20.0676 - val_loss: 526.7663 - val_mae: 20.2653\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 557.0875 - mae: 20.0665 - val_loss: 526.6102 - val_mae: 20.2627\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=adam; total time=  11.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 2s 102ms/step - loss: 3256.3555 - mae: 49.4905 - val_loss: 2676.5901 - val_mae: 46.4231\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3255.6194 - mae: 49.4851 - val_loss: 2676.0247 - val_mae: 46.4183\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3254.9106 - mae: 49.4798 - val_loss: 2675.4519 - val_mae: 46.4135\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 3254.2073 - mae: 49.4745 - val_loss: 2674.8845 - val_mae: 46.4087\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 3253.5012 - mae: 49.4691 - val_loss: 2674.3157 - val_mae: 46.4039\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 3252.7739 - mae: 49.4637 - val_loss: 2673.7559 - val_mae: 46.3992\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 3252.0437 - mae: 49.4584 - val_loss: 2673.1958 - val_mae: 46.3945\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 3251.3584 - mae: 49.4531 - val_loss: 2672.6233 - val_mae: 46.3897\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 3250.6152 - mae: 49.4477 - val_loss: 2672.0659 - val_mae: 46.3850\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3249.9214 - mae: 49.4424 - val_loss: 2671.4917 - val_mae: 46.3801\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3249.1812 - mae: 49.4370 - val_loss: 2670.9358 - val_mae: 46.3755\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 3248.4836 - mae: 49.4317 - val_loss: 2670.3718 - val_mae: 46.3707\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 3247.7798 - mae: 49.4264 - val_loss: 2669.7983 - val_mae: 46.3659\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 3247.0542 - mae: 49.4210 - val_loss: 2669.2322 - val_mae: 46.3611\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 3246.3372 - mae: 49.4157 - val_loss: 2668.6682 - val_mae: 46.3564\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 3245.6174 - mae: 49.4104 - val_loss: 2668.1106 - val_mae: 46.3517\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 3244.9216 - mae: 49.4050 - val_loss: 2667.5442 - val_mae: 46.3470\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3244.1953 - mae: 49.3997 - val_loss: 2666.9800 - val_mae: 46.3422\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 3243.4707 - mae: 49.3944 - val_loss: 2666.4109 - val_mae: 46.3374\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3242.7793 - mae: 49.3890 - val_loss: 2665.8276 - val_mae: 46.3326\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3242.0127 - mae: 49.3835 - val_loss: 2665.2715 - val_mae: 46.3279\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 3241.3374 - mae: 49.3783 - val_loss: 2664.6975 - val_mae: 46.3231\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 3240.5969 - mae: 49.3729 - val_loss: 2664.1370 - val_mae: 46.3184\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 3239.8811 - mae: 49.3676 - val_loss: 2663.5762 - val_mae: 46.3137\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 3239.1741 - mae: 49.3623 - val_loss: 2663.0115 - val_mae: 46.3089\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 3238.4392 - mae: 49.3569 - val_loss: 2662.4600 - val_mae: 46.3043\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 3237.7529 - mae: 49.3517 - val_loss: 2661.8833 - val_mae: 46.2995\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 3237.0137 - mae: 49.3462 - val_loss: 2661.3135 - val_mae: 46.2947\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 3236.2849 - mae: 49.3409 - val_loss: 2660.7634 - val_mae: 46.2900\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 3235.5940 - mae: 49.3357 - val_loss: 2660.2017 - val_mae: 46.2853\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 3234.8647 - mae: 49.3303 - val_loss: 2659.6440 - val_mae: 46.2806\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 3234.1655 - mae: 49.3251 - val_loss: 2659.0803 - val_mae: 46.2759\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 3233.4497 - mae: 49.3198 - val_loss: 2658.5139 - val_mae: 46.2711\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 3232.7395 - mae: 49.3144 - val_loss: 2657.9458 - val_mae: 46.2664\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 3232.0171 - mae: 49.3091 - val_loss: 2657.3855 - val_mae: 46.2616\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 3231.2778 - mae: 49.3036 - val_loss: 2656.8403 - val_mae: 46.2570\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 3230.5818 - mae: 49.2984 - val_loss: 2656.2793 - val_mae: 46.2523\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3229.8613 - mae: 49.2931 - val_loss: 2655.7124 - val_mae: 46.2476\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3229.1487 - mae: 49.2878 - val_loss: 2655.1401 - val_mae: 46.2427\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3228.4265 - mae: 49.2824 - val_loss: 2654.5737 - val_mae: 46.2380\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 3227.7002 - mae: 49.2770 - val_loss: 2654.0166 - val_mae: 46.2333\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3226.9788 - mae: 49.2717 - val_loss: 2653.4590 - val_mae: 46.2286\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3226.2881 - mae: 49.2665 - val_loss: 2652.8865 - val_mae: 46.2238\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 3225.5635 - mae: 49.2611 - val_loss: 2652.3225 - val_mae: 46.2191\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3224.8508 - mae: 49.2558 - val_loss: 2651.7620 - val_mae: 46.2143\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 3224.1208 - mae: 49.2504 - val_loss: 2651.2092 - val_mae: 46.2097\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 3223.4370 - mae: 49.2451 - val_loss: 2650.6348 - val_mae: 46.2048\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3222.7007 - mae: 49.2398 - val_loss: 2650.0706 - val_mae: 46.2000\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3221.9846 - mae: 49.2344 - val_loss: 2649.5095 - val_mae: 46.1953\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3221.2534 - mae: 49.2290 - val_loss: 2648.9543 - val_mae: 46.1906\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3220.5542 - mae: 49.2238 - val_loss: 2648.3862 - val_mae: 46.1857\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 3219.8311 - mae: 49.2185 - val_loss: 2647.8237 - val_mae: 46.1810\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 3219.1189 - mae: 49.2132 - val_loss: 2647.2605 - val_mae: 46.1762\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3218.4060 - mae: 49.2078 - val_loss: 2646.6912 - val_mae: 46.1714\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 3217.6877 - mae: 49.2025 - val_loss: 2646.1270 - val_mae: 46.1666\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 3216.9971 - mae: 49.1971 - val_loss: 2645.5535 - val_mae: 46.1618\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 3216.2603 - mae: 49.1918 - val_loss: 2644.9941 - val_mae: 46.1570\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 3215.5515 - mae: 49.1865 - val_loss: 2644.4409 - val_mae: 46.1523\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3214.8521 - mae: 49.1813 - val_loss: 2643.8889 - val_mae: 46.1476\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 3214.1431 - mae: 49.1760 - val_loss: 2643.3369 - val_mae: 46.1429\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 3213.4666 - mae: 49.1708 - val_loss: 2642.7673 - val_mae: 46.1381\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 3212.7190 - mae: 49.1654 - val_loss: 2642.2261 - val_mae: 46.1335\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 3212.0210 - mae: 49.1602 - val_loss: 2641.6733 - val_mae: 46.1288\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 3211.3325 - mae: 49.1550 - val_loss: 2641.1021 - val_mae: 46.1240\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 3210.6008 - mae: 49.1495 - val_loss: 2640.5459 - val_mae: 46.1193\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3209.8889 - mae: 49.1443 - val_loss: 2639.9834 - val_mae: 46.1145\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3209.1711 - mae: 49.1389 - val_loss: 2639.4280 - val_mae: 46.1098\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3208.4717 - mae: 49.1336 - val_loss: 2638.8613 - val_mae: 46.1050\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3207.7468 - mae: 49.1283 - val_loss: 2638.3020 - val_mae: 46.1002\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3207.0566 - mae: 49.1230 - val_loss: 2637.7449 - val_mae: 46.0955\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3206.3467 - mae: 49.1177 - val_loss: 2637.1855 - val_mae: 46.0907\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 3205.6389 - mae: 49.1125 - val_loss: 2636.6296 - val_mae: 46.0860\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 3204.9194 - mae: 49.1072 - val_loss: 2636.0771 - val_mae: 46.0813\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 3204.2114 - mae: 49.1019 - val_loss: 2635.5139 - val_mae: 46.0765\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 3203.5146 - mae: 49.0965 - val_loss: 2634.9453 - val_mae: 46.0717\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3202.7715 - mae: 49.0911 - val_loss: 2634.3953 - val_mae: 46.0670\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3202.0750 - mae: 49.0859 - val_loss: 2633.8362 - val_mae: 46.0623\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 3201.3694 - mae: 49.0806 - val_loss: 2633.2754 - val_mae: 46.0575\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 3200.6528 - mae: 49.0752 - val_loss: 2632.7192 - val_mae: 46.0528\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3199.9517 - mae: 49.0700 - val_loss: 2632.1499 - val_mae: 46.0479\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 3199.2510 - mae: 49.0647 - val_loss: 2631.5798 - val_mae: 46.0431\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3198.5137 - mae: 49.0593 - val_loss: 2631.0330 - val_mae: 46.0384\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3197.8174 - mae: 49.0540 - val_loss: 2630.4763 - val_mae: 46.0337\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3197.1284 - mae: 49.0488 - val_loss: 2629.9199 - val_mae: 46.0290\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3196.4006 - mae: 49.0435 - val_loss: 2629.3762 - val_mae: 46.0243\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3195.7144 - mae: 49.0382 - val_loss: 2628.8218 - val_mae: 46.0196\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3195.0046 - mae: 49.0330 - val_loss: 2628.2683 - val_mae: 46.0149\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 3194.3030 - mae: 49.0277 - val_loss: 2627.7095 - val_mae: 46.0101\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3193.6006 - mae: 49.0224 - val_loss: 2627.1511 - val_mae: 46.0054\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 3192.8691 - mae: 49.0170 - val_loss: 2626.5964 - val_mae: 46.0007\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 3192.1777 - mae: 49.0118 - val_loss: 2626.0193 - val_mae: 45.9958\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 3191.4597 - mae: 49.0064 - val_loss: 2625.4546 - val_mae: 45.9910\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3190.7451 - mae: 49.0011 - val_loss: 2624.8989 - val_mae: 45.9862\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 3190.0212 - mae: 48.9957 - val_loss: 2624.3589 - val_mae: 45.9816\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3189.3464 - mae: 48.9906 - val_loss: 2623.7925 - val_mae: 45.9768\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 3188.6064 - mae: 48.9851 - val_loss: 2623.2437 - val_mae: 45.9721\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 3187.9514 - mae: 48.9800 - val_loss: 2622.6631 - val_mae: 45.9672\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 3187.1897 - mae: 48.9744 - val_loss: 2622.1199 - val_mae: 45.9625\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 3186.4993 - mae: 48.9693 - val_loss: 2621.5662 - val_mae: 45.9578\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3185.7871 - mae: 48.9640 - val_loss: 2621.0264 - val_mae: 45.9532\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=adam; total time=  21.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 35ms/step - loss: 1624.9377 - mae: 35.2862 - val_loss: 281.5084 - val_mae: 15.5023\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 253.4858 - mae: 12.8877 - val_loss: 56.9761 - val_mae: 6.7841\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 111.2978 - mae: 7.6680 - val_loss: 50.0145 - val_mae: 6.4290\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 103.8279 - mae: 7.3553 - val_loss: 53.2070 - val_mae: 6.2649\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 100.0591 - mae: 7.2093 - val_loss: 45.4094 - val_mae: 5.9719\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 97.7104 - mae: 7.0647 - val_loss: 39.7847 - val_mae: 5.7114\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 93.8326 - mae: 6.8523 - val_loss: 38.8480 - val_mae: 5.5297\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 92.0896 - mae: 6.7747 - val_loss: 38.4998 - val_mae: 5.3739\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 88.3052 - mae: 6.6065 - val_loss: 39.8626 - val_mae: 5.2260\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 86.3634 - mae: 6.5680 - val_loss: 32.4520 - val_mae: 4.9682\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 84.3101 - mae: 6.4241 - val_loss: 32.8188 - val_mae: 4.8138\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 83.2633 - mae: 6.3703 - val_loss: 28.5707 - val_mae: 4.5917\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 81.0234 - mae: 6.2403 - val_loss: 30.4786 - val_mae: 4.5041\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 81.5725 - mae: 6.3067 - val_loss: 24.7705 - val_mae: 4.2464\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 78.0616 - mae: 6.1470 - val_loss: 22.6714 - val_mae: 4.0896\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 77.6746 - mae: 6.1339 - val_loss: 20.9656 - val_mae: 3.9723\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 76.9500 - mae: 5.9956 - val_loss: 22.7529 - val_mae: 3.8712\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 75.9766 - mae: 6.0225 - val_loss: 22.1229 - val_mae: 3.7840\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 74.8177 - mae: 5.9197 - val_loss: 23.3114 - val_mae: 3.7480\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 74.9098 - mae: 6.0243 - val_loss: 22.3348 - val_mae: 3.6752\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 73.9375 - mae: 6.0518 - val_loss: 19.5404 - val_mae: 3.5263\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 74.0408 - mae: 5.9467 - val_loss: 18.3011 - val_mae: 3.4437\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 1s 78ms/step - loss: 72.7652 - mae: 5.8059 - val_loss: 22.2285 - val_mae: 3.5737\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 73.1105 - mae: 6.0275 - val_loss: 19.1766 - val_mae: 3.3724\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 72.5999 - mae: 5.9778 - val_loss: 17.9653 - val_mae: 3.3001\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 72.3585 - mae: 5.8621 - val_loss: 19.9236 - val_mae: 3.3929\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 72.7668 - mae: 5.9383 - val_loss: 18.3019 - val_mae: 3.2646\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 73.6008 - mae: 5.9883 - val_loss: 17.2877 - val_mae: 3.2116\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 71.9417 - mae: 5.8105 - val_loss: 20.2996 - val_mae: 3.3787\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 71.9712 - mae: 6.0094 - val_loss: 16.6688 - val_mae: 3.2155\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 71.8247 - mae: 5.8886 - val_loss: 15.9184 - val_mae: 3.2689\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 71.9516 - mae: 5.7457 - val_loss: 19.7527 - val_mae: 3.3116\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 72.2354 - mae: 5.9773 - val_loss: 18.8474 - val_mae: 3.2708\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 71.2929 - mae: 5.8696 - val_loss: 20.2840 - val_mae: 3.3419\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 71.8618 - mae: 5.9496 - val_loss: 18.2652 - val_mae: 3.2551\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 71.7713 - mae: 5.9678 - val_loss: 16.5869 - val_mae: 3.1919\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 71.5747 - mae: 5.8622 - val_loss: 16.8896 - val_mae: 3.2024\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 50ms/step - loss: 72.0075 - mae: 5.8784 - val_loss: 16.0949 - val_mae: 3.1905\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 71.4116 - mae: 5.8341 - val_loss: 17.4476 - val_mae: 3.2281\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 71.7086 - mae: 5.8355 - val_loss: 20.9408 - val_mae: 3.3958\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 71.6308 - mae: 6.0761 - val_loss: 16.7481 - val_mae: 3.1975\n",
      "Epoch 41: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=sgd; total time=   9.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 29ms/step - loss: 133.4836 - mae: 8.5039 - val_loss: 33.0136 - val_mae: 4.5898\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 95.9838 - mae: 7.5475 - val_loss: 31.1501 - val_mae: 4.5389\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 91.1006 - mae: 7.2175 - val_loss: 34.1226 - val_mae: 4.7406\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 86.3528 - mae: 7.0601 - val_loss: 33.0206 - val_mae: 4.5328\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 86.0948 - mae: 6.9563 - val_loss: 30.2891 - val_mae: 4.2739\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 87.3271 - mae: 6.9690 - val_loss: 26.7733 - val_mae: 4.1133\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 86.7436 - mae: 6.9177 - val_loss: 28.3592 - val_mae: 4.1330\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 90.9905 - mae: 6.9522 - val_loss: 32.6337 - val_mae: 4.5690\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 84.3638 - mae: 6.8583 - val_loss: 36.9403 - val_mae: 4.8973\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 85.1898 - mae: 6.9148 - val_loss: 37.9287 - val_mae: 4.9971\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 87.3986 - mae: 7.0527 - val_loss: 26.9626 - val_mae: 4.0269\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 84.4662 - mae: 6.7819 - val_loss: 32.7293 - val_mae: 4.6608\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 85.4453 - mae: 6.9631 - val_loss: 29.3738 - val_mae: 4.1432\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 86.8306 - mae: 6.9956 - val_loss: 27.4594 - val_mae: 4.0830\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 83.6095 - mae: 6.7630 - val_loss: 27.9278 - val_mae: 4.1573\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 84.0201 - mae: 6.8305 - val_loss: 26.8456 - val_mae: 4.0766\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=sgd; total time=   4.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 27ms/step - loss: 431.0354 - mae: 12.5942 - val_loss: 105.3679 - val_mae: 8.1710\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 132.2458 - mae: 8.4028 - val_loss: 70.0852 - val_mae: 6.2972\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 90.5233 - mae: 7.1047 - val_loss: 61.6389 - val_mae: 5.9369\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 70.5112 - mae: 6.1031 - val_loss: 57.6474 - val_mae: 5.9582\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 64.3383 - mae: 5.8179 - val_loss: 50.9868 - val_mae: 5.4137\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 61.4787 - mae: 5.6178 - val_loss: 48.9124 - val_mae: 5.4453\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 61.9817 - mae: 5.6573 - val_loss: 49.1133 - val_mae: 5.6645\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 60.7294 - mae: 5.6647 - val_loss: 50.4200 - val_mae: 5.3871\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 59.7247 - mae: 5.5199 - val_loss: 47.2101 - val_mae: 5.4179\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 58.2122 - mae: 5.4832 - val_loss: 46.1821 - val_mae: 5.4007\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 58.3589 - mae: 5.4694 - val_loss: 46.1651 - val_mae: 5.4016\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 58.8688 - mae: 5.5053 - val_loss: 48.8822 - val_mae: 5.3654\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 59.3672 - mae: 5.4412 - val_loss: 48.9095 - val_mae: 5.3524\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 58.8907 - mae: 5.4335 - val_loss: 46.7480 - val_mae: 5.5895\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 58.1828 - mae: 5.4945 - val_loss: 47.7975 - val_mae: 5.3314\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 58.4637 - mae: 5.4782 - val_loss: 48.5001 - val_mae: 5.3524\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 58.4983 - mae: 5.5220 - val_loss: 47.5174 - val_mae: 5.3661\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 57.9399 - mae: 5.4493 - val_loss: 45.6790 - val_mae: 5.3896\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 57.8347 - mae: 5.4766 - val_loss: 46.1572 - val_mae: 5.3621\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 57.5189 - mae: 5.3994 - val_loss: 45.5470 - val_mae: 5.4856\n",
      "Epoch 20: early stopping\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=sgd; total time=   3.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 55ms/step - loss: 531.5649 - mae: 17.5912 - val_loss: 101.3703 - val_mae: 8.1375\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 176.7771 - mae: 10.0843 - val_loss: 72.6799 - val_mae: 6.7401\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 162.9839 - mae: 9.4874 - val_loss: 66.6144 - val_mae: 6.7382\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 153.4223 - mae: 9.2700 - val_loss: 67.8956 - val_mae: 6.9325\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 144.1995 - mae: 8.9617 - val_loss: 62.8123 - val_mae: 6.6661\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 139.9660 - mae: 8.8742 - val_loss: 57.5075 - val_mae: 6.0406\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 137.0959 - mae: 8.5789 - val_loss: 54.0056 - val_mae: 6.1243\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 138.1464 - mae: 8.5652 - val_loss: 65.8484 - val_mae: 7.0886\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 130.0390 - mae: 8.3213 - val_loss: 71.8044 - val_mae: 7.5275\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 128.7153 - mae: 8.3189 - val_loss: 63.3350 - val_mae: 6.9447\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 128.3507 - mae: 8.3630 - val_loss: 51.3999 - val_mae: 5.9099\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 127.5019 - mae: 8.2545 - val_loss: 53.5871 - val_mae: 6.0739\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 128.2791 - mae: 8.1027 - val_loss: 105.4054 - val_mae: 9.1934\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 132.5365 - mae: 8.8507 - val_loss: 49.1573 - val_mae: 5.8282\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 122.4548 - mae: 8.2257 - val_loss: 49.7649 - val_mae: 5.8203\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 124.4396 - mae: 8.1360 - val_loss: 48.4086 - val_mae: 5.8056\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 123.6460 - mae: 7.9629 - val_loss: 57.6256 - val_mae: 6.3742\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 124.1598 - mae: 8.1080 - val_loss: 50.8150 - val_mae: 5.8607\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 118.0183 - mae: 7.9150 - val_loss: 60.2105 - val_mae: 6.5880\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 121.2673 - mae: 8.1262 - val_loss: 75.8951 - val_mae: 7.6118\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 118.2732 - mae: 8.1323 - val_loss: 57.8269 - val_mae: 6.2883\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 119.8914 - mae: 8.0649 - val_loss: 49.7065 - val_mae: 5.8081\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 117.1424 - mae: 7.9395 - val_loss: 61.3856 - val_mae: 6.6192\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 117.4577 - mae: 7.9559 - val_loss: 50.9273 - val_mae: 5.8630\n",
      "Epoch 24: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   3.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 28ms/step - loss: 3296.4126 - mae: 32.9137 - val_loss: 166.2444 - val_mae: 10.6375\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 320.7148 - mae: 14.0399 - val_loss: 101.0767 - val_mae: 8.2603\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 247.3953 - mae: 12.3527 - val_loss: 82.8469 - val_mae: 6.8461\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 205.2215 - mae: 11.1196 - val_loss: 81.4037 - val_mae: 6.9313\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 177.2804 - mae: 10.4084 - val_loss: 63.2100 - val_mae: 6.4002\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 162.7628 - mae: 9.7525 - val_loss: 59.2280 - val_mae: 6.3899\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 153.2505 - mae: 9.4194 - val_loss: 69.5562 - val_mae: 7.2809\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 155.2024 - mae: 9.5006 - val_loss: 71.2218 - val_mae: 7.4285\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 138.6041 - mae: 8.9371 - val_loss: 84.6464 - val_mae: 8.0529\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 136.6639 - mae: 8.8884 - val_loss: 81.4936 - val_mae: 7.9285\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 138.1169 - mae: 9.0346 - val_loss: 67.3657 - val_mae: 7.2027\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 134.6789 - mae: 8.7007 - val_loss: 81.2850 - val_mae: 7.9127\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 133.5141 - mae: 8.6614 - val_loss: 89.0194 - val_mae: 8.2164\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 134.6802 - mae: 8.8009 - val_loss: 62.9046 - val_mae: 6.7987\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 129.4655 - mae: 8.5643 - val_loss: 69.4483 - val_mae: 7.2984\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 129.4693 - mae: 8.4190 - val_loss: 73.4825 - val_mae: 7.5245\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   2.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 27ms/step - loss: 708.3345 - mae: 20.9446 - val_loss: 131.9853 - val_mae: 9.0086\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 114.2909 - mae: 8.1332 - val_loss: 51.1279 - val_mae: 5.1911\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 71.7958 - mae: 6.1564 - val_loss: 38.4002 - val_mae: 4.7347\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 55.9042 - mae: 5.3542 - val_loss: 36.3420 - val_mae: 4.6160\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 53.6503 - mae: 5.2485 - val_loss: 36.7696 - val_mae: 4.7503\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 52.5734 - mae: 5.1878 - val_loss: 34.2973 - val_mae: 4.5245\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 54.6081 - mae: 5.2317 - val_loss: 39.4233 - val_mae: 4.8557\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 53.3949 - mae: 5.2557 - val_loss: 33.5154 - val_mae: 4.4243\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 48.7249 - mae: 4.8562 - val_loss: 31.2660 - val_mae: 4.3631\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 47.5297 - mae: 4.8790 - val_loss: 31.1574 - val_mae: 4.3942\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 47.7850 - mae: 4.7824 - val_loss: 31.7709 - val_mae: 4.3594\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 47.5952 - mae: 4.7593 - val_loss: 35.8027 - val_mae: 4.4879\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 47.4415 - mae: 4.6987 - val_loss: 29.5030 - val_mae: 4.1792\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 49.1500 - mae: 4.8718 - val_loss: 29.2269 - val_mae: 4.2551\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 45.6610 - mae: 4.7317 - val_loss: 29.7081 - val_mae: 4.2269\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 46.9361 - mae: 4.6560 - val_loss: 31.7191 - val_mae: 4.3749\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 47.7638 - mae: 4.9123 - val_loss: 29.4206 - val_mae: 4.2401\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 47.4603 - mae: 4.7246 - val_loss: 28.5129 - val_mae: 4.1842\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 45.3426 - mae: 4.6876 - val_loss: 29.5051 - val_mae: 4.2095\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 46.0114 - mae: 4.6561 - val_loss: 28.6323 - val_mae: 4.2232\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 48.0158 - mae: 4.8297 - val_loss: 28.7093 - val_mae: 4.2276\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 44.7276 - mae: 4.5424 - val_loss: 29.0254 - val_mae: 4.1562\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 45.1328 - mae: 4.8071 - val_loss: 32.2120 - val_mae: 4.3175\n",
      "Epoch 23: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   3.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 4398294082982715810906112000.0000 - mae: 18107013791744.0000 - val_loss: 241178128418974380422998337257472.0000 - val_mae: 15200878176763904.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: inf - mae: 3924114476035955609883901952.0000 - val_loss: inf - val_mae: 3239093989075134878199758979072.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 5ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=sgd; total time=   1.9s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 23ms/step - loss: 80897245168702750737163091968.0000 - mae: 77932215140352.0000 - val_loss: 3931304828292904466187406788263936.0000 - val_mae: 61551799089233920.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: inf - mae: 13256123078089621709260849152.0000 - val_loss: inf - val_mae: 10932836024877525917413073223680.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=sgd; total time=   1.5s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 24ms/step - loss: 116527207495232285956496162816.0000 - mae: 96997533024256.0000 - val_loss: 5089082988789389405420644397481984.0000 - val_mae: 69752991896305664.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: inf - mae: 13826185909252472366054768640.0000 - val_loss: inf - val_mae: 10903842356945708265916095004672.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=0, model__n_neurons=125, model__optimizer=sgd; total time=   1.4s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 26ms/step - loss: 266.8164 - mae: 13.2180 - val_loss: 227.9820 - val_mae: 13.4339\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 250.6090 - mae: 12.7824 - val_loss: 213.1372 - val_mae: 12.9361\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 236.7703 - mae: 12.3396 - val_loss: 199.2937 - val_mae: 12.4427\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 223.9361 - mae: 11.9315 - val_loss: 185.7510 - val_mae: 11.9325\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 212.1494 - mae: 11.5326 - val_loss: 173.3929 - val_mae: 11.4399\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 201.6769 - mae: 11.1669 - val_loss: 162.3067 - val_mae: 10.9723\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 191.2905 - mae: 10.7985 - val_loss: 151.3480 - val_mae: 10.4981\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 182.4329 - mae: 10.4492 - val_loss: 141.3695 - val_mae: 10.0458\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 173.8938 - mae: 10.1196 - val_loss: 132.9128 - val_mae: 9.6692\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 165.8202 - mae: 9.8304 - val_loss: 125.6975 - val_mae: 9.3280\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 159.0761 - mae: 9.5692 - val_loss: 118.5477 - val_mae: 8.9784\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 152.3456 - mae: 9.3188 - val_loss: 112.4681 - val_mae: 8.6877\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 146.4930 - mae: 9.1122 - val_loss: 106.7094 - val_mae: 8.3964\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 141.0948 - mae: 8.9150 - val_loss: 101.4957 - val_mae: 8.1173\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 136.0878 - mae: 8.7344 - val_loss: 96.7572 - val_mae: 7.8500\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 131.5338 - mae: 8.5832 - val_loss: 92.2105 - val_mae: 7.5758\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 126.9763 - mae: 8.4223 - val_loss: 87.6698 - val_mae: 7.2784\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 123.3397 - mae: 8.2823 - val_loss: 83.6653 - val_mae: 7.0294\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 119.7657 - mae: 8.1644 - val_loss: 80.0922 - val_mae: 6.8078\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 116.2647 - mae: 8.0728 - val_loss: 77.3889 - val_mae: 6.6326\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 113.1179 - mae: 7.9915 - val_loss: 75.1152 - val_mae: 6.4763\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 110.4921 - mae: 7.9360 - val_loss: 72.3177 - val_mae: 6.3029\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 108.0486 - mae: 7.8388 - val_loss: 69.3757 - val_mae: 6.1160\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 105.6247 - mae: 7.7496 - val_loss: 67.0232 - val_mae: 5.9623\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 103.4742 - mae: 7.6982 - val_loss: 65.5162 - val_mae: 5.8873\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 101.6241 - mae: 7.6486 - val_loss: 63.7882 - val_mae: 5.7911\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 99.9958 - mae: 7.5916 - val_loss: 61.9788 - val_mae: 5.6718\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 98.3624 - mae: 7.5279 - val_loss: 60.4554 - val_mae: 5.5761\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 96.9708 - mae: 7.4748 - val_loss: 58.9120 - val_mae: 5.4638\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 95.5812 - mae: 7.4328 - val_loss: 58.2228 - val_mae: 5.4505\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 94.3006 - mae: 7.3893 - val_loss: 56.7459 - val_mae: 5.3419\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 93.0826 - mae: 7.3214 - val_loss: 55.1195 - val_mae: 5.2214\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 92.0049 - mae: 7.2720 - val_loss: 54.1290 - val_mae: 5.1682\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 90.8829 - mae: 7.2340 - val_loss: 53.2322 - val_mae: 5.1233\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 89.9870 - mae: 7.1907 - val_loss: 52.2021 - val_mae: 5.0623\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 89.0293 - mae: 7.1580 - val_loss: 51.6192 - val_mae: 5.0482\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 88.1610 - mae: 7.1288 - val_loss: 50.7085 - val_mae: 4.9966\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 87.3361 - mae: 7.0904 - val_loss: 49.9902 - val_mae: 4.9590\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 86.6043 - mae: 7.0463 - val_loss: 48.8864 - val_mae: 4.8761\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 85.9846 - mae: 6.9992 - val_loss: 48.0856 - val_mae: 4.8315\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 85.1781 - mae: 6.9808 - val_loss: 47.9315 - val_mae: 4.8452\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 84.5419 - mae: 6.9815 - val_loss: 47.4465 - val_mae: 4.8246\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 83.8573 - mae: 6.9539 - val_loss: 46.7635 - val_mae: 4.7809\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 83.1739 - mae: 6.9088 - val_loss: 45.8709 - val_mae: 4.7163\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 82.6711 - mae: 6.8698 - val_loss: 45.1272 - val_mae: 4.6737\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 82.0583 - mae: 6.8378 - val_loss: 44.4447 - val_mae: 4.6353\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81.6027 - mae: 6.8087 - val_loss: 43.7522 - val_mae: 4.5930\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 81.0577 - mae: 6.7728 - val_loss: 43.4133 - val_mae: 4.5725\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80.5156 - mae: 6.7553 - val_loss: 43.0687 - val_mae: 4.5576\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 80.0146 - mae: 6.7461 - val_loss: 43.1388 - val_mae: 4.5723\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 79.6646 - mae: 6.7422 - val_loss: 42.8742 - val_mae: 4.5659\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 79.2153 - mae: 6.7150 - val_loss: 42.3696 - val_mae: 4.5435\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 78.8488 - mae: 6.6935 - val_loss: 41.9114 - val_mae: 4.5317\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 78.6777 - mae: 6.6936 - val_loss: 42.1012 - val_mae: 4.6045\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78.0152 - mae: 6.6565 - val_loss: 40.9471 - val_mae: 4.5023\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 77.5682 - mae: 6.6079 - val_loss: 40.3545 - val_mae: 4.4686\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 77.3159 - mae: 6.5791 - val_loss: 39.7395 - val_mae: 4.4346\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 76.8738 - mae: 6.5497 - val_loss: 39.4718 - val_mae: 4.4407\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 76.5139 - mae: 6.5454 - val_loss: 39.5624 - val_mae: 4.4947\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 76.2080 - mae: 6.5435 - val_loss: 39.2184 - val_mae: 4.4967\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 75.8588 - mae: 6.5188 - val_loss: 38.6370 - val_mae: 4.4552\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 75.5718 - mae: 6.5096 - val_loss: 38.7522 - val_mae: 4.5060\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 75.0968 - mae: 6.4872 - val_loss: 38.2104 - val_mae: 4.4717\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 74.8071 - mae: 6.4576 - val_loss: 37.7347 - val_mae: 4.4371\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 74.5067 - mae: 6.4218 - val_loss: 37.3928 - val_mae: 4.4280\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 74.0744 - mae: 6.4070 - val_loss: 37.2173 - val_mae: 4.4364\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 73.7425 - mae: 6.3964 - val_loss: 37.0370 - val_mae: 4.4428\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 73.4972 - mae: 6.3971 - val_loss: 37.0502 - val_mae: 4.4809\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 73.3759 - mae: 6.4148 - val_loss: 37.2354 - val_mae: 4.5363\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 72.8486 - mae: 6.3796 - val_loss: 36.4439 - val_mae: 4.4637\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 72.5480 - mae: 6.3510 - val_loss: 36.0093 - val_mae: 4.4349\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 72.3831 - mae: 6.3060 - val_loss: 35.0759 - val_mae: 4.3284\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 72.0946 - mae: 6.2858 - val_loss: 35.1665 - val_mae: 4.3771\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 71.7784 - mae: 6.2737 - val_loss: 34.8032 - val_mae: 4.3585\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 71.5072 - mae: 6.2762 - val_loss: 35.0291 - val_mae: 4.4168\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71.3064 - mae: 6.2698 - val_loss: 34.6628 - val_mae: 4.3887\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71.0701 - mae: 6.2639 - val_loss: 34.4902 - val_mae: 4.3963\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 70.7973 - mae: 6.2590 - val_loss: 34.4642 - val_mae: 4.4129\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 70.5410 - mae: 6.2376 - val_loss: 33.9557 - val_mae: 4.3676\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 70.3428 - mae: 6.2124 - val_loss: 33.4915 - val_mae: 4.3270\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 70.2575 - mae: 6.2111 - val_loss: 33.7256 - val_mae: 4.3805\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 69.9201 - mae: 6.1939 - val_loss: 32.9410 - val_mae: 4.2972\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 69.6881 - mae: 6.1742 - val_loss: 32.9957 - val_mae: 4.3258\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 69.4758 - mae: 6.1617 - val_loss: 32.7661 - val_mae: 4.3203\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 69.3263 - mae: 6.1470 - val_loss: 32.4103 - val_mae: 4.2888\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 69.0518 - mae: 6.1262 - val_loss: 32.3504 - val_mae: 4.2998\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 68.8740 - mae: 6.1439 - val_loss: 32.6297 - val_mae: 4.3519\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 68.6633 - mae: 6.1443 - val_loss: 32.3407 - val_mae: 4.3347\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 68.5592 - mae: 6.1302 - val_loss: 31.9441 - val_mae: 4.3053\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 68.3531 - mae: 6.1282 - val_loss: 32.0931 - val_mae: 4.3383\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 68.0839 - mae: 6.1054 - val_loss: 31.4893 - val_mae: 4.2721\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 67.9170 - mae: 6.0727 - val_loss: 31.2689 - val_mae: 4.2597\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 67.8260 - mae: 6.0479 - val_loss: 30.7248 - val_mae: 4.2014\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 67.5769 - mae: 6.0260 - val_loss: 30.5919 - val_mae: 4.2043\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 67.4060 - mae: 6.0417 - val_loss: 30.8919 - val_mae: 4.2628\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 67.2065 - mae: 6.0461 - val_loss: 30.7102 - val_mae: 4.2539\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 67.0539 - mae: 6.0246 - val_loss: 30.2904 - val_mae: 4.2136\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 67.0266 - mae: 6.0415 - val_loss: 30.6387 - val_mae: 4.2764\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 66.7088 - mae: 6.0342 - val_loss: 30.2847 - val_mae: 4.2417\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 66.7032 - mae: 5.9943 - val_loss: 29.6213 - val_mae: 4.1699\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=adam; total time=   8.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 27ms/step - loss: 452.5029 - mae: 16.9895 - val_loss: 480.4131 - val_mae: 17.7013\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 413.8074 - mae: 16.3817 - val_loss: 430.9295 - val_mae: 17.0129\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 383.0849 - mae: 15.8776 - val_loss: 387.0092 - val_mae: 16.3324\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 356.1927 - mae: 15.3746 - val_loss: 351.7272 - val_mae: 15.6735\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 333.6493 - mae: 14.9125 - val_loss: 321.9895 - val_mae: 15.0440\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 313.5251 - mae: 14.4450 - val_loss: 296.6074 - val_mae: 14.4438\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 295.8453 - mae: 14.0188 - val_loss: 272.8463 - val_mae: 13.8577\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 280.2556 - mae: 13.6046 - val_loss: 250.3529 - val_mae: 13.2770\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 264.1139 - mae: 13.1768 - val_loss: 232.3726 - val_mae: 12.7490\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 249.9376 - mae: 12.7915 - val_loss: 216.0067 - val_mae: 12.2415\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 237.7427 - mae: 12.4431 - val_loss: 200.9080 - val_mae: 11.7880\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 225.2693 - mae: 12.0893 - val_loss: 186.8965 - val_mae: 11.3544\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 215.1444 - mae: 11.7725 - val_loss: 174.0719 - val_mae: 10.9325\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 205.9338 - mae: 11.4716 - val_loss: 161.9446 - val_mae: 10.5187\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 196.9297 - mae: 11.1726 - val_loss: 151.3615 - val_mae: 10.1356\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 189.3833 - mae: 10.9166 - val_loss: 142.7337 - val_mae: 9.7897\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 181.9455 - mae: 10.6715 - val_loss: 133.4623 - val_mae: 9.4280\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 175.3794 - mae: 10.4158 - val_loss: 125.4483 - val_mae: 9.1000\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 169.9382 - mae: 10.2054 - val_loss: 117.3979 - val_mae: 8.7677\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 164.4953 - mae: 10.0108 - val_loss: 111.2011 - val_mae: 8.5079\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 159.4049 - mae: 9.8350 - val_loss: 105.7430 - val_mae: 8.2735\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 155.1573 - mae: 9.6691 - val_loss: 99.7324 - val_mae: 8.0119\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 151.2101 - mae: 9.4996 - val_loss: 94.4253 - val_mae: 7.7764\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 147.9305 - mae: 9.3690 - val_loss: 89.8962 - val_mae: 7.5695\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 144.7363 - mae: 9.2778 - val_loss: 86.8730 - val_mae: 7.4365\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 142.1250 - mae: 9.2001 - val_loss: 83.3637 - val_mae: 7.2705\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 139.4783 - mae: 9.0784 - val_loss: 79.9088 - val_mae: 7.0990\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 137.6377 - mae: 8.9768 - val_loss: 75.9385 - val_mae: 6.8918\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 135.3585 - mae: 8.8665 - val_loss: 73.3242 - val_mae: 6.7479\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 133.5794 - mae: 8.7866 - val_loss: 71.1106 - val_mae: 6.6206\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 132.0455 - mae: 8.7301 - val_loss: 69.2381 - val_mae: 6.5080\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 130.5707 - mae: 8.6714 - val_loss: 67.0530 - val_mae: 6.3796\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 129.1064 - mae: 8.6308 - val_loss: 66.3618 - val_mae: 6.3526\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 127.8483 - mae: 8.5970 - val_loss: 64.9907 - val_mae: 6.2743\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 126.7460 - mae: 8.5538 - val_loss: 63.4615 - val_mae: 6.1825\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 125.6803 - mae: 8.5213 - val_loss: 62.6833 - val_mae: 6.1418\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 124.5684 - mae: 8.4808 - val_loss: 61.2618 - val_mae: 6.0745\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 123.6277 - mae: 8.4313 - val_loss: 59.9307 - val_mae: 6.0084\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 122.7287 - mae: 8.3910 - val_loss: 58.6858 - val_mae: 5.9447\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 122.0228 - mae: 8.3375 - val_loss: 57.0862 - val_mae: 5.8532\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 121.2350 - mae: 8.3228 - val_loss: 57.2970 - val_mae: 5.8830\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 120.3368 - mae: 8.2996 - val_loss: 56.5221 - val_mae: 5.8431\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 119.6185 - mae: 8.2451 - val_loss: 55.0420 - val_mae: 5.7495\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 118.8347 - mae: 8.2082 - val_loss: 54.6896 - val_mae: 5.7345\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 118.8386 - mae: 8.2358 - val_loss: 55.2071 - val_mae: 5.7872\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 117.6953 - mae: 8.1753 - val_loss: 53.3873 - val_mae: 5.6598\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 116.9075 - mae: 8.1222 - val_loss: 52.3240 - val_mae: 5.5840\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 116.3493 - mae: 8.0821 - val_loss: 51.7168 - val_mae: 5.5438\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 115.8511 - mae: 8.0569 - val_loss: 51.2603 - val_mae: 5.5216\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 115.1475 - mae: 8.0495 - val_loss: 52.2107 - val_mae: 5.6193\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 114.6789 - mae: 8.0616 - val_loss: 52.0802 - val_mae: 5.6209\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 114.1490 - mae: 8.0262 - val_loss: 51.1260 - val_mae: 5.5548\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 113.5396 - mae: 7.9967 - val_loss: 50.8964 - val_mae: 5.5470\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 113.0773 - mae: 7.9766 - val_loss: 50.6590 - val_mae: 5.5362\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 112.5331 - mae: 7.9470 - val_loss: 49.7432 - val_mae: 5.4654\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 112.0421 - mae: 7.9193 - val_loss: 49.4714 - val_mae: 5.4572\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 111.6262 - mae: 7.8971 - val_loss: 48.7395 - val_mae: 5.4094\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 111.0565 - mae: 7.8780 - val_loss: 48.7454 - val_mae: 5.4184\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 110.6410 - mae: 7.8509 - val_loss: 48.0963 - val_mae: 5.3641\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 110.1760 - mae: 7.8360 - val_loss: 48.3720 - val_mae: 5.3985\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 109.7518 - mae: 7.8405 - val_loss: 48.2790 - val_mae: 5.3997\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 109.1928 - mae: 7.8141 - val_loss: 47.8389 - val_mae: 5.3632\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 108.6589 - mae: 7.7753 - val_loss: 46.8981 - val_mae: 5.2929\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 108.2594 - mae: 7.7371 - val_loss: 46.3462 - val_mae: 5.2547\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 108.2732 - mae: 7.6944 - val_loss: 45.2140 - val_mae: 5.1766\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 107.4520 - mae: 7.6646 - val_loss: 45.3560 - val_mae: 5.1879\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 107.0707 - mae: 7.6841 - val_loss: 46.3567 - val_mae: 5.2814\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 106.6116 - mae: 7.6997 - val_loss: 46.1773 - val_mae: 5.2740\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 106.3069 - mae: 7.6887 - val_loss: 46.0863 - val_mae: 5.2941\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 105.8836 - mae: 7.6427 - val_loss: 44.9028 - val_mae: 5.1842\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 105.5312 - mae: 7.6306 - val_loss: 45.1526 - val_mae: 5.2284\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 105.0132 - mae: 7.5979 - val_loss: 44.0732 - val_mae: 5.1435\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 104.6258 - mae: 7.5594 - val_loss: 43.5637 - val_mae: 5.1125\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 104.2470 - mae: 7.5353 - val_loss: 43.4663 - val_mae: 5.1179\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 103.9220 - mae: 7.5529 - val_loss: 44.4418 - val_mae: 5.2171\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 103.7918 - mae: 7.5484 - val_loss: 43.3844 - val_mae: 5.1333\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 103.0525 - mae: 7.5152 - val_loss: 43.4298 - val_mae: 5.1433\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 103.1194 - mae: 7.5565 - val_loss: 44.6855 - val_mae: 5.2831\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 102.2697 - mae: 7.5273 - val_loss: 43.3458 - val_mae: 5.1559\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 101.8589 - mae: 7.4770 - val_loss: 42.6659 - val_mae: 5.0929\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 101.5182 - mae: 7.4578 - val_loss: 42.6447 - val_mae: 5.0939\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 101.0863 - mae: 7.4152 - val_loss: 41.3141 - val_mae: 4.9875\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 100.7042 - mae: 7.3884 - val_loss: 41.5389 - val_mae: 5.0075\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 100.2698 - mae: 7.3777 - val_loss: 41.6206 - val_mae: 5.0356\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 99.9515 - mae: 7.3756 - val_loss: 41.1935 - val_mae: 4.9932\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 99.5057 - mae: 7.3572 - val_loss: 41.5615 - val_mae: 5.0378\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 99.1123 - mae: 7.3503 - val_loss: 41.2271 - val_mae: 5.0133\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 98.7773 - mae: 7.3404 - val_loss: 41.3787 - val_mae: 5.0352\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 98.5463 - mae: 7.3523 - val_loss: 41.5504 - val_mae: 5.0756\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 98.1788 - mae: 7.3530 - val_loss: 41.4122 - val_mae: 5.0751\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 97.7268 - mae: 7.3185 - val_loss: 40.4514 - val_mae: 4.9786\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 97.4689 - mae: 7.2677 - val_loss: 39.4973 - val_mae: 4.9093\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 97.2382 - mae: 7.2217 - val_loss: 38.6599 - val_mae: 4.8509\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 96.8416 - mae: 7.2020 - val_loss: 39.0079 - val_mae: 4.8942\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 96.6126 - mae: 7.2553 - val_loss: 40.2053 - val_mae: 5.0136\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 96.1271 - mae: 7.2519 - val_loss: 39.5096 - val_mae: 4.9560\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 95.7918 - mae: 7.2012 - val_loss: 38.6022 - val_mae: 4.8861\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 95.6499 - mae: 7.2056 - val_loss: 38.9833 - val_mae: 4.9292\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 95.1544 - mae: 7.1790 - val_loss: 38.4334 - val_mae: 4.8927\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 94.9659 - mae: 7.1328 - val_loss: 37.4095 - val_mae: 4.8236\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=adam; total time=   9.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 27ms/step - loss: 3599.2463 - mae: 53.0438 - val_loss: 3989.0847 - val_mae: 55.2565\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 3230.2034 - mae: 49.8938 - val_loss: 3604.0459 - val_mae: 52.0978\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2901.0430 - mae: 46.8164 - val_loss: 3240.3943 - val_mae: 49.0075\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2589.0354 - mae: 43.8098 - val_loss: 2906.9241 - val_mae: 46.1131\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 2305.7183 - mae: 40.8994 - val_loss: 2599.1873 - val_mae: 43.2840\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 2040.6390 - mae: 38.1213 - val_loss: 2323.3767 - val_mae: 40.5813\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1808.2831 - mae: 35.4919 - val_loss: 2072.1245 - val_mae: 37.9602\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1599.2100 - mae: 32.9699 - val_loss: 1844.9310 - val_mae: 35.4412\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1410.7029 - mae: 30.6060 - val_loss: 1639.9354 - val_mae: 33.0135\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1243.8219 - mae: 28.4226 - val_loss: 1454.5016 - val_mae: 30.6727\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1093.1664 - mae: 26.3073 - val_loss: 1290.1194 - val_mae: 28.4519\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 961.1486 - mae: 24.3299 - val_loss: 1144.9292 - val_mae: 26.3834\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 849.5189 - mae: 22.4869 - val_loss: 1013.1379 - val_mae: 24.5709\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 744.7278 - mae: 20.7349 - val_loss: 900.3239 - val_mae: 22.9139\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 655.3101 - mae: 19.2040 - val_loss: 803.6555 - val_mae: 21.4746\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 580.2368 - mae: 17.8572 - val_loss: 719.6895 - val_mae: 20.1297\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 514.9095 - mae: 16.5960 - val_loss: 646.7339 - val_mae: 18.8745\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 461.7653 - mae: 15.5362 - val_loss: 580.9401 - val_mae: 17.7125\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 414.6187 - mae: 14.6395 - val_loss: 523.5958 - val_mae: 16.7173\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 372.5328 - mae: 13.8531 - val_loss: 474.9130 - val_mae: 15.9387\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 338.2375 - mae: 13.2124 - val_loss: 433.0656 - val_mae: 15.3237\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 309.4904 - mae: 12.6606 - val_loss: 396.7173 - val_mae: 14.7620\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 284.4583 - mae: 12.1773 - val_loss: 365.9721 - val_mae: 14.2450\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 264.3024 - mae: 11.7587 - val_loss: 339.2616 - val_mae: 13.7583\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 245.9347 - mae: 11.3825 - val_loss: 317.3713 - val_mae: 13.3284\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 231.5500 - mae: 11.0685 - val_loss: 298.0637 - val_mae: 12.9211\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 219.1443 - mae: 10.8006 - val_loss: 280.6297 - val_mae: 12.5282\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 208.6410 - mae: 10.5471 - val_loss: 264.7438 - val_mae: 12.1581\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 198.6293 - mae: 10.2968 - val_loss: 251.5651 - val_mae: 11.8622\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 190.1030 - mae: 10.0876 - val_loss: 239.9088 - val_mae: 11.5868\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 183.2351 - mae: 9.8899 - val_loss: 228.7606 - val_mae: 11.3242\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 176.9686 - mae: 9.7148 - val_loss: 218.6327 - val_mae: 11.0717\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 170.5232 - mae: 9.5346 - val_loss: 210.2670 - val_mae: 10.8500\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 165.3402 - mae: 9.3929 - val_loss: 202.3247 - val_mae: 10.6305\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 160.5626 - mae: 9.2636 - val_loss: 195.0576 - val_mae: 10.4216\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 156.0805 - mae: 9.1442 - val_loss: 188.4041 - val_mae: 10.2231\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 151.7007 - mae: 9.0255 - val_loss: 182.6033 - val_mae: 10.0431\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 147.7864 - mae: 8.9098 - val_loss: 176.6512 - val_mae: 9.8613\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 144.2316 - mae: 8.8139 - val_loss: 170.7108 - val_mae: 9.6821\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 140.8363 - mae: 8.7131 - val_loss: 165.2276 - val_mae: 9.5069\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 137.2269 - mae: 8.5978 - val_loss: 160.4755 - val_mae: 9.3462\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 133.8168 - mae: 8.4761 - val_loss: 155.6512 - val_mae: 9.1772\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 130.3752 - mae: 8.3407 - val_loss: 151.1168 - val_mae: 9.0110\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 127.0303 - mae: 8.1942 - val_loss: 146.5453 - val_mae: 8.8597\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 123.6442 - mae: 8.0624 - val_loss: 141.9695 - val_mae: 8.7165\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 120.4367 - mae: 7.9279 - val_loss: 137.7381 - val_mae: 8.5848\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 117.3000 - mae: 7.8000 - val_loss: 133.8646 - val_mae: 8.4603\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 114.6113 - mae: 7.6933 - val_loss: 129.6604 - val_mae: 8.3221\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 111.9969 - mae: 7.5941 - val_loss: 125.6586 - val_mae: 8.1973\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 109.6107 - mae: 7.5009 - val_loss: 122.3035 - val_mae: 8.0859\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 107.2951 - mae: 7.4095 - val_loss: 119.3093 - val_mae: 7.9853\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 105.3363 - mae: 7.3315 - val_loss: 116.1475 - val_mae: 7.8808\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 103.4608 - mae: 7.2565 - val_loss: 112.9795 - val_mae: 7.7757\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 101.6090 - mae: 7.1758 - val_loss: 110.3560 - val_mae: 7.6781\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 99.7305 - mae: 7.0873 - val_loss: 107.9613 - val_mae: 7.5863\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 98.0002 - mae: 7.0067 - val_loss: 105.6999 - val_mae: 7.5091\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 96.4058 - mae: 6.9356 - val_loss: 103.0513 - val_mae: 7.4246\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 94.8293 - mae: 6.8655 - val_loss: 100.6115 - val_mae: 7.3444\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 93.2903 - mae: 6.7923 - val_loss: 98.4285 - val_mae: 7.2709\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 91.8073 - mae: 6.7230 - val_loss: 96.3412 - val_mae: 7.1992\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 90.4102 - mae: 6.6574 - val_loss: 94.2270 - val_mae: 7.1260\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 88.9906 - mae: 6.5936 - val_loss: 92.5034 - val_mae: 7.0640\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 87.7196 - mae: 6.5351 - val_loss: 90.6893 - val_mae: 6.9978\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 86.4792 - mae: 6.4756 - val_loss: 88.5646 - val_mae: 6.9203\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 85.2368 - mae: 6.4137 - val_loss: 86.8979 - val_mae: 6.8551\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 84.0141 - mae: 6.3601 - val_loss: 85.1831 - val_mae: 6.7881\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 82.8954 - mae: 6.3120 - val_loss: 83.4475 - val_mae: 6.7187\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 81.7935 - mae: 6.2592 - val_loss: 82.0879 - val_mae: 6.6617\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 80.7342 - mae: 6.2145 - val_loss: 80.3313 - val_mae: 6.5898\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 79.7303 - mae: 6.1688 - val_loss: 78.8069 - val_mae: 6.5244\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 78.7630 - mae: 6.1310 - val_loss: 77.3177 - val_mae: 6.4600\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 77.7621 - mae: 6.0917 - val_loss: 76.2019 - val_mae: 6.4095\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 76.9498 - mae: 6.0564 - val_loss: 74.8199 - val_mae: 6.3479\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 76.1119 - mae: 6.0204 - val_loss: 73.6234 - val_mae: 6.2928\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 75.2782 - mae: 5.9850 - val_loss: 72.5128 - val_mae: 6.2409\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 74.4676 - mae: 5.9482 - val_loss: 71.3945 - val_mae: 6.1879\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 73.7411 - mae: 5.9120 - val_loss: 70.4387 - val_mae: 6.1401\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 72.9826 - mae: 5.8823 - val_loss: 69.4263 - val_mae: 6.0896\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 72.3727 - mae: 5.8519 - val_loss: 68.5721 - val_mae: 6.0450\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 71.5934 - mae: 5.8240 - val_loss: 67.3989 - val_mae: 5.9878\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 70.9714 - mae: 5.7912 - val_loss: 66.2562 - val_mae: 5.9309\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 70.2802 - mae: 5.7602 - val_loss: 65.4117 - val_mae: 5.8861\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 69.7269 - mae: 5.7357 - val_loss: 64.5898 - val_mae: 5.8437\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 69.1656 - mae: 5.7133 - val_loss: 63.9760 - val_mae: 5.8072\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 68.6760 - mae: 5.6946 - val_loss: 63.3578 - val_mae: 5.7722\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 68.0744 - mae: 5.6697 - val_loss: 62.5385 - val_mae: 5.7324\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 67.6130 - mae: 5.6523 - val_loss: 61.4672 - val_mae: 5.6868\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 67.0816 - mae: 5.6280 - val_loss: 60.8298 - val_mae: 5.6623\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 66.6184 - mae: 5.6059 - val_loss: 60.0813 - val_mae: 5.6380\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 66.1934 - mae: 5.5884 - val_loss: 59.3759 - val_mae: 5.6140\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 65.7206 - mae: 5.5709 - val_loss: 58.9306 - val_mae: 5.5974\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 65.3135 - mae: 5.5546 - val_loss: 58.4847 - val_mae: 5.5796\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 64.9333 - mae: 5.5432 - val_loss: 58.0099 - val_mae: 5.5583\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 64.5539 - mae: 5.5320 - val_loss: 57.5757 - val_mae: 5.5392\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 64.1421 - mae: 5.5203 - val_loss: 56.9828 - val_mae: 5.5204\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 63.8174 - mae: 5.5052 - val_loss: 56.3384 - val_mae: 5.4975\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 63.4240 - mae: 5.4917 - val_loss: 56.0070 - val_mae: 5.4859\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 63.0916 - mae: 5.4745 - val_loss: 55.5493 - val_mae: 5.4686\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 62.7814 - mae: 5.4633 - val_loss: 54.9377 - val_mae: 5.4485\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 62.4741 - mae: 5.4480 - val_loss: 54.5408 - val_mae: 5.4360\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=2, model__n_neurons=125, model__optimizer=adam; total time=  10.3s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 28ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   2.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 74ms/step - loss: 5720798719468463540058193920.0000 - mae: 27545569329152.0000 - val_loss: 28971980662047571443712.0000 - val_mae: 170211557376.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 57870133815403658346496.0000 - mae: 237154025472.0000 - val_loss: 95161771695079705018368.0000 - val_mae: 308482965504.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 113311692524548521984000.0000 - mae: 336207413248.0000 - val_loss: 133368347364253459546112.0000 - val_mae: 365196345344.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 141254456556553859235840.0000 - mae: 375786209280.0000 - val_loss: 149216334308985140150272.0000 - val_mae: 386285305856.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 151637748691836415770624.0000 - mae: 389402984448.0000 - val_loss: 153544230500493403619328.0000 - val_mae: 391847215104.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 153444646905532987211776.0000 - mae: 391720009728.0000 - val_loss: 152626649098014429282304.0000 - val_mae: 390674612224.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 151440238826578455298048.0000 - mae: 389151752192.0000 - val_loss: 149470157183983741304832.0000 - val_mae: 386613706752.0000\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 147845285460026230571008.0000 - mae: 384503906304.0000 - val_loss: 145425114070672109207552.0000 - val_mae: 381346414592.0000\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 143645858951488337870848.0000 - mae: 379003174912.0000 - val_loss: 141081212086088170995712.0000 - val_mae: 375607754752.0000\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 139269918359155285950464.0000 - mae: 373185347584.0000 - val_loss: 136691760694873007587328.0000 - val_mae: 369718493184.0000\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 134900183727108750573568.0000 - mae: 367284027392.0000 - val_loss: 132363531236992318701568.0000 - val_mae: 363817992192.0000\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   3.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 28ms/step - loss: 5713609193199455610863616.0000 - mae: 868754980864.0000 - val_loss: 29054425439190122496.0000 - val_mae: 5390215680.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 58160233255769473024.0000 - mae: 7517942784.0000 - val_loss: 95432780235887607808.0000 - val_mae: 9768969216.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 113703602449152999424.0000 - mae: 10650126336.0000 - val_loss: 133748192004718723072.0000 - val_mae: 11564955648.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 141684801185540734976.0000 - mae: 11901485056.0000 - val_loss: 149641368701430136832.0000 - val_mae: 12232799232.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 152076883713977221120.0000 - mae: 12331822080.0000 - val_loss: 153981607289192185856.0000 - val_mae: 12408933376.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 153879502241390395392.0000 - mae: 12404813824.0000 - val_loss: 153061430405952962560.0000 - val_mae: 12371799040.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 151865408045537296384.0000 - mae: 12323320832.0000 - val_loss: 149895962817864925184.0000 - val_mae: 12243200000.0000\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 148258640470525083648.0000 - mae: 12176065536.0000 - val_loss: 145839380637883039744.0000 - val_mae: 12076397568.0000\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 144046754472143093760.0000 - mae: 12001845248.0000 - val_loss: 141483168345192660992.0000 - val_mae: 11894670336.0000\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 139658295702433300480.0000 - mae: 11817598976.0000 - val_loss: 137081189980507734016.0000 - val_mae: 11708168192.0000\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 135276240488443674624.0000 - mae: 11630718976.0000 - val_loss: 132740599549024796672.0000 - val_mae: 11521311744.0000\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.9, model__n_hidden=2, model__n_neurons=25, model__optimizer=momentum; total time=   1.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 7398895386624.0000 - mae: 964441.2500 - val_loss: 2753871021604864.0000 - val_mae: 51363040.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1916616709258422412902400.0000 - mae: 513914011648.0000 - val_loss: 686706832816233085174546432.0000 - val_mae: 25699838590976.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 484321440537047838651983794228166656.0000 - mae: 250556606658379776.0000 - val_loss: inf - val_mae: 13433161266562596864.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: inf - mae: 135467186920194696019968.0000 - val_loss: inf - val_mae: 6190162379599668167311360.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: inf - mae: 63799378967694153101422362624.0000 - val_loss: inf - val_mae: 3119920197397709059758279884800.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: inf - mae: 29784485770235092366074857478160384.0000 - val_loss: inf - val_mae: 1547403182133871406666729461869707264.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan   \n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.8s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 22ms/step - loss: 15110339821568.0000 - mae: 1443344.0000 - val_loss: 4648027029504000.0000 - val_mae: 66950564.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 2159589007631604303003648.0000 - mae: 538413498368.0000 - val_loss: 757789147494983997180608512.0000 - val_mae: 27033348341760.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 350696489291089835096114064294674432.0000 - mae: 217975689125560320.0000 - val_loss: inf - val_mae: 10074774467776610304.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: inf - mae: 83859538070230708256768.0000 - val_loss: inf - val_mae: 3674325671076130803679232.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: inf - mae: 30238873331936926721038614528.0000 - val_loss: inf - val_mae: 1399422607871859270567795359744.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: inf - mae: 10521110030781929137636043380490240.0000 - val_loss: inf - val_mae: 517280554213388088751742061427818496.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan   \n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.4s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 24ms/step - loss: 16332936192.0000 - mae: 48650.6914 - val_loss: 4364600082432.0000 - val_mae: 2043486.5000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1554613177233228955648.0000 - mae: 14251580416.0000 - val_loss: 515407037422230508142592.0000 - val_mae: 701478141952.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 195777277105389587697143610605568.0000 - mae: 5368880381820928.0000 - val_loss: 57793097036513735312595925292023808.0000 - val_mae: 235842306400321536.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: inf - mae: 1796140521561666879488.0000 - val_loss: inf - val_mae: 81923764849597302702080.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: inf - mae: 659674531328761339207221248.0000 - val_loss: inf - val_mae: 28199627980738776437343911936.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: inf - mae: 218143062423906097787478906765312.0000 - val_loss: inf - val_mae: 9319734407466901497226477139132416.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan \n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=momentum; total time=   1.8s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 21ms/step - loss: 82173.2422 - mae: 274.4535 - val_loss: 85247.1484 - val_mae: 281.6966\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 82131.3750 - mae: 274.3808 - val_loss: 85203.9375 - val_mae: 281.6236\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 82088.9688 - mae: 274.3079 - val_loss: 85160.8672 - val_mae: 281.5508\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 82046.8516 - mae: 274.2351 - val_loss: 85117.7578 - val_mae: 281.4779\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 82005.2578 - mae: 274.1625 - val_loss: 85074.6797 - val_mae: 281.4050\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 81962.6094 - mae: 274.0896 - val_loss: 85031.8516 - val_mae: 281.3326\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 81920.9922 - mae: 274.0172 - val_loss: 84988.7188 - val_mae: 281.2596\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81878.5625 - mae: 273.9442 - val_loss: 84945.7656 - val_mae: 281.1869\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81836.5938 - mae: 273.8716 - val_loss: 84902.6094 - val_mae: 281.1138\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 81794.4219 - mae: 273.7987 - val_loss: 84859.7812 - val_mae: 281.0413\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 81752.9375 - mae: 273.7264 - val_loss: 84816.6562 - val_mae: 280.9683\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81710.7500 - mae: 273.6537 - val_loss: 84774.0859 - val_mae: 280.8961\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81668.7656 - mae: 273.5813 - val_loss: 84731.4688 - val_mae: 280.8239\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81627.5781 - mae: 273.5091 - val_loss: 84688.3906 - val_mae: 280.7509\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 81584.8750 - mae: 273.4362 - val_loss: 84645.8516 - val_mae: 280.6788\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 81543.0234 - mae: 273.3636 - val_loss: 84603.0000 - val_mae: 280.6061\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 81501.6484 - mae: 273.2911 - val_loss: 84559.6875 - val_mae: 280.5327\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 81459.7188 - mae: 273.2181 - val_loss: 84516.5703 - val_mae: 280.4595\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 81417.5234 - mae: 273.1452 - val_loss: 84474.1406 - val_mae: 280.3875\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 81375.7500 - mae: 273.0731 - val_loss: 84431.6797 - val_mae: 280.3154\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 81334.0469 - mae: 273.0009 - val_loss: 84389.0469 - val_mae: 280.2430\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 81292.4688 - mae: 272.9284 - val_loss: 84346.2812 - val_mae: 280.1703\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81251.1328 - mae: 272.8560 - val_loss: 84303.3047 - val_mae: 280.0973\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 81208.9922 - mae: 272.7834 - val_loss: 84260.7578 - val_mae: 280.0250\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 81167.4844 - mae: 272.7110 - val_loss: 84218.2188 - val_mae: 279.9527\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 81125.7578 - mae: 272.6387 - val_loss: 84175.7109 - val_mae: 279.8804\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 81084.1484 - mae: 272.5663 - val_loss: 84133.0547 - val_mae: 279.8079\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 81042.5078 - mae: 272.4940 - val_loss: 84090.5938 - val_mae: 279.7356\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 81001.5078 - mae: 272.4219 - val_loss: 84047.6797 - val_mae: 279.6626\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 80958.9375 - mae: 272.3488 - val_loss: 84005.4062 - val_mae: 279.5906\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 80917.6875 - mae: 272.2768 - val_loss: 83962.7656 - val_mae: 279.5180\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 80876.1250 - mae: 272.2042 - val_loss: 83919.9531 - val_mae: 279.4451\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 80834.8359 - mae: 272.1317 - val_loss: 83877.1172 - val_mae: 279.3722\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 80792.6250 - mae: 272.0591 - val_loss: 83835.1328 - val_mae: 279.3006\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80751.5547 - mae: 271.9873 - val_loss: 83792.9531 - val_mae: 279.2287\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 80709.6562 - mae: 271.9153 - val_loss: 83750.9688 - val_mae: 279.1571\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 80669.0391 - mae: 271.8434 - val_loss: 83707.7656 - val_mae: 279.0834\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80626.7969 - mae: 271.7701 - val_loss: 83665.1484 - val_mae: 279.0108\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80585.5703 - mae: 271.6975 - val_loss: 83622.2656 - val_mae: 278.9377\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 80543.6562 - mae: 271.6248 - val_loss: 83579.9297 - val_mae: 278.8654\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80502.0547 - mae: 271.5525 - val_loss: 83537.6641 - val_mae: 278.7933\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 80460.5234 - mae: 271.4803 - val_loss: 83495.5625 - val_mae: 278.7214\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 80418.9922 - mae: 271.4083 - val_loss: 83453.4531 - val_mae: 278.6495\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 80378.3438 - mae: 271.3362 - val_loss: 83410.5625 - val_mae: 278.5762\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 80336.0391 - mae: 271.2634 - val_loss: 83368.5156 - val_mae: 278.5044\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80295.1875 - mae: 271.1915 - val_loss: 83326.1406 - val_mae: 278.4320\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 80253.9375 - mae: 271.1193 - val_loss: 83283.7500 - val_mae: 278.3595\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80212.2891 - mae: 271.0469 - val_loss: 83241.7188 - val_mae: 278.2876\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 80171.2422 - mae: 270.9750 - val_loss: 83199.5625 - val_mae: 278.2155\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 80130.0547 - mae: 270.9028 - val_loss: 83157.1797 - val_mae: 278.1430\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 80088.6016 - mae: 270.8305 - val_loss: 83115.0859 - val_mae: 278.0710\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 80047.3828 - mae: 270.7584 - val_loss: 83072.7344 - val_mae: 277.9985\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 80006.0234 - mae: 270.6860 - val_loss: 83030.5469 - val_mae: 277.9262\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 79964.6172 - mae: 270.6137 - val_loss: 82988.3047 - val_mae: 277.8539\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 79923.7188 - mae: 270.5413 - val_loss: 82945.6484 - val_mae: 277.7809\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 79881.5781 - mae: 270.4686 - val_loss: 82903.6328 - val_mae: 277.7089\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 79840.6719 - mae: 270.3966 - val_loss: 82861.3125 - val_mae: 277.6364\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79799.4297 - mae: 270.3240 - val_loss: 82818.8516 - val_mae: 277.5636\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79757.5156 - mae: 270.2515 - val_loss: 82776.9375 - val_mae: 277.4917\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79717.1094 - mae: 270.1796 - val_loss: 82734.3047 - val_mae: 277.4186\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 79675.2344 - mae: 270.1067 - val_loss: 82692.2891 - val_mae: 277.3466\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 79633.9844 - mae: 270.0347 - val_loss: 82650.3359 - val_mae: 277.2745\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79592.9375 - mae: 269.9625 - val_loss: 82608.1875 - val_mae: 277.2022\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79551.6641 - mae: 269.8901 - val_loss: 82565.9062 - val_mae: 277.1296\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79510.2812 - mae: 269.8176 - val_loss: 82523.7500 - val_mae: 277.0572\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79469.3359 - mae: 269.7451 - val_loss: 82481.1328 - val_mae: 276.9840\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79427.8906 - mae: 269.6722 - val_loss: 82438.6484 - val_mae: 276.9111\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79386.4766 - mae: 269.5995 - val_loss: 82396.4062 - val_mae: 276.8385\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79345.4062 - mae: 269.5273 - val_loss: 82354.4062 - val_mae: 276.7662\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79304.1484 - mae: 269.4550 - val_loss: 82312.5078 - val_mae: 276.6942\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79262.8672 - mae: 269.3829 - val_loss: 82270.9062 - val_mae: 276.6226\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79222.4141 - mae: 269.3111 - val_loss: 82228.4297 - val_mae: 276.5496\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79180.8906 - mae: 269.2386 - val_loss: 82186.7578 - val_mae: 276.4778\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 79140.0156 - mae: 269.1665 - val_loss: 82144.7188 - val_mae: 276.4055\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 79099.3359 - mae: 269.0943 - val_loss: 82102.4297 - val_mae: 276.3327\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 79057.9375 - mae: 269.0217 - val_loss: 82060.5469 - val_mae: 276.2605\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 79016.6328 - mae: 268.9495 - val_loss: 82018.6797 - val_mae: 276.1884\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78975.7656 - mae: 268.8773 - val_loss: 81976.7188 - val_mae: 276.1161\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78934.8281 - mae: 268.8051 - val_loss: 81934.6016 - val_mae: 276.0435\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 78893.5469 - mae: 268.7327 - val_loss: 81892.6797 - val_mae: 275.9713\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 78852.7578 - mae: 268.6604 - val_loss: 81850.6562 - val_mae: 275.8988\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 78811.5547 - mae: 268.5879 - val_loss: 81808.5547 - val_mae: 275.8263\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 78770.7656 - mae: 268.5157 - val_loss: 81766.4609 - val_mae: 275.7536\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 78729.6484 - mae: 268.4433 - val_loss: 81724.7500 - val_mae: 275.6817\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 78688.9219 - mae: 268.3715 - val_loss: 81683.1094 - val_mae: 275.6098\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78648.0078 - mae: 268.2995 - val_loss: 81641.5859 - val_mae: 275.5381\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78607.1641 - mae: 268.2278 - val_loss: 81600.0625 - val_mae: 275.4664\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78567.2188 - mae: 268.1561 - val_loss: 81557.9531 - val_mae: 275.3936\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 78526.2266 - mae: 268.0836 - val_loss: 81515.9609 - val_mae: 275.3211\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78485.0547 - mae: 268.0115 - val_loss: 81474.5938 - val_mae: 275.2496\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 78444.8594 - mae: 267.9398 - val_loss: 81432.7109 - val_mae: 275.1772\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78403.4062 - mae: 267.8674 - val_loss: 81391.4375 - val_mae: 275.1058\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 78362.8203 - mae: 267.7957 - val_loss: 81349.7031 - val_mae: 275.0336\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78322.2812 - mae: 267.7236 - val_loss: 81307.8359 - val_mae: 274.9612\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 78281.4062 - mae: 267.6514 - val_loss: 81266.1172 - val_mae: 274.8890\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78240.5234 - mae: 267.5792 - val_loss: 81224.5000 - val_mae: 274.8170\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 78200.1719 - mae: 267.5072 - val_loss: 81182.5312 - val_mae: 274.7443\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 78158.9375 - mae: 267.4348 - val_loss: 81140.9141 - val_mae: 274.6722\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 78118.2656 - mae: 267.3626 - val_loss: 81099.1016 - val_mae: 274.5998\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 78077.6953 - mae: 267.2905 - val_loss: 81057.5156 - val_mae: 274.5277\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=5, model__optimizer=adam; total time=   8.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 13881.6504 - mae: 108.8581 - val_loss: 12605.7305 - val_mae: 105.8324\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13870.5615 - mae: 108.8047 - val_loss: 12595.7139 - val_mae: 105.7834\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13859.4883 - mae: 108.7516 - val_loss: 12585.7061 - val_mae: 105.7344\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13848.6865 - mae: 108.6991 - val_loss: 12575.6738 - val_mae: 105.6852\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13837.6221 - mae: 108.6460 - val_loss: 12565.7383 - val_mae: 105.6365\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13826.5996 - mae: 108.5947 - val_loss: 12555.9609 - val_mae: 105.5885\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13815.7324 - mae: 108.5432 - val_loss: 12546.2207 - val_mae: 105.5407\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13804.8770 - mae: 108.4921 - val_loss: 12536.6963 - val_mae: 105.4939\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13794.5566 - mae: 108.4418 - val_loss: 12526.9434 - val_mae: 105.4459\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13783.8145 - mae: 108.3912 - val_loss: 12517.1943 - val_mae: 105.3979\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13772.9990 - mae: 108.3410 - val_loss: 12507.5791 - val_mae: 105.3505\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13762.3828 - mae: 108.2897 - val_loss: 12497.8379 - val_mae: 105.3025\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13751.5732 - mae: 108.2400 - val_loss: 12488.1562 - val_mae: 105.2546\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13740.5430 - mae: 108.1892 - val_loss: 12478.6367 - val_mae: 105.2076\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13730.0732 - mae: 108.1391 - val_loss: 12468.9805 - val_mae: 105.1598\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13719.3574 - mae: 108.0874 - val_loss: 12459.1992 - val_mae: 105.1113\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13708.4697 - mae: 108.0374 - val_loss: 12449.5088 - val_mae: 105.0632\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13697.9219 - mae: 107.9876 - val_loss: 12439.7617 - val_mae: 105.0148\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13687.1016 - mae: 107.9358 - val_loss: 12430.1494 - val_mae: 104.9670\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13676.3965 - mae: 107.8848 - val_loss: 12420.5312 - val_mae: 104.9191\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13665.5908 - mae: 107.8338 - val_loss: 12410.9854 - val_mae: 104.8716\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13655.1553 - mae: 107.7850 - val_loss: 12401.2998 - val_mae: 104.8232\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13644.5391 - mae: 107.7337 - val_loss: 12391.5996 - val_mae: 104.7747\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13633.6660 - mae: 107.6829 - val_loss: 12382.0420 - val_mae: 104.7270\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13623.0322 - mae: 107.6315 - val_loss: 12372.4180 - val_mae: 104.6788\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13612.4346 - mae: 107.5824 - val_loss: 12362.8633 - val_mae: 104.6310\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13602.0244 - mae: 107.5304 - val_loss: 12353.2588 - val_mae: 104.5828\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13591.0605 - mae: 107.4794 - val_loss: 12343.9746 - val_mae: 104.5363\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13580.9668 - mae: 107.4316 - val_loss: 12334.3545 - val_mae: 104.4879\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13570.3516 - mae: 107.3806 - val_loss: 12324.8496 - val_mae: 104.4401\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13559.7393 - mae: 107.3293 - val_loss: 12315.2979 - val_mae: 104.3920\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13549.2676 - mae: 107.2794 - val_loss: 12305.6807 - val_mae: 104.3435\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13538.6504 - mae: 107.2279 - val_loss: 12296.1709 - val_mae: 104.2956\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13528.2256 - mae: 107.1782 - val_loss: 12286.7246 - val_mae: 104.2479\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13517.3906 - mae: 107.1283 - val_loss: 12277.6172 - val_mae: 104.2020\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13507.4355 - mae: 107.0787 - val_loss: 12268.0713 - val_mae: 104.1537\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13496.8799 - mae: 107.0289 - val_loss: 12258.6016 - val_mae: 104.1057\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13486.3271 - mae: 106.9780 - val_loss: 12249.2637 - val_mae: 104.0584\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13475.9678 - mae: 106.9277 - val_loss: 12239.9062 - val_mae: 104.0109\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13465.6641 - mae: 106.8783 - val_loss: 12230.5859 - val_mae: 103.9635\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13455.1758 - mae: 106.8267 - val_loss: 12221.4141 - val_mae: 103.9170\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13444.9268 - mae: 106.7773 - val_loss: 12212.1191 - val_mae: 103.8697\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 13434.5713 - mae: 106.7299 - val_loss: 12202.7383 - val_mae: 103.8219\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13423.9854 - mae: 106.6797 - val_loss: 12193.5059 - val_mae: 103.7748\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 13413.8018 - mae: 106.6317 - val_loss: 12184.1357 - val_mae: 103.7270\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13403.6074 - mae: 106.5842 - val_loss: 12174.6748 - val_mae: 103.6785\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13393.2900 - mae: 106.5349 - val_loss: 12165.2314 - val_mae: 103.6301\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13382.5830 - mae: 106.4862 - val_loss: 12156.1367 - val_mae: 103.5836\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13372.5547 - mae: 106.4403 - val_loss: 12146.8955 - val_mae: 103.5362\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13362.2676 - mae: 106.3924 - val_loss: 12137.8799 - val_mae: 103.4900\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13352.2217 - mae: 106.3451 - val_loss: 12128.8145 - val_mae: 103.4435\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13342.2939 - mae: 106.2993 - val_loss: 12119.7051 - val_mae: 103.3966\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13332.0391 - mae: 106.2509 - val_loss: 12110.7920 - val_mae: 103.3509\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13322.1396 - mae: 106.2036 - val_loss: 12101.7051 - val_mae: 103.3041\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13312.1475 - mae: 106.1590 - val_loss: 12092.6074 - val_mae: 103.2571\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13301.9102 - mae: 106.1103 - val_loss: 12083.5947 - val_mae: 103.2106\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13291.9424 - mae: 106.0641 - val_loss: 12074.4121 - val_mae: 103.1631\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13281.6631 - mae: 106.0155 - val_loss: 12065.3604 - val_mae: 103.1162\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13271.5889 - mae: 105.9688 - val_loss: 12056.3018 - val_mae: 103.0693\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13261.6123 - mae: 105.9218 - val_loss: 12047.0645 - val_mae: 103.0214\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13251.3477 - mae: 105.8727 - val_loss: 12037.9170 - val_mae: 102.9739\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13241.0176 - mae: 105.8244 - val_loss: 12028.8955 - val_mae: 102.9270\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13230.9102 - mae: 105.7775 - val_loss: 12019.8896 - val_mae: 102.8802\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 13221.0586 - mae: 105.7294 - val_loss: 12010.7334 - val_mae: 102.8325\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13211.0176 - mae: 105.6824 - val_loss: 12001.6299 - val_mae: 102.7850\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13200.6914 - mae: 105.6357 - val_loss: 11992.8584 - val_mae: 102.7393\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13191.0967 - mae: 105.5891 - val_loss: 11983.8086 - val_mae: 102.6920\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13180.7881 - mae: 105.5410 - val_loss: 11974.9492 - val_mae: 102.6457\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13171.0439 - mae: 105.4956 - val_loss: 11965.9053 - val_mae: 102.5983\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13160.9795 - mae: 105.4466 - val_loss: 11956.9053 - val_mae: 102.5511\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13150.8115 - mae: 105.3982 - val_loss: 11947.9746 - val_mae: 102.5043\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13140.7559 - mae: 105.3509 - val_loss: 11939.0977 - val_mae: 102.4577\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13131.0439 - mae: 105.3042 - val_loss: 11930.0137 - val_mae: 102.4099\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13120.9854 - mae: 105.2575 - val_loss: 11921.0137 - val_mae: 102.3625\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13111.0146 - mae: 105.2103 - val_loss: 11912.1104 - val_mae: 102.3156\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13100.9971 - mae: 105.1622 - val_loss: 11903.2852 - val_mae: 102.2691\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13090.9893 - mae: 105.1148 - val_loss: 11894.5430 - val_mae: 102.2230\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13081.3633 - mae: 105.0680 - val_loss: 11885.6006 - val_mae: 102.1757\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13071.3418 - mae: 105.0213 - val_loss: 11876.8252 - val_mae: 102.1292\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13061.6719 - mae: 104.9744 - val_loss: 11867.8770 - val_mae: 102.0818\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13051.4668 - mae: 104.9262 - val_loss: 11859.1641 - val_mae: 102.0356\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 13041.8447 - mae: 104.8790 - val_loss: 11850.3633 - val_mae: 101.9889\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 13032.1201 - mae: 104.8325 - val_loss: 11841.6426 - val_mae: 101.9426\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 13022.5400 - mae: 104.7866 - val_loss: 11832.8105 - val_mae: 101.8956\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13012.8389 - mae: 104.7403 - val_loss: 11824.0049 - val_mae: 101.8487\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13002.8438 - mae: 104.6924 - val_loss: 11815.4189 - val_mae: 101.8030\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12993.0537 - mae: 104.6460 - val_loss: 11806.8564 - val_mae: 101.7575\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 12983.4150 - mae: 104.5989 - val_loss: 11798.1572 - val_mae: 101.7111\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 12973.5518 - mae: 104.5511 - val_loss: 11789.5557 - val_mae: 101.6652\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12964.0127 - mae: 104.5044 - val_loss: 11780.7373 - val_mae: 101.6179\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12954.1807 - mae: 104.4584 - val_loss: 11772.0479 - val_mae: 101.5713\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 12944.7197 - mae: 104.4107 - val_loss: 11763.2607 - val_mae: 101.5242\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 12934.6045 - mae: 104.3634 - val_loss: 11754.6699 - val_mae: 101.4781\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12925.4404 - mae: 104.3181 - val_loss: 11745.7617 - val_mae: 101.4300\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 12915.6709 - mae: 104.2705 - val_loss: 11737.0820 - val_mae: 101.3833\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12905.6494 - mae: 104.2219 - val_loss: 11728.6787 - val_mae: 101.3382\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 12896.3203 - mae: 104.1761 - val_loss: 11720.0020 - val_mae: 101.2915\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 12886.7393 - mae: 104.1300 - val_loss: 11711.3799 - val_mae: 101.2449\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 12877.1582 - mae: 104.0836 - val_loss: 11702.8818 - val_mae: 101.1990\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 12867.5215 - mae: 104.0383 - val_loss: 11694.3203 - val_mae: 101.1527\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=5, model__optimizer=adam; total time=  10.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 13510.4678 - mae: 76.6243 - val_loss: 8987.7861 - val_mae: 68.4309\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13498.9746 - mae: 76.5800 - val_loss: 8976.8076 - val_mae: 68.3763\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13488.0117 - mae: 76.5359 - val_loss: 8965.6777 - val_mae: 68.3208\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13476.9219 - mae: 76.4895 - val_loss: 8954.6084 - val_mae: 68.2655\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13466.0107 - mae: 76.4440 - val_loss: 8943.4482 - val_mae: 68.2098\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 13454.4404 - mae: 76.3965 - val_loss: 8932.5908 - val_mae: 68.1570\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13443.1914 - mae: 76.3531 - val_loss: 8921.7324 - val_mae: 68.1071\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13432.4922 - mae: 76.3101 - val_loss: 8910.6260 - val_mae: 68.0560\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13420.9258 - mae: 76.2643 - val_loss: 8899.8945 - val_mae: 68.0065\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13410.1504 - mae: 76.2194 - val_loss: 8888.8350 - val_mae: 67.9555\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13398.8154 - mae: 76.1772 - val_loss: 8878.0557 - val_mae: 67.9057\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13387.9189 - mae: 76.1338 - val_loss: 8867.2129 - val_mae: 67.8556\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13377.1855 - mae: 76.0919 - val_loss: 8856.1299 - val_mae: 67.8043\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13365.9043 - mae: 76.0467 - val_loss: 8845.2510 - val_mae: 67.7540\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13354.7559 - mae: 76.0038 - val_loss: 8834.4854 - val_mae: 67.7041\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13343.7559 - mae: 75.9632 - val_loss: 8823.7871 - val_mae: 67.6544\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 13332.8916 - mae: 75.9178 - val_loss: 8813.0068 - val_mae: 67.6044\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 13321.9219 - mae: 75.8772 - val_loss: 8802.1484 - val_mae: 67.5539\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13310.7852 - mae: 75.8351 - val_loss: 8791.2217 - val_mae: 67.5031\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13299.9697 - mae: 75.7911 - val_loss: 8780.1240 - val_mae: 67.4515\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13288.4346 - mae: 75.7469 - val_loss: 8769.4795 - val_mae: 67.4018\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13277.8740 - mae: 75.7057 - val_loss: 8758.6104 - val_mae: 67.3512\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13266.7178 - mae: 75.6642 - val_loss: 8747.9355 - val_mae: 67.3014\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 13255.9248 - mae: 75.6212 - val_loss: 8737.3242 - val_mae: 67.2517\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 13245.0371 - mae: 75.5789 - val_loss: 8726.7959 - val_mae: 67.2025\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13234.1699 - mae: 75.5386 - val_loss: 8716.3926 - val_mae: 67.1538\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13223.7852 - mae: 75.4969 - val_loss: 8705.5674 - val_mae: 67.1031\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13212.8535 - mae: 75.4553 - val_loss: 8694.8340 - val_mae: 67.0527\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 13201.8242 - mae: 75.4141 - val_loss: 8684.5039 - val_mae: 67.0041\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13191.3066 - mae: 75.3723 - val_loss: 8674.0420 - val_mae: 66.9549\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13180.5977 - mae: 75.3323 - val_loss: 8663.4551 - val_mae: 66.9051\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13170.0146 - mae: 75.2917 - val_loss: 8652.8428 - val_mae: 66.8551\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13159.0859 - mae: 75.2494 - val_loss: 8642.3594 - val_mae: 66.8057\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13148.5244 - mae: 75.2079 - val_loss: 8631.7725 - val_mae: 66.7557\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 13137.7852 - mae: 75.1676 - val_loss: 8621.3037 - val_mae: 66.7063\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 13126.8467 - mae: 75.1258 - val_loss: 8611.0801 - val_mae: 66.6578\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13116.2539 - mae: 75.0863 - val_loss: 8600.6289 - val_mae: 66.6084\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13105.5762 - mae: 75.0468 - val_loss: 8590.0244 - val_mae: 66.5581\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 13095.0039 - mae: 75.0075 - val_loss: 8579.2852 - val_mae: 66.5071\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13084.2451 - mae: 74.9654 - val_loss: 8568.6484 - val_mae: 66.4566\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13073.1953 - mae: 74.9233 - val_loss: 8558.3867 - val_mae: 66.4079\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13062.5703 - mae: 74.8839 - val_loss: 8548.1006 - val_mae: 66.3589\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 13052.3740 - mae: 74.8456 - val_loss: 8537.4395 - val_mae: 66.3082\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 13041.5576 - mae: 74.8045 - val_loss: 8527.0137 - val_mae: 66.2585\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13031.0273 - mae: 74.7641 - val_loss: 8516.6133 - val_mae: 66.2088\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 13020.0811 - mae: 74.7225 - val_loss: 8506.5244 - val_mae: 66.1606\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 13010.0244 - mae: 74.6844 - val_loss: 8495.9229 - val_mae: 66.1099\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 12999.2852 - mae: 74.6442 - val_loss: 8485.4619 - val_mae: 66.0599\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 12988.8389 - mae: 74.6058 - val_loss: 8475.0293 - val_mae: 66.0098\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 12977.7539 - mae: 74.5654 - val_loss: 8464.9697 - val_mae: 65.9615\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 12967.4385 - mae: 74.5262 - val_loss: 8454.6426 - val_mae: 65.9120\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 12956.7988 - mae: 74.4851 - val_loss: 8444.3369 - val_mae: 65.8624\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12946.5332 - mae: 74.4479 - val_loss: 8433.8438 - val_mae: 65.8119\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 12935.9189 - mae: 74.4055 - val_loss: 8423.4824 - val_mae: 65.7621\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12925.2705 - mae: 74.3663 - val_loss: 8413.2979 - val_mae: 65.7131\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 12915.2139 - mae: 74.3273 - val_loss: 8402.8643 - val_mae: 65.6628\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12904.4941 - mae: 74.2879 - val_loss: 8392.7051 - val_mae: 65.6137\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 12894.1748 - mae: 74.2469 - val_loss: 8382.6240 - val_mae: 65.5649\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 12883.8271 - mae: 74.2095 - val_loss: 8372.6670 - val_mae: 65.5167\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 12873.5947 - mae: 74.1728 - val_loss: 8362.6182 - val_mae: 65.4680\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 30ms/step - loss: 12863.7227 - mae: 74.1338 - val_loss: 8352.3154 - val_mae: 65.4180\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 12852.8623 - mae: 74.0968 - val_loss: 8342.5576 - val_mae: 65.3706\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 12842.7178 - mae: 74.0578 - val_loss: 8332.6426 - val_mae: 65.3224\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 12832.8486 - mae: 74.0237 - val_loss: 8322.3047 - val_mae: 65.2722\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 12822.0342 - mae: 73.9825 - val_loss: 8312.3340 - val_mae: 65.2237\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 12811.8486 - mae: 73.9449 - val_loss: 8302.2090 - val_mae: 65.1744\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 12801.5137 - mae: 73.9080 - val_loss: 8292.2451 - val_mae: 65.1257\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 12791.4658 - mae: 73.8705 - val_loss: 8282.0781 - val_mae: 65.0761\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 12780.9170 - mae: 73.8309 - val_loss: 8272.1318 - val_mae: 65.0275\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12771.0068 - mae: 73.7956 - val_loss: 8262.1582 - val_mae: 64.9787\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12760.8477 - mae: 73.7576 - val_loss: 8252.1797 - val_mae: 64.9298\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12750.5439 - mae: 73.7204 - val_loss: 8242.3838 - val_mae: 64.8847\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 12740.4688 - mae: 73.6855 - val_loss: 8232.5107 - val_mae: 64.8405\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 12730.1172 - mae: 73.6461 - val_loss: 8222.5518 - val_mae: 64.7960\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 12720.2139 - mae: 73.6105 - val_loss: 8212.3379 - val_mae: 64.7503\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12709.5420 - mae: 73.5706 - val_loss: 8202.5645 - val_mae: 64.7064\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 12699.4814 - mae: 73.5345 - val_loss: 8192.6689 - val_mae: 64.6620\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 12689.3740 - mae: 73.4978 - val_loss: 8182.7505 - val_mae: 64.6174\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 12679.1865 - mae: 73.4597 - val_loss: 8172.8755 - val_mae: 64.5730\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 12669.2129 - mae: 73.4230 - val_loss: 8162.7554 - val_mae: 64.5274\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 12659.2070 - mae: 73.3859 - val_loss: 8152.5864 - val_mae: 64.4816\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 12648.6006 - mae: 73.3478 - val_loss: 8142.9614 - val_mae: 64.4381\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 12638.7344 - mae: 73.3124 - val_loss: 8133.2744 - val_mae: 64.3944\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 12628.9424 - mae: 73.2764 - val_loss: 8123.5781 - val_mae: 64.3505\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 12618.6836 - mae: 73.2398 - val_loss: 8114.0972 - val_mae: 64.3097\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 12608.9912 - mae: 73.2045 - val_loss: 8104.3755 - val_mae: 64.2695\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 26ms/step - loss: 12599.0684 - mae: 73.1690 - val_loss: 8094.5581 - val_mae: 64.2290\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 12588.9668 - mae: 73.1311 - val_loss: 8084.7744 - val_mae: 64.1885\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 12578.9512 - mae: 73.0933 - val_loss: 8075.0557 - val_mae: 64.1483\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 12568.7705 - mae: 73.0562 - val_loss: 8065.2754 - val_mae: 64.1078\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 12558.8799 - mae: 73.0209 - val_loss: 8055.2051 - val_mae: 64.0660\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 12548.8291 - mae: 72.9828 - val_loss: 8045.3589 - val_mae: 64.0252\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 12538.8057 - mae: 72.9463 - val_loss: 8035.7041 - val_mae: 63.9850\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 12528.5615 - mae: 72.9089 - val_loss: 8026.3350 - val_mae: 63.9461\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 12519.1436 - mae: 72.8756 - val_loss: 8016.3774 - val_mae: 63.9046\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 12508.6523 - mae: 72.8377 - val_loss: 8006.8647 - val_mae: 63.8651\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 12499.4629 - mae: 72.8041 - val_loss: 7996.8066 - val_mae: 63.8276\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 12488.7959 - mae: 72.7650 - val_loss: 7987.4248 - val_mae: 63.7925\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 12479.2490 - mae: 72.7297 - val_loss: 7977.8823 - val_mae: 63.7569\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 12469.2637 - mae: 72.6951 - val_loss: 7968.6177 - val_mae: 63.7223\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=5, model__optimizer=adam; total time=  13.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 55222391013376.0000 - mae: 2620234.7500 - val_loss: 21464623047770112.0000 - val_mae: 143397904.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 20592976629825473672642560.0000 - mae: 1672294563840.0000 - val_loss: 7732273013972478157662453760.0000 - val_mae: 86243958325248.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: inf - mae: 981444351198494720.0000 - val_loss: inf - val_mae: 54108893547228823552.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: inf - mae: 635136458494973789077504.0000 - val_loss: inf - val_mae: 29995081475853430539943936.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: inf - mae: 359829554432308549033406758912.0000 - val_loss: inf - val_mae: 18172725245100377197459250610176.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: inf - mae: 202748928317331295949180638037278720.0000 - val_loss: inf - val_mae: inf\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=25, model__optimizer=sgd; total time=   1.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 253325901824.0000 - mae: 185546.2656 - val_loss: 82132860928000.0000 - val_mae: 8899354.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 53185512001011005259776.0000 - mae: 83911196672.0000 - val_loss: 19563376221037393476583424.0000 - val_mae: 4343258152960.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 12595293421885055212792824807292928.0000 - mae: 41019119520186368.0000 - val_loss: 3981023117275848387321256753848385536.0000 - val_mae: 1960308698362413056.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: inf - mae: 19088151236239285026816.0000 - val_loss: inf - val_mae: 866476499462612972470272.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: inf - mae: 8349721841311873910647554048.0000 - val_loss: inf - val_mae: 399252663188873469377736867840.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: inf - mae: 3531537160891486435359301327388672.0000 - val_loss: inf - val_mae: 179110809890387959506458832734781440.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan   \n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=25, model__optimizer=sgd; total time=   1.8s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 24ms/step - loss: 60579288973312.0000 - mae: 2941685.7500 - val_loss: 17140472522735616.0000 - val_mae: 128052792.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 8613337057415696329211904.0000 - mae: 1054659903488.0000 - val_loss: 2992717412253092614741426176.0000 - val_mae: 53448833761280.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: inf - mae: 479897861580914688.0000 - val_loss: inf - val_mae: 21834946329305939968.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: inf - mae: 195189124341179437023232.0000 - val_loss: inf - val_mae: 9204575957565170381750272.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: inf - mae: 86688717644362220670078156800.0000 - val_loss: inf - val_mae: 3844426721009662191244048072704.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: inf - mae: 34876192572937747134988150569435136.0000 - val_loss: inf - val_mae: 1544302191853063100493318151653556224.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan   \n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=25, model__optimizer=sgd; total time=   1.5s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 27ms/step - loss: 3786615029760.0000 - mae: 686137.1250 - val_loss: 1471835363344384.0000 - val_mae: 37550072.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1412066553501089487912960.0000 - mae: 437905883136.0000 - val_loss: 530204102692559037977853952.0000 - val_mae: 22583781097472.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 516187839395991355007651903264784384.0000 - mae: 257000328912699392.0000 - val_loss: inf - val_mae: 14168914865451696128.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: inf - mae: 166316421029317620793344.0000 - val_loss: inf - val_mae: 7854491040612497290690560.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: inf - mae: 94224717301390429198546370560.0000 - val_loss: inf - val_mae: 4758697423832158953311563677696.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: inf - mae: 53091673114116503812189529296601088.0000 - val_loss: inf - val_mae: inf\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.5s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 29ms/step - loss: 28878040989696.0000 - mae: 1981051.7500 - val_loss: 9362767786016768.0000 - val_mae: 95017176.0000\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 6062907515607182825488384.0000 - mae: 895908577280.0000 - val_loss: 2230136833665427003785871360.0000 - val_mae: 46372388601856.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: inf - mae: 437955719229079552.0000 - val_loss: inf - val_mae: 20929955900669558784.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: inf - mae: 203801664153374697717760.0000 - val_loss: inf - val_mae: 9251255443443692402114560.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: inf - mae: 89148843908346128083443515392.0000 - val_loss: inf - val_mae: 4262767038406567314746897334272.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: inf - mae: 37705749644852262258564529009983488.0000 - val_loss: inf - val_mae: 1912342042831175456146009360852582400.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan    \n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.5s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 23ms/step - loss: 6744708608.0000 - mae: 31043.6777 - val_loss: 1908380794880.0000 - val_mae: 1351169.7500\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 958981516109763575808.0000 - mae: 11128377344.0000 - val_loss: 333199575035743544803328.0000 - val_mae: 563972145152.0000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 176947976994101587707126387572736.0000 - mae: 5063702017474560.0000 - val_loss: 55166559655161943983342453425963008.0000 - val_mae: 230394191924953088.0000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: inf - mae: 2059564226540891799552.0000 - val_loss: inf - val_mae: 97123323519980179292160.0000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: inf - mae: 914707704123567605089828864.0000 - val_loss: inf - val_mae: 40564976972122800557001801728.0000\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: inf - mae: 368000849367689659925785443565568.0000 - val_loss: inf - val_mae: 16294911891610514154976554726195200.0000\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan \n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.4s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 26ms/step - loss: 827.4715 - mae: 20.5337 - val_loss: 80.4939 - val_mae: 6.0509\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 133.1578 - mae: 9.1029 - val_loss: 80.8870 - val_mae: 6.2392\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 126.2879 - mae: 8.8456 - val_loss: 83.3341 - val_mae: 6.5870\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 132.1470 - mae: 8.8045 - val_loss: 81.3870 - val_mae: 6.8449\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 114.3997 - mae: 8.2747 - val_loss: 65.4055 - val_mae: 5.6011\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 110.2729 - mae: 8.1515 - val_loss: 68.4921 - val_mae: 5.9625\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 110.0989 - mae: 7.9201 - val_loss: 62.2023 - val_mae: 5.6887\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 101.2573 - mae: 7.6377 - val_loss: 77.1186 - val_mae: 6.9829\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 100.8244 - mae: 7.5596 - val_loss: 79.8222 - val_mae: 7.1898\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 95.7964 - mae: 7.4201 - val_loss: 55.3680 - val_mae: 5.4882\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 96.6561 - mae: 7.4304 - val_loss: 48.9551 - val_mae: 4.9687\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 92.3096 - mae: 7.2470 - val_loss: 47.1036 - val_mae: 4.7545\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 31ms/step - loss: 90.6731 - mae: 7.1729 - val_loss: 60.3897 - val_mae: 6.1364\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 94.2254 - mae: 7.2874 - val_loss: 48.7186 - val_mae: 5.2701\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 88.0814 - mae: 7.1294 - val_loss: 44.7602 - val_mae: 4.9897\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 83.4713 - mae: 6.8797 - val_loss: 59.2798 - val_mae: 6.2078\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 93.9427 - mae: 7.0501 - val_loss: 51.2059 - val_mae: 5.6100\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 84.4203 - mae: 6.8215 - val_loss: 38.8969 - val_mae: 4.4060\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 81.6812 - mae: 6.7994 - val_loss: 45.7073 - val_mae: 5.0847\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 82.1971 - mae: 6.6865 - val_loss: 55.0949 - val_mae: 5.7729\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78.9608 - mae: 6.6104 - val_loss: 49.5563 - val_mae: 5.5243\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 79.5113 - mae: 6.6123 - val_loss: 33.7747 - val_mae: 4.1561\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 82.7215 - mae: 6.7151 - val_loss: 37.3732 - val_mae: 4.5779\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 77.8845 - mae: 6.3686 - val_loss: 56.1774 - val_mae: 6.0080\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 74.8921 - mae: 6.4460 - val_loss: 37.2850 - val_mae: 4.6182\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 74.9205 - mae: 6.4010 - val_loss: 45.2174 - val_mae: 5.2135\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79.1400 - mae: 6.4986 - val_loss: 33.4017 - val_mae: 4.2294\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 77.0638 - mae: 6.3352 - val_loss: 34.0521 - val_mae: 4.3227\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 77.5528 - mae: 6.3935 - val_loss: 40.4863 - val_mae: 4.8789\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 72.4403 - mae: 6.1477 - val_loss: 31.3099 - val_mae: 4.1232\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 73.0612 - mae: 6.1486 - val_loss: 31.1205 - val_mae: 4.1215\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 74.8099 - mae: 6.1038 - val_loss: 45.8132 - val_mae: 5.3800\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 73.3847 - mae: 6.2290 - val_loss: 34.3151 - val_mae: 4.4436\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 69.9909 - mae: 6.0923 - val_loss: 37.3269 - val_mae: 4.6873\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 72.1111 - mae: 6.2049 - val_loss: 35.3236 - val_mae: 4.5580\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 73.0297 - mae: 6.1298 - val_loss: 31.4677 - val_mae: 4.2238\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 71.9197 - mae: 6.1937 - val_loss: 28.4778 - val_mae: 3.9497\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 75.3545 - mae: 6.2630 - val_loss: 27.1689 - val_mae: 3.9118\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 69.3460 - mae: 6.0292 - val_loss: 28.5318 - val_mae: 4.0092\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 69.7331 - mae: 6.1570 - val_loss: 44.1319 - val_mae: 5.2273\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 73.6948 - mae: 6.2683 - val_loss: 26.7090 - val_mae: 3.8332\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 74.9952 - mae: 6.3332 - val_loss: 27.8061 - val_mae: 3.9279\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 74.4635 - mae: 6.2384 - val_loss: 29.0870 - val_mae: 4.0901\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 70.3288 - mae: 6.0691 - val_loss: 26.6283 - val_mae: 3.8716\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 68.9137 - mae: 6.1145 - val_loss: 27.3923 - val_mae: 3.9379\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 69.5217 - mae: 6.0659 - val_loss: 26.5163 - val_mae: 3.8542\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 77.1462 - mae: 6.3296 - val_loss: 51.3359 - val_mae: 5.8483\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 72.5563 - mae: 5.9813 - val_loss: 37.9417 - val_mae: 4.8555\n",
      "Epoch 48: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=momentum; total time=   4.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 30ms/step - loss: 763.1794 - mae: 19.7342 - val_loss: 84.0131 - val_mae: 6.2345\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 102.6567 - mae: 7.9879 - val_loss: 38.8452 - val_mae: 5.3021\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 91.7825 - mae: 7.5670 - val_loss: 33.6539 - val_mae: 4.4789\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 79.6488 - mae: 6.9753 - val_loss: 22.7328 - val_mae: 3.9571\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 76.7938 - mae: 6.6668 - val_loss: 23.2110 - val_mae: 3.7713\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 76.6040 - mae: 6.6117 - val_loss: 15.8175 - val_mae: 3.3987\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 74.1481 - mae: 6.4509 - val_loss: 13.7091 - val_mae: 3.1510\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 77.7316 - mae: 6.3909 - val_loss: 23.3975 - val_mae: 3.6012\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 68.8929 - mae: 6.2473 - val_loss: 21.1559 - val_mae: 3.4998\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 67.5821 - mae: 6.2071 - val_loss: 27.4672 - val_mae: 4.0565\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 67.9169 - mae: 6.2607 - val_loss: 13.3400 - val_mae: 2.9782\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 67.5957 - mae: 6.1055 - val_loss: 14.9067 - val_mae: 3.0693\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 65.3113 - mae: 5.9900 - val_loss: 17.1735 - val_mae: 3.2274\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 65.8211 - mae: 6.0529 - val_loss: 13.3759 - val_mae: 2.9383\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 65.5919 - mae: 6.0489 - val_loss: 11.4933 - val_mae: 2.7885\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 64.6237 - mae: 5.9637 - val_loss: 11.4587 - val_mae: 2.8108\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 66.1891 - mae: 5.9203 - val_loss: 18.8092 - val_mae: 3.3409\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 65.8903 - mae: 6.0397 - val_loss: 13.3829 - val_mae: 2.9333\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 63.5084 - mae: 5.8655 - val_loss: 11.4049 - val_mae: 2.7543\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 66.1366 - mae: 6.0133 - val_loss: 13.7637 - val_mae: 2.9365\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 61.5134 - mae: 5.8049 - val_loss: 14.5194 - val_mae: 3.0121\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 64.2965 - mae: 5.9544 - val_loss: 12.0003 - val_mae: 2.8247\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 62.3284 - mae: 5.7478 - val_loss: 11.8932 - val_mae: 2.8192\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 66.9568 - mae: 5.9242 - val_loss: 28.2976 - val_mae: 4.2665\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 63.3718 - mae: 6.0279 - val_loss: 13.4279 - val_mae: 2.9301\n",
      "Epoch 25: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=momentum; total time=   2.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 26ms/step - loss: 705.8416 - mae: 19.6749 - val_loss: 64.1640 - val_mae: 6.4532\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 78.8339 - mae: 6.8112 - val_loss: 53.1816 - val_mae: 5.9754\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71.7373 - mae: 6.4005 - val_loss: 46.9457 - val_mae: 5.5565\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 62.5783 - mae: 5.7245 - val_loss: 46.4442 - val_mae: 5.4782\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 58.1651 - mae: 5.3954 - val_loss: 43.2158 - val_mae: 5.2202\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 53.7991 - mae: 5.1543 - val_loss: 34.8710 - val_mae: 4.5567\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 53.5364 - mae: 5.0886 - val_loss: 33.2831 - val_mae: 4.3453\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 52.0545 - mae: 5.1369 - val_loss: 40.4067 - val_mae: 5.0095\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 48.6937 - mae: 4.8458 - val_loss: 31.9172 - val_mae: 4.3178\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 48.7629 - mae: 5.0491 - val_loss: 31.5260 - val_mae: 4.2496\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 46.3304 - mae: 4.7899 - val_loss: 37.5415 - val_mae: 4.7768\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 47.8459 - mae: 4.8920 - val_loss: 36.9978 - val_mae: 4.7228\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 47.4808 - mae: 4.7345 - val_loss: 29.9489 - val_mae: 4.1513\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 46.6868 - mae: 4.6695 - val_loss: 30.9347 - val_mae: 4.2089\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 44.3385 - mae: 4.5944 - val_loss: 33.1239 - val_mae: 4.3841\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 46.3100 - mae: 4.7030 - val_loss: 31.5372 - val_mae: 4.2217\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 45.3605 - mae: 4.7011 - val_loss: 31.2900 - val_mae: 4.2188\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 47.5237 - mae: 4.7138 - val_loss: 28.8207 - val_mae: 4.1087\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 44.5077 - mae: 4.6730 - val_loss: 34.6875 - val_mae: 4.5754\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 47.9937 - mae: 4.7521 - val_loss: 28.5573 - val_mae: 4.0807\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 49.3384 - mae: 4.9308 - val_loss: 28.5599 - val_mae: 4.1273\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 44.0216 - mae: 4.4201 - val_loss: 34.8583 - val_mae: 4.4281\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 47.2823 - mae: 5.1823 - val_loss: 34.2405 - val_mae: 4.5771\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 44.4261 - mae: 4.5034 - val_loss: 30.0266 - val_mae: 4.1562\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 43.3387 - mae: 4.5474 - val_loss: 28.7821 - val_mae: 4.1480\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 42.6616 - mae: 4.4285 - val_loss: 29.8772 - val_mae: 4.1353\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 43.3184 - mae: 4.6828 - val_loss: 34.5155 - val_mae: 4.6362\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 44.8974 - mae: 4.5539 - val_loss: 28.2983 - val_mae: 4.0116\n",
      "Epoch 28: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=1, model__n_neurons=125, model__optimizer=momentum; total time=   3.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 26322.6777 - mae: 127.9647 - val_loss: 31808.9219 - val_mae: 141.1243\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26302.3535 - mae: 127.8941 - val_loss: 31786.2109 - val_mae: 141.0518\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26281.5703 - mae: 127.8235 - val_loss: 31763.7090 - val_mae: 140.9798\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 26261.1230 - mae: 127.7535 - val_loss: 31741.1426 - val_mae: 140.9076\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 26241.0352 - mae: 127.6838 - val_loss: 31718.6348 - val_mae: 140.8355\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 26220.2363 - mae: 127.6137 - val_loss: 31696.3105 - val_mae: 140.7640\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 26200.0371 - mae: 127.5447 - val_loss: 31673.7754 - val_mae: 140.6918\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26179.5352 - mae: 127.4732 - val_loss: 31651.1953 - val_mae: 140.6196\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26158.8418 - mae: 127.4035 - val_loss: 31628.6348 - val_mae: 140.5472\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26138.2969 - mae: 127.3328 - val_loss: 31606.3008 - val_mae: 140.4755\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 26118.4297 - mae: 127.2631 - val_loss: 31583.5410 - val_mae: 140.4026\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 26097.8320 - mae: 127.1929 - val_loss: 31561.4043 - val_mae: 140.3315\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 26077.3516 - mae: 127.1240 - val_loss: 31539.3027 - val_mae: 140.2605\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26057.9082 - mae: 127.0541 - val_loss: 31516.5742 - val_mae: 140.1875\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26036.8281 - mae: 126.9834 - val_loss: 31494.5625 - val_mae: 140.1166\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 26016.5215 - mae: 126.9136 - val_loss: 31472.3867 - val_mae: 140.0451\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25996.9297 - mae: 126.8444 - val_loss: 31449.5977 - val_mae: 139.9719\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25976.4688 - mae: 126.7738 - val_loss: 31427.0625 - val_mae: 139.8993\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25955.7773 - mae: 126.7032 - val_loss: 31405.1250 - val_mae: 139.8286\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 25935.7578 - mae: 126.6342 - val_loss: 31382.9648 - val_mae: 139.7572\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25915.6328 - mae: 126.5645 - val_loss: 31360.7129 - val_mae: 139.6855\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 25895.4160 - mae: 126.4951 - val_loss: 31338.5234 - val_mae: 139.6138\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 25875.6426 - mae: 126.4252 - val_loss: 31316.0918 - val_mae: 139.5413\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25855.2812 - mae: 126.3545 - val_loss: 31293.9121 - val_mae: 139.4698\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 25835.2227 - mae: 126.2859 - val_loss: 31271.8164 - val_mae: 139.3983\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25814.9629 - mae: 126.2164 - val_loss: 31249.8086 - val_mae: 139.3272\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25795.1582 - mae: 126.1460 - val_loss: 31227.5176 - val_mae: 139.2551\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25775.1094 - mae: 126.0772 - val_loss: 31205.5449 - val_mae: 139.1840\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 25755.3691 - mae: 126.0076 - val_loss: 31183.4395 - val_mae: 139.1123\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25734.8828 - mae: 125.9379 - val_loss: 31161.5391 - val_mae: 139.0414\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 25715.2539 - mae: 125.8688 - val_loss: 31139.3770 - val_mae: 138.9697\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25695.0840 - mae: 125.7991 - val_loss: 31117.3574 - val_mae: 138.8982\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25675.5020 - mae: 125.7299 - val_loss: 31095.1543 - val_mae: 138.8261\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25655.2227 - mae: 125.6599 - val_loss: 31073.5078 - val_mae: 138.7558\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25635.5996 - mae: 125.5916 - val_loss: 31051.8242 - val_mae: 138.6855\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25615.1523 - mae: 125.5233 - val_loss: 31030.4824 - val_mae: 138.6160\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 25596.0801 - mae: 125.4539 - val_loss: 31007.9785 - val_mae: 138.5428\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25575.9141 - mae: 125.3838 - val_loss: 30985.7129 - val_mae: 138.4706\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 25555.9238 - mae: 125.3137 - val_loss: 30963.6035 - val_mae: 138.3987\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 25535.9629 - mae: 125.2434 - val_loss: 30941.6641 - val_mae: 138.3273\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25515.9453 - mae: 125.1747 - val_loss: 30919.8242 - val_mae: 138.2562\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25496.0449 - mae: 125.1055 - val_loss: 30898.1270 - val_mae: 138.1855\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 25476.0703 - mae: 125.0363 - val_loss: 30876.4141 - val_mae: 138.1148\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25456.9570 - mae: 124.9660 - val_loss: 30853.9590 - val_mae: 138.0417\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 25436.3066 - mae: 124.8966 - val_loss: 30832.3262 - val_mae: 137.9712\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25416.8438 - mae: 124.8279 - val_loss: 30810.5391 - val_mae: 137.9000\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 25397.0254 - mae: 124.7593 - val_loss: 30788.8418 - val_mae: 137.8291\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 25377.2930 - mae: 124.6898 - val_loss: 30767.0762 - val_mae: 137.7581\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25357.5703 - mae: 124.6205 - val_loss: 30745.4082 - val_mae: 137.6873\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 25337.9648 - mae: 124.5511 - val_loss: 30723.4102 - val_mae: 137.6154\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25318.0918 - mae: 124.4814 - val_loss: 30701.6699 - val_mae: 137.5443\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 25298.2520 - mae: 124.4124 - val_loss: 30679.8340 - val_mae: 137.4729\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 25278.4648 - mae: 124.3428 - val_loss: 30658.1035 - val_mae: 137.4017\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25258.8262 - mae: 124.2737 - val_loss: 30636.2520 - val_mae: 137.3301\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25239.1934 - mae: 124.2035 - val_loss: 30614.2148 - val_mae: 137.2579\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 25218.9844 - mae: 124.1337 - val_loss: 30592.5742 - val_mae: 137.1870\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 25199.4004 - mae: 124.0652 - val_loss: 30570.8418 - val_mae: 137.1157\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25179.9668 - mae: 123.9950 - val_loss: 30548.6543 - val_mae: 137.0431\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25159.5352 - mae: 123.9258 - val_loss: 30527.2500 - val_mae: 136.9728\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25140.4961 - mae: 123.8564 - val_loss: 30505.3672 - val_mae: 136.9009\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25120.4434 - mae: 123.7865 - val_loss: 30483.8281 - val_mae: 136.8301\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 25100.8711 - mae: 123.7180 - val_loss: 30462.3262 - val_mae: 136.7594\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 25081.3887 - mae: 123.6491 - val_loss: 30440.7363 - val_mae: 136.6883\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 25061.7969 - mae: 123.5796 - val_loss: 30418.9844 - val_mae: 136.6168\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 25041.9883 - mae: 123.5104 - val_loss: 30397.3789 - val_mae: 136.5456\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 25022.6934 - mae: 123.4407 - val_loss: 30375.2832 - val_mae: 136.4729\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 25002.7188 - mae: 123.3698 - val_loss: 30353.4258 - val_mae: 136.4009\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 24983.2148 - mae: 123.3001 - val_loss: 30331.4961 - val_mae: 136.3287\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 24963.3066 - mae: 123.2311 - val_loss: 30310.1230 - val_mae: 136.2579\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 24943.8809 - mae: 123.1615 - val_loss: 30288.6582 - val_mae: 136.1871\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 24924.3164 - mae: 123.0922 - val_loss: 30267.3652 - val_mae: 136.1168\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 24905.2207 - mae: 123.0239 - val_loss: 30245.5215 - val_mae: 136.0448\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 24885.3535 - mae: 122.9546 - val_loss: 30224.3848 - val_mae: 135.9748\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 24866.2715 - mae: 122.8860 - val_loss: 30202.8145 - val_mae: 135.9035\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 24846.9180 - mae: 122.8164 - val_loss: 30181.1328 - val_mae: 135.8317\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 24827.2695 - mae: 122.7474 - val_loss: 30159.7207 - val_mae: 135.7609\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 24807.8340 - mae: 122.6775 - val_loss: 30138.1504 - val_mae: 135.6895\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 24788.1934 - mae: 122.6087 - val_loss: 30116.8535 - val_mae: 135.6188\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 24769.0254 - mae: 122.5397 - val_loss: 30095.3281 - val_mae: 135.5475\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 24749.3555 - mae: 122.4709 - val_loss: 30074.0547 - val_mae: 135.4769\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 24730.2930 - mae: 122.4015 - val_loss: 30052.4785 - val_mae: 135.4054\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 24710.5742 - mae: 122.3311 - val_loss: 30030.7793 - val_mae: 135.3335\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 24691.2285 - mae: 122.2624 - val_loss: 30009.1621 - val_mae: 135.2617\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 24671.7383 - mae: 122.1933 - val_loss: 29987.8848 - val_mae: 135.1911\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 24652.5059 - mae: 122.1242 - val_loss: 29966.7148 - val_mae: 135.1208\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 24633.2188 - mae: 122.0560 - val_loss: 29945.6289 - val_mae: 135.0506\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 24613.9707 - mae: 121.9874 - val_loss: 29924.6250 - val_mae: 134.9807\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 24595.4883 - mae: 121.9194 - val_loss: 29902.9766 - val_mae: 134.9087\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 24576.0508 - mae: 121.8500 - val_loss: 29881.4629 - val_mae: 134.8372\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 24556.4961 - mae: 121.7816 - val_loss: 29860.4395 - val_mae: 134.7671\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 24537.6934 - mae: 121.7134 - val_loss: 29839.1367 - val_mae: 134.6961\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 24518.1406 - mae: 121.6447 - val_loss: 29818.0918 - val_mae: 134.6260\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 24498.9453 - mae: 121.5770 - val_loss: 29796.8848 - val_mae: 134.5553\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 24479.9316 - mae: 121.5091 - val_loss: 29775.6426 - val_mae: 134.4843\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 24460.5078 - mae: 121.4412 - val_loss: 29754.5723 - val_mae: 134.4138\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 24441.4883 - mae: 121.3734 - val_loss: 29733.3613 - val_mae: 134.3430\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 24422.6328 - mae: 121.3050 - val_loss: 29711.8633 - val_mae: 134.2712\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 24402.8145 - mae: 121.2376 - val_loss: 29690.9844 - val_mae: 134.2011\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 24384.0410 - mae: 121.1696 - val_loss: 29669.5703 - val_mae: 134.1296\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 24364.9531 - mae: 121.1011 - val_loss: 29648.4492 - val_mae: 134.0589\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=  10.4s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 27ms/step - loss: 8788.8359 - mae: 89.7263 - val_loss: 9053.3301 - val_mae: 89.8900\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8776.2070 - mae: 89.6617 - val_loss: 9040.5234 - val_mae: 89.8224\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 8763.7305 - mae: 89.5981 - val_loss: 9027.6807 - val_mae: 89.7546\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 8751.3223 - mae: 89.5348 - val_loss: 9014.8662 - val_mae: 89.6869\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8738.7646 - mae: 89.4709 - val_loss: 9002.1484 - val_mae: 89.6196\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8726.2324 - mae: 89.4079 - val_loss: 8989.4893 - val_mae: 89.5526\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8713.7812 - mae: 89.3439 - val_loss: 8976.8066 - val_mae: 89.4854\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8701.3525 - mae: 89.2811 - val_loss: 8964.1826 - val_mae: 89.4184\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8689.0322 - mae: 89.2186 - val_loss: 8951.5146 - val_mae: 89.3512\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8676.7227 - mae: 89.1560 - val_loss: 8938.8330 - val_mae: 89.2838\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8664.2725 - mae: 89.0933 - val_loss: 8926.2666 - val_mae: 89.2170\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 8651.9746 - mae: 89.0309 - val_loss: 8913.6631 - val_mae: 89.1500\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8639.6035 - mae: 88.9687 - val_loss: 8901.0781 - val_mae: 89.0829\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8627.2334 - mae: 88.9063 - val_loss: 8888.5264 - val_mae: 89.0160\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 8614.9678 - mae: 88.8449 - val_loss: 8875.9707 - val_mae: 88.9490\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8602.5625 - mae: 88.7813 - val_loss: 8863.4248 - val_mae: 88.8821\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 8590.3330 - mae: 88.7195 - val_loss: 8850.8447 - val_mae: 88.8148\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8578.1719 - mae: 88.6584 - val_loss: 8838.2236 - val_mae: 88.7473\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8565.8096 - mae: 88.5957 - val_loss: 8825.7705 - val_mae: 88.6807\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 8553.4697 - mae: 88.5333 - val_loss: 8813.3672 - val_mae: 88.6142\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 8541.2412 - mae: 88.4706 - val_loss: 8800.8896 - val_mae: 88.5473\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8529.1387 - mae: 88.4098 - val_loss: 8788.3076 - val_mae: 88.4797\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8516.8574 - mae: 88.3477 - val_loss: 8775.7744 - val_mae: 88.4124\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8504.5166 - mae: 88.2858 - val_loss: 8763.3760 - val_mae: 88.3457\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8492.4482 - mae: 88.2251 - val_loss: 8750.8994 - val_mae: 88.2786\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8480.2461 - mae: 88.1633 - val_loss: 8738.4990 - val_mae: 88.2118\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 8468.1143 - mae: 88.1024 - val_loss: 8726.1162 - val_mae: 88.1451\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 8455.9102 - mae: 88.0404 - val_loss: 8713.7979 - val_mae: 88.0786\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 8443.9805 - mae: 87.9807 - val_loss: 8701.3516 - val_mae: 88.0114\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 8431.8730 - mae: 87.9192 - val_loss: 8688.9814 - val_mae: 87.9445\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8419.6582 - mae: 87.8573 - val_loss: 8676.7021 - val_mae: 87.8781\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 8407.6953 - mae: 87.7963 - val_loss: 8664.3252 - val_mae: 87.8111\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8395.6338 - mae: 87.7355 - val_loss: 8652.0420 - val_mae: 87.7446\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8383.6113 - mae: 87.6738 - val_loss: 8639.8496 - val_mae: 87.6785\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8371.5557 - mae: 87.6120 - val_loss: 8627.7451 - val_mae: 87.6128\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8359.6777 - mae: 87.5524 - val_loss: 8615.5420 - val_mae: 87.5465\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8347.7705 - mae: 87.4916 - val_loss: 8603.2314 - val_mae: 87.4796\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8335.7266 - mae: 87.4304 - val_loss: 8591.0088 - val_mae: 87.4130\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 8323.8379 - mae: 87.3702 - val_loss: 8578.7539 - val_mae: 87.3463\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 8311.8701 - mae: 87.3082 - val_loss: 8566.5938 - val_mae: 87.2800\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 8299.7568 - mae: 87.2463 - val_loss: 8554.6230 - val_mae: 87.2147\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 8288.0156 - mae: 87.1866 - val_loss: 8542.4766 - val_mae: 87.1484\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8276.1035 - mae: 87.1244 - val_loss: 8530.2900 - val_mae: 87.0818\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8264.1592 - mae: 87.0640 - val_loss: 8518.1357 - val_mae: 87.0153\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8252.1748 - mae: 87.0028 - val_loss: 8506.0820 - val_mae: 86.9493\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8240.5566 - mae: 86.9433 - val_loss: 8493.8271 - val_mae: 86.8821\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8228.6436 - mae: 86.8821 - val_loss: 8481.6211 - val_mae: 86.8151\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8216.6396 - mae: 86.8196 - val_loss: 8469.6113 - val_mae: 86.7492\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 8204.9619 - mae: 86.7598 - val_loss: 8457.5674 - val_mae: 86.6830\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 8193.0977 - mae: 86.6983 - val_loss: 8445.6777 - val_mae: 86.6177\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8181.3711 - mae: 86.6383 - val_loss: 8433.7373 - val_mae: 86.5519\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 8169.7622 - mae: 86.5783 - val_loss: 8421.7041 - val_mae: 86.4856\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8157.9087 - mae: 86.5171 - val_loss: 8409.8408 - val_mae: 86.4202\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 8146.2222 - mae: 86.4564 - val_loss: 8397.9170 - val_mae: 86.3544\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8134.6450 - mae: 86.3968 - val_loss: 8385.9258 - val_mae: 86.2882\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8122.9424 - mae: 86.3364 - val_loss: 8373.9912 - val_mae: 86.2222\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 8111.2695 - mae: 86.2754 - val_loss: 8362.0615 - val_mae: 86.1562\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 8099.6001 - mae: 86.2153 - val_loss: 8350.1582 - val_mae: 86.0903\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8087.8994 - mae: 86.1540 - val_loss: 8338.2852 - val_mae: 86.0245\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 8076.3296 - mae: 86.0940 - val_loss: 8326.3291 - val_mae: 85.9581\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 8064.5630 - mae: 86.0336 - val_loss: 8314.4834 - val_mae: 85.8924\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 8052.9131 - mae: 85.9723 - val_loss: 8302.6826 - val_mae: 85.8268\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8041.2915 - mae: 85.9117 - val_loss: 8290.8945 - val_mae: 85.7612\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 8029.7856 - mae: 85.8521 - val_loss: 8279.0166 - val_mae: 85.6951\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 8018.2749 - mae: 85.7917 - val_loss: 8267.0947 - val_mae: 85.6287\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 8006.6045 - mae: 85.7304 - val_loss: 8255.3506 - val_mae: 85.5632\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 7995.1074 - mae: 85.6705 - val_loss: 8243.5957 - val_mae: 85.4976\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 7983.5874 - mae: 85.6101 - val_loss: 8231.8164 - val_mae: 85.4317\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 7972.0747 - mae: 85.5500 - val_loss: 8220.0518 - val_mae: 85.3659\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7960.5767 - mae: 85.4893 - val_loss: 8208.3086 - val_mae: 85.3002\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 7949.0029 - mae: 85.4292 - val_loss: 8196.6289 - val_mae: 85.2348\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7937.5063 - mae: 85.3682 - val_loss: 8184.9536 - val_mae: 85.1693\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 7926.1343 - mae: 85.3092 - val_loss: 8173.2197 - val_mae: 85.1035\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7914.6094 - mae: 85.2490 - val_loss: 8161.5376 - val_mae: 85.0379\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 7903.2402 - mae: 85.1907 - val_loss: 8149.8179 - val_mae: 84.9720\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7891.7559 - mae: 85.1306 - val_loss: 8138.1792 - val_mae: 84.9065\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7880.3042 - mae: 85.0712 - val_loss: 8126.6035 - val_mae: 84.8413\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7868.9204 - mae: 85.0131 - val_loss: 8115.0410 - val_mae: 84.7762\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7857.6396 - mae: 84.9539 - val_loss: 8103.3755 - val_mae: 84.7103\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 7846.3096 - mae: 84.8951 - val_loss: 8091.6792 - val_mae: 84.6443\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 7834.7471 - mae: 84.8354 - val_loss: 8080.1772 - val_mae: 84.5793\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 7823.5283 - mae: 84.7763 - val_loss: 8068.6118 - val_mae: 84.5139\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7812.2466 - mae: 84.7177 - val_loss: 8057.0991 - val_mae: 84.4487\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7801.0054 - mae: 84.6590 - val_loss: 8045.6074 - val_mae: 84.3836\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7789.8447 - mae: 84.6006 - val_loss: 8034.0664 - val_mae: 84.3181\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 7778.5483 - mae: 84.5421 - val_loss: 8022.6465 - val_mae: 84.2533\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 7767.1885 - mae: 84.4823 - val_loss: 8011.2837 - val_mae: 84.1887\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7756.0190 - mae: 84.4246 - val_loss: 7999.8354 - val_mae: 84.1236\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7744.6992 - mae: 84.3654 - val_loss: 7988.4897 - val_mae: 84.0591\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7733.6143 - mae: 84.3089 - val_loss: 7976.9849 - val_mae: 83.9935\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7722.4346 - mae: 84.2508 - val_loss: 7965.4375 - val_mae: 83.9277\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7711.1489 - mae: 84.1917 - val_loss: 7954.0415 - val_mae: 83.8627\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7699.8760 - mae: 84.1317 - val_loss: 7942.6699 - val_mae: 83.7977\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 7689.0063 - mae: 84.0762 - val_loss: 7931.1030 - val_mae: 83.7316\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7677.6851 - mae: 84.0180 - val_loss: 7919.7866 - val_mae: 83.6668\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 7666.4834 - mae: 83.9585 - val_loss: 7908.5625 - val_mae: 83.6025\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 7655.4214 - mae: 83.9008 - val_loss: 7897.2441 - val_mae: 83.5377\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7644.4595 - mae: 83.8442 - val_loss: 7885.8291 - val_mae: 83.4722\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 7633.2744 - mae: 83.7851 - val_loss: 7874.5532 - val_mae: 83.4074\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 7622.2393 - mae: 83.7274 - val_loss: 7863.2217 - val_mae: 83.3422\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=  10.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 57605.9062 - mae: 225.6517 - val_loss: 59301.7656 - val_mae: 231.0281\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 57571.2578 - mae: 225.5799 - val_loss: 59265.4805 - val_mae: 230.9542\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 57537.7266 - mae: 225.5088 - val_loss: 59228.8672 - val_mae: 230.8796\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 57503.5391 - mae: 225.4369 - val_loss: 59192.4453 - val_mae: 230.8053\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 57469.7500 - mae: 225.3652 - val_loss: 59155.9023 - val_mae: 230.7308\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 57435.2070 - mae: 225.2934 - val_loss: 59119.7266 - val_mae: 230.6569\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 57401.1406 - mae: 225.2223 - val_loss: 59083.5742 - val_mae: 230.5831\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 57367.5117 - mae: 225.1507 - val_loss: 59047.1680 - val_mae: 230.5088\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 57333.1523 - mae: 225.0793 - val_loss: 59011.0586 - val_mae: 230.4350\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 57299.4648 - mae: 225.0079 - val_loss: 58974.6055 - val_mae: 230.3606\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 57265.2461 - mae: 224.9364 - val_loss: 58938.4531 - val_mae: 230.2867\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 57231.4180 - mae: 224.8649 - val_loss: 58902.2578 - val_mae: 230.2127\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 57197.9922 - mae: 224.7936 - val_loss: 58865.6484 - val_mae: 230.1378\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 57163.6367 - mae: 224.7216 - val_loss: 58829.3672 - val_mae: 230.0636\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 57129.5977 - mae: 224.6502 - val_loss: 58793.2422 - val_mae: 229.9897\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 57095.5898 - mae: 224.5788 - val_loss: 58757.3789 - val_mae: 229.9163\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 57061.7852 - mae: 224.5075 - val_loss: 58721.3711 - val_mae: 229.8426\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 57028.2734 - mae: 224.4365 - val_loss: 58685.0000 - val_mae: 229.7681\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 56994.3438 - mae: 224.3648 - val_loss: 58648.5977 - val_mae: 229.6935\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 56960.3555 - mae: 224.2927 - val_loss: 58612.2227 - val_mae: 229.6190\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 56926.2617 - mae: 224.2213 - val_loss: 58576.1719 - val_mae: 229.5451\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 56892.6484 - mae: 224.1499 - val_loss: 58540.0430 - val_mae: 229.4710\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 56858.6484 - mae: 224.0785 - val_loss: 58504.1484 - val_mae: 229.3974\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 56825.1992 - mae: 224.0076 - val_loss: 58468.1602 - val_mae: 229.3235\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 56791.3125 - mae: 223.9363 - val_loss: 58432.4336 - val_mae: 229.2502\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 56757.5898 - mae: 223.8655 - val_loss: 58396.8516 - val_mae: 229.1771\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 56724.3711 - mae: 223.7946 - val_loss: 58360.8008 - val_mae: 229.1031\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 56691.0469 - mae: 223.7235 - val_loss: 58324.5312 - val_mae: 229.0285\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 56656.8555 - mae: 223.6519 - val_loss: 58289.0273 - val_mae: 228.9556\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 56623.3398 - mae: 223.5812 - val_loss: 58253.4180 - val_mae: 228.8823\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 56590.1836 - mae: 223.5106 - val_loss: 58217.3672 - val_mae: 228.8082\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 56556.7383 - mae: 223.4392 - val_loss: 58181.3516 - val_mae: 228.7341\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 56522.8750 - mae: 223.3678 - val_loss: 58145.5820 - val_mae: 228.6605\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 56489.3945 - mae: 223.2966 - val_loss: 58109.7734 - val_mae: 228.5868\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 56455.9180 - mae: 223.2258 - val_loss: 58074.0703 - val_mae: 228.5133\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 56422.1758 - mae: 223.1548 - val_loss: 58038.5898 - val_mae: 228.4401\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 56388.4961 - mae: 223.0838 - val_loss: 58002.9844 - val_mae: 228.3668\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 56355.2148 - mae: 223.0129 - val_loss: 57966.9648 - val_mae: 228.2925\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 56321.9922 - mae: 222.9414 - val_loss: 57930.5898 - val_mae: 228.2176\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 56288.0977 - mae: 222.8694 - val_loss: 57894.6406 - val_mae: 228.1434\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 56254.1328 - mae: 222.7980 - val_loss: 57859.1797 - val_mae: 228.0703\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 56220.6328 - mae: 222.7273 - val_loss: 57823.7031 - val_mae: 227.9971\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 56187.6758 - mae: 222.6564 - val_loss: 57787.7695 - val_mae: 227.9229\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 56153.9297 - mae: 222.5850 - val_loss: 57752.0703 - val_mae: 227.8492\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 56120.6836 - mae: 222.5139 - val_loss: 57716.2695 - val_mae: 227.7753\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 56086.7500 - mae: 222.4426 - val_loss: 57680.8906 - val_mae: 227.7021\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 56053.7969 - mae: 222.3717 - val_loss: 57644.9961 - val_mae: 227.6279\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 56020.3281 - mae: 222.3004 - val_loss: 57609.1680 - val_mae: 227.5538\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 55987.2500 - mae: 222.2291 - val_loss: 57573.1758 - val_mae: 227.4794\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 55952.9492 - mae: 222.1576 - val_loss: 57537.9062 - val_mae: 227.4064\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 55919.9297 - mae: 222.0868 - val_loss: 57502.2969 - val_mae: 227.3327\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 55886.3281 - mae: 222.0156 - val_loss: 57466.6797 - val_mae: 227.2590\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 55853.4492 - mae: 221.9447 - val_loss: 57430.7070 - val_mae: 227.1845\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 55819.8594 - mae: 221.8730 - val_loss: 57395.1328 - val_mae: 227.1109\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 55786.3711 - mae: 221.8020 - val_loss: 57359.9102 - val_mae: 227.0379\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 55753.5664 - mae: 221.7314 - val_loss: 57324.3164 - val_mae: 226.9642\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 55720.2070 - mae: 221.6603 - val_loss: 57288.7773 - val_mae: 226.8905\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 55686.9727 - mae: 221.5892 - val_loss: 57253.3203 - val_mae: 226.8169\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 55653.6367 - mae: 221.5185 - val_loss: 57218.1914 - val_mae: 226.7440\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 55620.7344 - mae: 221.4481 - val_loss: 57182.8203 - val_mae: 226.6705\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 55588.1055 - mae: 221.3772 - val_loss: 57147.1250 - val_mae: 226.5965\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 55554.3438 - mae: 221.3060 - val_loss: 57112.1953 - val_mae: 226.5239\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 55521.3320 - mae: 221.2358 - val_loss: 57077.1719 - val_mae: 226.4511\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 55488.9453 - mae: 221.1654 - val_loss: 57041.3594 - val_mae: 226.3767\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 55454.8711 - mae: 221.0937 - val_loss: 57006.3242 - val_mae: 226.3039\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 55422.1836 - mae: 221.0233 - val_loss: 56970.9219 - val_mae: 226.2303\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 55389.0938 - mae: 220.9524 - val_loss: 56935.6211 - val_mae: 226.1568\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 55356.0508 - mae: 220.8815 - val_loss: 56900.2422 - val_mae: 226.0832\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 55322.8047 - mae: 220.8105 - val_loss: 56864.8594 - val_mae: 226.0096\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 55289.9102 - mae: 220.7396 - val_loss: 56829.6094 - val_mae: 225.9362\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 55257.0703 - mae: 220.6689 - val_loss: 56794.2656 - val_mae: 225.8626\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 55223.7773 - mae: 220.5980 - val_loss: 56759.3320 - val_mae: 225.7898\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 55191.1562 - mae: 220.5279 - val_loss: 56724.1797 - val_mae: 225.7165\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 55157.8594 - mae: 220.4569 - val_loss: 56689.0352 - val_mae: 225.6433\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 55125.1562 - mae: 220.3860 - val_loss: 56653.5273 - val_mae: 225.5693\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 55091.8438 - mae: 220.3150 - val_loss: 56618.3750 - val_mae: 225.4959\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 55058.7461 - mae: 220.2441 - val_loss: 56583.2930 - val_mae: 225.4227\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 55025.8906 - mae: 220.1734 - val_loss: 56548.1406 - val_mae: 225.3493\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 54992.9883 - mae: 220.1027 - val_loss: 56513.0703 - val_mae: 225.2761\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 54960.3164 - mae: 220.0319 - val_loss: 56477.6250 - val_mae: 225.2021\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 54927.5195 - mae: 219.9606 - val_loss: 56442.0469 - val_mae: 225.1279\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 54894.0000 - mae: 219.8894 - val_loss: 56407.1445 - val_mae: 225.0549\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 54861.1992 - mae: 219.8190 - val_loss: 56372.3281 - val_mae: 224.9821\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 54828.5703 - mae: 219.7486 - val_loss: 56337.5586 - val_mae: 224.9093\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 54795.4922 - mae: 219.6783 - val_loss: 56302.9688 - val_mae: 224.8369\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 54763.1016 - mae: 219.6082 - val_loss: 56267.9453 - val_mae: 224.7636\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 54730.6133 - mae: 219.5375 - val_loss: 56232.5664 - val_mae: 224.6896\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 54697.3867 - mae: 219.4662 - val_loss: 56197.5469 - val_mae: 224.6162\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 54664.5469 - mae: 219.3954 - val_loss: 56162.5742 - val_mae: 224.5430\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 54631.6211 - mae: 219.3247 - val_loss: 56127.5547 - val_mae: 224.4696\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 54598.9805 - mae: 219.2539 - val_loss: 56092.2305 - val_mae: 224.3957\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 54566.1875 - mae: 219.1827 - val_loss: 56057.1836 - val_mae: 224.3222\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 54533.4922 - mae: 219.1120 - val_loss: 56022.2578 - val_mae: 224.2489\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 54500.2305 - mae: 219.0413 - val_loss: 55987.8164 - val_mae: 224.1767\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 54468.2188 - mae: 218.9711 - val_loss: 55952.4648 - val_mae: 224.1025\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 54434.9062 - mae: 218.8999 - val_loss: 55917.6055 - val_mae: 224.0293\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 54402.7734 - mae: 218.8293 - val_loss: 55882.3672 - val_mae: 223.9554\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 54369.3047 - mae: 218.7581 - val_loss: 55847.8438 - val_mae: 223.8828\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 54337.2148 - mae: 218.6882 - val_loss: 55812.9570 - val_mae: 223.8095\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 54304.2422 - mae: 218.6175 - val_loss: 55778.5703 - val_mae: 223.7371\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.5, model__n_hidden=0, model__n_neurons=25, model__optimizer=adam; total time=   9.5s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 28ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   2.4s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 36ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 4ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   2.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 47ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.1, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   2.2s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 30ms/step - loss: 308.4606 - mae: 14.0903 - val_loss: 319.5730 - val_mae: 15.0321\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 308.3799 - mae: 14.0887 - val_loss: 319.4714 - val_mae: 15.0298\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 308.2966 - mae: 14.0870 - val_loss: 319.3701 - val_mae: 15.0274\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 308.2189 - mae: 14.0855 - val_loss: 319.2661 - val_mae: 15.0250\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 308.1413 - mae: 14.0840 - val_loss: 319.1640 - val_mae: 15.0225\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 308.0588 - mae: 14.0823 - val_loss: 319.0643 - val_mae: 15.0201\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 307.9822 - mae: 14.0808 - val_loss: 318.9633 - val_mae: 15.0178\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 307.9066 - mae: 14.0792 - val_loss: 318.8607 - val_mae: 15.0154\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 307.8237 - mae: 14.0776 - val_loss: 318.7613 - val_mae: 15.0131\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 307.7433 - mae: 14.0760 - val_loss: 318.6648 - val_mae: 15.0108\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 307.6719 - mae: 14.0745 - val_loss: 318.5639 - val_mae: 15.0085\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 307.5916 - mae: 14.0729 - val_loss: 318.4666 - val_mae: 15.0061\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 307.5143 - mae: 14.0714 - val_loss: 318.3683 - val_mae: 15.0038\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 307.4424 - mae: 14.0700 - val_loss: 318.2668 - val_mae: 15.0014\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 307.3596 - mae: 14.0683 - val_loss: 318.1719 - val_mae: 14.9991\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 307.2844 - mae: 14.0669 - val_loss: 318.0736 - val_mae: 14.9967\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 307.2097 - mae: 14.0653 - val_loss: 317.9713 - val_mae: 14.9944\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 307.1328 - mae: 14.0638 - val_loss: 317.8687 - val_mae: 14.9919\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 307.0527 - mae: 14.0622 - val_loss: 317.7719 - val_mae: 14.9896\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 306.9763 - mae: 14.0606 - val_loss: 317.6736 - val_mae: 14.9873\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 306.8984 - mae: 14.0591 - val_loss: 317.5785 - val_mae: 14.9850\n",
      "Epoch 21: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=adam; total time=   2.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 31ms/step - loss: 257.7128 - mae: 12.7641 - val_loss: 104.2864 - val_mae: 9.1650\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 257.6518 - mae: 12.7621 - val_loss: 104.2391 - val_mae: 9.1628\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 257.5888 - mae: 12.7601 - val_loss: 104.1918 - val_mae: 9.1607\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 257.5287 - mae: 12.7580 - val_loss: 104.1433 - val_mae: 9.1584\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 257.4658 - mae: 12.7559 - val_loss: 104.0953 - val_mae: 9.1562\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 257.4059 - mae: 12.7539 - val_loss: 104.0471 - val_mae: 9.1540\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 257.3445 - mae: 12.7519 - val_loss: 104.0001 - val_mae: 9.1518\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 257.2826 - mae: 12.7498 - val_loss: 103.9545 - val_mae: 9.1497\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 257.2249 - mae: 12.7478 - val_loss: 103.9075 - val_mae: 9.1475\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 257.1626 - mae: 12.7458 - val_loss: 103.8607 - val_mae: 9.1454\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 257.1023 - mae: 12.7437 - val_loss: 103.8133 - val_mae: 9.1432\n",
      "Epoch 11: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=adam; total time=   2.1s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 30ms/step - loss: 1404.1656 - mae: 35.6016 - val_loss: 1391.3906 - val_mae: 36.0465\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1403.8635 - mae: 35.5977 - val_loss: 1391.0903 - val_mae: 36.0424\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1403.5687 - mae: 35.5938 - val_loss: 1390.7874 - val_mae: 36.0382\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1403.2690 - mae: 35.5898 - val_loss: 1390.4834 - val_mae: 36.0340\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1402.9720 - mae: 35.5859 - val_loss: 1390.1787 - val_mae: 36.0299\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1402.6740 - mae: 35.5819 - val_loss: 1389.8756 - val_mae: 36.0257\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1402.3763 - mae: 35.5780 - val_loss: 1389.5717 - val_mae: 36.0216\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1402.0743 - mae: 35.5740 - val_loss: 1389.2672 - val_mae: 36.0174\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1401.7778 - mae: 35.5701 - val_loss: 1388.9633 - val_mae: 36.0132\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1401.4788 - mae: 35.5662 - val_loss: 1388.6609 - val_mae: 36.0091\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1401.1825 - mae: 35.5623 - val_loss: 1388.3561 - val_mae: 36.0049\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1400.8824 - mae: 35.5583 - val_loss: 1388.0525 - val_mae: 36.0008\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1400.5845 - mae: 35.5544 - val_loss: 1387.7510 - val_mae: 35.9966\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1400.2870 - mae: 35.5505 - val_loss: 1387.4485 - val_mae: 35.9925\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1399.9906 - mae: 35.5465 - val_loss: 1387.1438 - val_mae: 35.9883\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1399.6908 - mae: 35.5425 - val_loss: 1386.8397 - val_mae: 35.9841\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1399.3947 - mae: 35.5386 - val_loss: 1386.5375 - val_mae: 35.9800\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1399.0946 - mae: 35.5347 - val_loss: 1386.2329 - val_mae: 35.9758\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1398.7959 - mae: 35.5308 - val_loss: 1385.9307 - val_mae: 35.9717\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1398.4985 - mae: 35.5268 - val_loss: 1385.6320 - val_mae: 35.9676\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1398.2070 - mae: 35.5229 - val_loss: 1385.3274 - val_mae: 35.9634\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1397.9067 - mae: 35.5190 - val_loss: 1385.0281 - val_mae: 35.9593\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1397.6091 - mae: 35.5151 - val_loss: 1384.7295 - val_mae: 35.9552\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1397.3169 - mae: 35.5112 - val_loss: 1384.4263 - val_mae: 35.9511\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1397.0231 - mae: 35.5073 - val_loss: 1384.1226 - val_mae: 35.9469\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1396.7203 - mae: 35.5033 - val_loss: 1383.8214 - val_mae: 35.9428\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1396.4241 - mae: 35.4994 - val_loss: 1383.5201 - val_mae: 35.9386\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1396.1316 - mae: 35.4955 - val_loss: 1383.2153 - val_mae: 35.9345\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1395.8296 - mae: 35.4915 - val_loss: 1382.9141 - val_mae: 35.9303\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1395.5333 - mae: 35.4876 - val_loss: 1382.6127 - val_mae: 35.9262\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1395.2382 - mae: 35.4837 - val_loss: 1382.3107 - val_mae: 35.9221\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1394.9432 - mae: 35.4798 - val_loss: 1382.0094 - val_mae: 35.9179\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1394.6458 - mae: 35.4758 - val_loss: 1381.7070 - val_mae: 35.9138\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1394.3462 - mae: 35.4718 - val_loss: 1381.4095 - val_mae: 35.9097\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1394.0504 - mae: 35.4680 - val_loss: 1381.1115 - val_mae: 35.9056\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1393.7601 - mae: 35.4641 - val_loss: 1380.8091 - val_mae: 35.9015\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1393.4620 - mae: 35.4601 - val_loss: 1380.5083 - val_mae: 35.8973\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1393.1692 - mae: 35.4563 - val_loss: 1380.2064 - val_mae: 35.8932\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1392.8760 - mae: 35.4524 - val_loss: 1379.9052 - val_mae: 35.8890\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1392.5823 - mae: 35.4485 - val_loss: 1379.6062 - val_mae: 35.8849\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1392.2909 - mae: 35.4446 - val_loss: 1379.3094 - val_mae: 35.8808\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1391.9983 - mae: 35.4408 - val_loss: 1379.0125 - val_mae: 35.8768\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1391.7042 - mae: 35.4368 - val_loss: 1378.7107 - val_mae: 35.8726\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1391.4108 - mae: 35.4329 - val_loss: 1378.4099 - val_mae: 35.8685\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1391.1150 - mae: 35.4290 - val_loss: 1378.1102 - val_mae: 35.8643\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1390.8230 - mae: 35.4251 - val_loss: 1377.8093 - val_mae: 35.8602\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1390.5304 - mae: 35.4212 - val_loss: 1377.5068 - val_mae: 35.8560\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1390.2362 - mae: 35.4174 - val_loss: 1377.2076 - val_mae: 35.8519\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1389.9469 - mae: 35.4135 - val_loss: 1376.9042 - val_mae: 35.8477\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1389.6508 - mae: 35.4096 - val_loss: 1376.6091 - val_mae: 35.8436\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1389.3617 - mae: 35.4057 - val_loss: 1376.3081 - val_mae: 35.8395\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1389.0658 - mae: 35.4018 - val_loss: 1376.0107 - val_mae: 35.8354\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1388.7761 - mae: 35.3980 - val_loss: 1375.7115 - val_mae: 35.8313\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1388.4888 - mae: 35.3941 - val_loss: 1375.4156 - val_mae: 35.8272\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1388.1941 - mae: 35.3902 - val_loss: 1375.1208 - val_mae: 35.8231\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1387.9043 - mae: 35.3863 - val_loss: 1374.8236 - val_mae: 35.8190\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1387.6144 - mae: 35.3825 - val_loss: 1374.5237 - val_mae: 35.8149\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1387.3212 - mae: 35.3786 - val_loss: 1374.2244 - val_mae: 35.8108\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1387.0264 - mae: 35.3747 - val_loss: 1373.9288 - val_mae: 35.8067\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1386.7373 - mae: 35.3709 - val_loss: 1373.6283 - val_mae: 35.8026\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1386.4451 - mae: 35.3670 - val_loss: 1373.3292 - val_mae: 35.7985\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1386.1534 - mae: 35.3631 - val_loss: 1373.0284 - val_mae: 35.7943\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1385.8600 - mae: 35.3592 - val_loss: 1372.7321 - val_mae: 35.7902\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1385.5671 - mae: 35.3553 - val_loss: 1372.4352 - val_mae: 35.7861\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1385.2788 - mae: 35.3515 - val_loss: 1372.1390 - val_mae: 35.7821\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1384.9849 - mae: 35.3476 - val_loss: 1371.8419 - val_mae: 35.7780\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1384.6946 - mae: 35.3437 - val_loss: 1371.5439 - val_mae: 35.7739\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1384.4044 - mae: 35.3398 - val_loss: 1371.2451 - val_mae: 35.7697\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1384.1122 - mae: 35.3360 - val_loss: 1370.9481 - val_mae: 35.7656\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1383.8199 - mae: 35.3321 - val_loss: 1370.6508 - val_mae: 35.7615\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1383.5314 - mae: 35.3283 - val_loss: 1370.3549 - val_mae: 35.7574\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1383.2449 - mae: 35.3244 - val_loss: 1370.0580 - val_mae: 35.7533\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1382.9539 - mae: 35.3206 - val_loss: 1369.7615 - val_mae: 35.7492\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1382.6650 - mae: 35.3167 - val_loss: 1369.4677 - val_mae: 35.7452\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1382.3757 - mae: 35.3129 - val_loss: 1369.1711 - val_mae: 35.7411\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1382.0912 - mae: 35.3091 - val_loss: 1368.8713 - val_mae: 35.7369\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1381.8003 - mae: 35.3052 - val_loss: 1368.5726 - val_mae: 35.7328\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1381.5089 - mae: 35.3013 - val_loss: 1368.2762 - val_mae: 35.7287\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1381.2234 - mae: 35.2975 - val_loss: 1367.9783 - val_mae: 35.7246\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1380.9301 - mae: 35.2936 - val_loss: 1367.6846 - val_mae: 35.7205\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1380.6433 - mae: 35.2898 - val_loss: 1367.3901 - val_mae: 35.7165\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1380.3552 - mae: 35.2859 - val_loss: 1367.0923 - val_mae: 35.7123\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1380.0664 - mae: 35.2821 - val_loss: 1366.7966 - val_mae: 35.7083\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1379.7732 - mae: 35.2782 - val_loss: 1366.5006 - val_mae: 35.7042\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1379.4866 - mae: 35.2743 - val_loss: 1366.2020 - val_mae: 35.7000\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1379.1951 - mae: 35.2705 - val_loss: 1365.9054 - val_mae: 35.6959\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1378.9072 - mae: 35.2666 - val_loss: 1365.6066 - val_mae: 35.6918\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1378.6196 - mae: 35.2628 - val_loss: 1365.3101 - val_mae: 35.6877\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1378.3270 - mae: 35.2589 - val_loss: 1365.0172 - val_mae: 35.6837\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1378.0427 - mae: 35.2551 - val_loss: 1364.7225 - val_mae: 35.6796\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1377.7531 - mae: 35.2513 - val_loss: 1364.4297 - val_mae: 35.6755\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1377.4673 - mae: 35.2475 - val_loss: 1364.1368 - val_mae: 35.6715\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1377.1824 - mae: 35.2436 - val_loss: 1363.8385 - val_mae: 35.6673\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1376.8937 - mae: 35.2398 - val_loss: 1363.5411 - val_mae: 35.6632\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1376.6018 - mae: 35.2359 - val_loss: 1363.2444 - val_mae: 35.6591\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1376.3181 - mae: 35.2321 - val_loss: 1362.9471 - val_mae: 35.6550\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1376.0250 - mae: 35.2282 - val_loss: 1362.6539 - val_mae: 35.6509\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1375.7397 - mae: 35.2244 - val_loss: 1362.3595 - val_mae: 35.6468\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1375.4550 - mae: 35.2206 - val_loss: 1362.0640 - val_mae: 35.6428\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1375.1652 - mae: 35.2167 - val_loss: 1361.7679 - val_mae: 35.6387\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=3, model__n_neurons=5, model__optimizer=adam; total time=   9.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 26ms/step - loss: 18475.4844 - mae: 94.6074 - val_loss: 449.1196 - val_mae: 20.2733\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 597.9331 - mae: 22.6922 - val_loss: 448.9728 - val_mae: 20.2697\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 597.6467 - mae: 22.6873 - val_loss: 448.8265 - val_mae: 20.2661\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 596.3835 - mae: 22.6680 - val_loss: 446.3818 - val_mae: 20.2193\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 546.7879 - mae: 21.5311 - val_loss: 63.4254 - val_mae: 7.0084\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 104.0180 - mae: 6.9537 - val_loss: 25.8785 - val_mae: 4.3073\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 90.1256 - mae: 6.5010 - val_loss: 26.0087 - val_mae: 4.1633\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 92.0663 - mae: 6.6668 - val_loss: 32.1631 - val_mae: 4.3839\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 85.8397 - mae: 6.3652 - val_loss: 50.6527 - val_mae: 5.4080\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 87.7014 - mae: 6.6546 - val_loss: 31.3995 - val_mae: 4.3157\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 85.6506 - mae: 6.5054 - val_loss: 30.3493 - val_mae: 4.2363\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 88.3019 - mae: 6.6724 - val_loss: 24.6843 - val_mae: 3.9585\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 84.4846 - mae: 6.4128 - val_loss: 48.0619 - val_mae: 5.3129\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 94.7049 - mae: 7.0334 - val_loss: 22.8955 - val_mae: 3.9607\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 82.0217 - mae: 6.3856 - val_loss: 22.6127 - val_mae: 4.0291\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 85.1208 - mae: 6.6739 - val_loss: 23.1011 - val_mae: 4.1245\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 86.2510 - mae: 6.4909 - val_loss: 27.6075 - val_mae: 4.0528\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 85.0959 - mae: 6.6080 - val_loss: 26.5134 - val_mae: 4.0045\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80.7851 - mae: 6.3407 - val_loss: 31.3066 - val_mae: 4.2955\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 84.6705 - mae: 6.4919 - val_loss: 32.3257 - val_mae: 4.3591\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 81.3056 - mae: 6.5684 - val_loss: 22.8083 - val_mae: 3.8573\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 85.7615 - mae: 6.4488 - val_loss: 22.0148 - val_mae: 3.9996\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 80.0031 - mae: 6.1839 - val_loss: 35.7201 - val_mae: 4.6173\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 83.9544 - mae: 6.4944 - val_loss: 23.9696 - val_mae: 3.8801\n",
      "Epoch 24: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=nesterov; total time=   2.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 27ms/step - loss: 1059.4690 - mae: 25.0394 - val_loss: 23.6281 - val_mae: 3.9095\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 104.3909 - mae: 7.6255 - val_loss: 21.8718 - val_mae: 3.7412\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 104.2300 - mae: 7.3647 - val_loss: 30.4001 - val_mae: 4.2730\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 95.3111 - mae: 7.3275 - val_loss: 28.5559 - val_mae: 4.2510\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 95.0739 - mae: 7.2159 - val_loss: 20.9158 - val_mae: 3.6588\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 97.0012 - mae: 7.2252 - val_loss: 17.3389 - val_mae: 3.3801\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 95.9275 - mae: 7.0393 - val_loss: 18.3039 - val_mae: 3.4751\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 106.0662 - mae: 7.3262 - val_loss: 30.1333 - val_mae: 4.5946\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 87.3604 - mae: 6.7927 - val_loss: 47.4977 - val_mae: 5.8722\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 87.6493 - mae: 6.9107 - val_loss: 44.3565 - val_mae: 5.6190\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 90.9440 - mae: 6.9606 - val_loss: 16.4218 - val_mae: 3.3904\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 88.5770 - mae: 6.7390 - val_loss: 40.5969 - val_mae: 5.4022\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 87.5655 - mae: 6.7987 - val_loss: 39.5280 - val_mae: 5.2780\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 92.4271 - mae: 7.1757 - val_loss: 16.1495 - val_mae: 3.3342\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 83.0087 - mae: 6.5509 - val_loss: 21.0882 - val_mae: 3.8947\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 84.6940 - mae: 6.6244 - val_loss: 18.3834 - val_mae: 3.5691\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 86.6808 - mae: 6.6209 - val_loss: 44.3082 - val_mae: 5.6716\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 92.9177 - mae: 6.9948 - val_loss: 17.8292 - val_mae: 3.5834\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 81.7576 - mae: 6.4439 - val_loss: 25.9726 - val_mae: 4.3243\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 85.0129 - mae: 6.6714 - val_loss: 37.5161 - val_mae: 5.1796\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 79.6589 - mae: 6.6305 - val_loss: 16.8617 - val_mae: 3.5086\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 84.9533 - mae: 6.5998 - val_loss: 15.3938 - val_mae: 3.2193\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 80.5859 - mae: 6.3594 - val_loss: 25.2519 - val_mae: 4.3133\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 82.6054 - mae: 6.4765 - val_loss: 26.9586 - val_mae: 4.4517\n",
      "Epoch 24: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=nesterov; total time=   2.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 29ms/step - loss: 1967.8907 - mae: 33.3343 - val_loss: 154.7218 - val_mae: 11.1258\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 149.3541 - mae: 9.7558 - val_loss: 93.5486 - val_mae: 8.2734\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 109.1479 - mae: 7.8275 - val_loss: 72.1032 - val_mae: 7.2947\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 97.4075 - mae: 7.3776 - val_loss: 60.7371 - val_mae: 6.7294\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 91.9801 - mae: 7.1471 - val_loss: 61.0775 - val_mae: 6.4835\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 87.8450 - mae: 6.9602 - val_loss: 54.7149 - val_mae: 6.2476\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 87.0050 - mae: 6.9360 - val_loss: 50.5416 - val_mae: 5.9967\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 84.3418 - mae: 6.8714 - val_loss: 57.4550 - val_mae: 6.0958\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 81.1031 - mae: 6.6657 - val_loss: 50.4001 - val_mae: 5.7498\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 77.4969 - mae: 6.4412 - val_loss: 45.3908 - val_mae: 5.6609\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 76.3867 - mae: 6.3844 - val_loss: 48.0917 - val_mae: 5.7037\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 73.3812 - mae: 6.2757 - val_loss: 51.3656 - val_mae: 5.7612\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 73.4369 - mae: 6.2314 - val_loss: 52.5263 - val_mae: 5.6953\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 69.2689 - mae: 5.9803 - val_loss: 40.4200 - val_mae: 5.2084\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 66.7482 - mae: 5.8542 - val_loss: 45.5313 - val_mae: 5.3216\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 66.1517 - mae: 5.7748 - val_loss: 42.7415 - val_mae: 5.1934\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 66.6473 - mae: 5.8940 - val_loss: 37.7489 - val_mae: 5.0614\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 62.7203 - mae: 5.6892 - val_loss: 37.2303 - val_mae: 4.9430\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 62.3911 - mae: 5.6244 - val_loss: 44.1464 - val_mae: 5.2476\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 61.6437 - mae: 5.5301 - val_loss: 35.6656 - val_mae: 4.8973\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 60.9867 - mae: 5.5850 - val_loss: 38.2410 - val_mae: 4.9173\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 58.5332 - mae: 5.3055 - val_loss: 36.7245 - val_mae: 4.7805\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 57.9481 - mae: 5.4004 - val_loss: 46.0227 - val_mae: 5.2226\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 57.0551 - mae: 5.2547 - val_loss: 40.0359 - val_mae: 5.0833\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 60.1055 - mae: 5.5798 - val_loss: 37.3320 - val_mae: 4.7683\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 55.2919 - mae: 5.2135 - val_loss: 36.1903 - val_mae: 4.8328\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 58.5466 - mae: 5.4827 - val_loss: 36.6633 - val_mae: 4.7377\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 57.2924 - mae: 5.5208 - val_loss: 34.6069 - val_mae: 4.7133\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 55.0213 - mae: 5.2867 - val_loss: 36.4608 - val_mae: 4.6667\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 54.4413 - mae: 5.2037 - val_loss: 39.1045 - val_mae: 4.8123\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 53.7912 - mae: 5.2161 - val_loss: 39.2728 - val_mae: 4.7209\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 56.3217 - mae: 5.2011 - val_loss: 38.3808 - val_mae: 5.0239\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 54.2362 - mae: 5.2611 - val_loss: 39.5200 - val_mae: 4.7375\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 54.3282 - mae: 5.1263 - val_loss: 34.6159 - val_mae: 4.6225\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 53.6034 - mae: 5.1401 - val_loss: 33.9901 - val_mae: 4.6156\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 54.8201 - mae: 5.1974 - val_loss: 38.7685 - val_mae: 4.7036\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 53.9960 - mae: 5.2428 - val_loss: 40.6666 - val_mae: 4.8579\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 57.4883 - mae: 5.3972 - val_loss: 36.4569 - val_mae: 4.5663\n",
      "Epoch 38: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.1, model__n_hidden=2, model__n_neurons=25, model__optimizer=nesterov; total time=   5.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 27ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   2.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 30ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   2.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 28ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 23ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=0.0001, model__momentum=0.5, model__n_hidden=3, model__n_neurons=125, model__optimizer=momentum; total time=   2.0s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:771: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_scorer.py\", line 418, in _passthrough_scorer\n",
      "    return estimator.score(*args, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1100, in score\n",
      "    return self.scorer(y, y_pred, sample_weight=sample_weight, **score_args)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/scikeras/wrappers.py\", line 1697, in scorer\n",
      "    return sklearn_r2_score(y_true, y_pred, **kwargs)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 774, in r2_score\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_regression.py\", line 91, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 792, in check_array\n",
      "    _assert_all_finite(array, allow_nan=force_all_finite == \"allow-nan\")\n",
      "  File \"/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\", line 114, in _assert_all_finite\n",
      "    raise ValueError(\n",
      "ValueError: Input contains NaN, infinity or a value too large for dtype('float32').\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 24ms/step - loss: 1576.4769 - mae: 31.0177 - val_loss: 108.1109 - val_mae: 8.9697\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 98.0885 - mae: 7.9139 - val_loss: 54.9634 - val_mae: 5.7934\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 85.2625 - mae: 6.8335 - val_loss: 52.0326 - val_mae: 5.6580\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 84.3973 - mae: 6.7652 - val_loss: 50.1613 - val_mae: 5.5616\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 84.2943 - mae: 6.7329 - val_loss: 49.0771 - val_mae: 5.5018\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 84.0199 - mae: 6.7467 - val_loss: 47.2891 - val_mae: 5.4162\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 83.3032 - mae: 6.6852 - val_loss: 45.7361 - val_mae: 5.3428\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 83.4284 - mae: 6.6636 - val_loss: 47.9029 - val_mae: 5.4222\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 82.1941 - mae: 6.6226 - val_loss: 49.4546 - val_mae: 5.4862\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 82.1439 - mae: 6.7191 - val_loss: 46.9784 - val_mae: 5.3544\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81.7033 - mae: 6.6954 - val_loss: 43.4731 - val_mae: 5.2097\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 81.5628 - mae: 6.5759 - val_loss: 43.1470 - val_mae: 5.1851\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 80.8279 - mae: 6.5309 - val_loss: 47.4877 - val_mae: 5.3947\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 80.7822 - mae: 6.7169 - val_loss: 41.6226 - val_mae: 5.1040\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80.2408 - mae: 6.5545 - val_loss: 40.7673 - val_mae: 5.0587\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79.9474 - mae: 6.5038 - val_loss: 41.6433 - val_mae: 5.0812\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79.8159 - mae: 6.5515 - val_loss: 40.2082 - val_mae: 5.0172\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 79.6610 - mae: 6.5089 - val_loss: 40.2514 - val_mae: 5.0098\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 79.1311 - mae: 6.4104 - val_loss: 43.4099 - val_mae: 5.1685\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 79.6854 - mae: 6.5575 - val_loss: 43.6987 - val_mae: 5.1860\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 78.9300 - mae: 6.6029 - val_loss: 39.5248 - val_mae: 4.9544\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 78.7739 - mae: 6.4472 - val_loss: 39.7014 - val_mae: 4.9639\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 78.3874 - mae: 6.4059 - val_loss: 42.5602 - val_mae: 5.1182\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 78.3424 - mae: 6.5981 - val_loss: 37.3209 - val_mae: 4.8398\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 77.8873 - mae: 6.3850 - val_loss: 37.6833 - val_mae: 4.8483\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 77.5901 - mae: 6.3301 - val_loss: 40.6321 - val_mae: 5.0132\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 77.6795 - mae: 6.4879 - val_loss: 37.7547 - val_mae: 4.8465\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 78.0603 - mae: 6.4624 - val_loss: 36.5571 - val_mae: 4.7816\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 77.1885 - mae: 6.3448 - val_loss: 37.8426 - val_mae: 4.8534\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 77.1990 - mae: 6.3783 - val_loss: 37.8150 - val_mae: 4.8514\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 76.8408 - mae: 6.4313 - val_loss: 35.3307 - val_mae: 4.7059\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 76.9131 - mae: 6.2761 - val_loss: 38.0731 - val_mae: 4.8642\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 77.7513 - mae: 6.4970 - val_loss: 37.7671 - val_mae: 4.8439\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 76.3550 - mae: 6.3776 - val_loss: 38.0230 - val_mae: 4.8567\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 76.1383 - mae: 6.3715 - val_loss: 38.0102 - val_mae: 4.8535\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 76.0844 - mae: 6.3918 - val_loss: 36.6647 - val_mae: 4.7729\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 75.7636 - mae: 6.3900 - val_loss: 35.0379 - val_mae: 4.6658\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 76.4802 - mae: 6.3039 - val_loss: 34.6242 - val_mae: 4.6416\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 75.5715 - mae: 6.2721 - val_loss: 36.2613 - val_mae: 4.7442\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 75.4261 - mae: 6.2978 - val_loss: 37.3399 - val_mae: 4.8050\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 75.1606 - mae: 6.4026 - val_loss: 34.3988 - val_mae: 4.6195\n",
      "Epoch 41: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=sgd; total time=   5.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 24ms/step - loss: 1750.3989 - mae: 38.6098 - val_loss: 426.0981 - val_mae: 19.9612\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 432.1005 - mae: 17.1973 - val_loss: 97.6699 - val_mae: 8.4988\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 182.6268 - mae: 9.6596 - val_loss: 42.3904 - val_mae: 5.5825\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 126.2798 - mae: 7.9132 - val_loss: 39.9496 - val_mae: 5.2355\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 113.6287 - mae: 7.8580 - val_loss: 42.5121 - val_mae: 5.3126\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 111.5801 - mae: 7.9707 - val_loss: 42.5798 - val_mae: 5.3129\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 109.2734 - mae: 7.8790 - val_loss: 43.3797 - val_mae: 5.3241\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 108.3742 - mae: 7.8203 - val_loss: 45.3650 - val_mae: 5.3521\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 106.5871 - mae: 7.8466 - val_loss: 46.2414 - val_mae: 5.3851\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 105.8932 - mae: 7.8174 - val_loss: 46.6221 - val_mae: 5.3885\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 105.3293 - mae: 7.8195 - val_loss: 45.0941 - val_mae: 5.3630\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 104.5598 - mae: 7.7218 - val_loss: 45.7348 - val_mae: 5.3705\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 104.0199 - mae: 7.7318 - val_loss: 45.7586 - val_mae: 5.3685\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 104.0241 - mae: 7.7425 - val_loss: 45.3705 - val_mae: 5.3599\n",
      "Epoch 14: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=sgd; total time=   1.9s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 26ms/step - loss: 7415.4009 - mae: 57.2012 - val_loss: 499.0460 - val_mae: 13.7170\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 514.4987 - mae: 15.2259 - val_loss: 292.0953 - val_mae: 10.5366\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 218.5316 - mae: 10.3178 - val_loss: 223.3253 - val_mae: 9.3603\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 148.8849 - mae: 8.5173 - val_loss: 192.5954 - val_mae: 8.9117\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 126.5730 - mae: 7.9917 - val_loss: 169.3153 - val_mae: 8.4247\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 112.7005 - mae: 7.5762 - val_loss: 153.6285 - val_mae: 8.0798\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 102.8212 - mae: 7.2768 - val_loss: 138.6425 - val_mae: 7.6517\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 95.5071 - mae: 7.0802 - val_loss: 126.7384 - val_mae: 7.2741\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 88.9520 - mae: 6.7612 - val_loss: 115.7164 - val_mae: 6.9569\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 83.5592 - mae: 6.5028 - val_loss: 109.8096 - val_mae: 6.8036\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80.0721 - mae: 6.4017 - val_loss: 104.3709 - val_mae: 6.6081\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 77.4943 - mae: 6.2927 - val_loss: 98.8370 - val_mae: 6.3927\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 75.1651 - mae: 6.1632 - val_loss: 95.3928 - val_mae: 6.2853\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 73.3540 - mae: 6.0697 - val_loss: 91.3128 - val_mae: 6.1747\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71.5610 - mae: 6.0410 - val_loss: 88.0245 - val_mae: 6.0702\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 70.0036 - mae: 5.9612 - val_loss: 85.6290 - val_mae: 6.0005\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 69.0325 - mae: 5.9447 - val_loss: 81.9158 - val_mae: 5.8856\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 67.8463 - mae: 5.9023 - val_loss: 78.1820 - val_mae: 5.7700\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 66.8895 - mae: 5.8052 - val_loss: 76.7246 - val_mae: 5.7162\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 66.0716 - mae: 5.7523 - val_loss: 75.8954 - val_mae: 5.7105\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 65.4917 - mae: 5.7597 - val_loss: 74.4750 - val_mae: 5.6814\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 64.5836 - mae: 5.7045 - val_loss: 74.0427 - val_mae: 5.6860\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 64.4594 - mae: 5.6783 - val_loss: 73.9719 - val_mae: 5.6876\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 63.5979 - mae: 5.6716 - val_loss: 74.5679 - val_mae: 5.7224\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 63.4042 - mae: 5.6893 - val_loss: 71.6683 - val_mae: 5.6317\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 62.9312 - mae: 5.6155 - val_loss: 72.6434 - val_mae: 5.6789\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 62.8318 - mae: 5.6790 - val_loss: 71.4218 - val_mae: 5.6287\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 62.5879 - mae: 5.6264 - val_loss: 71.2467 - val_mae: 5.6314\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 62.1161 - mae: 5.5979 - val_loss: 70.8228 - val_mae: 5.6179\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 61.9594 - mae: 5.5903 - val_loss: 70.8775 - val_mae: 5.6174\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 61.7184 - mae: 5.5829 - val_loss: 70.3362 - val_mae: 5.5939\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 61.6271 - mae: 5.5482 - val_loss: 70.9067 - val_mae: 5.6272\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 61.4486 - mae: 5.5886 - val_loss: 69.7919 - val_mae: 5.5823\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 61.2824 - mae: 5.5504 - val_loss: 69.9205 - val_mae: 5.5872\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 61.0451 - mae: 5.5262 - val_loss: 69.9925 - val_mae: 5.5989\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 60.9263 - mae: 5.5381 - val_loss: 69.3091 - val_mae: 5.5733\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 61.0105 - mae: 5.5783 - val_loss: 68.0188 - val_mae: 5.5286\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 61.0155 - mae: 5.5176 - val_loss: 67.9690 - val_mae: 5.5299\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 60.8391 - mae: 5.4853 - val_loss: 68.9284 - val_mae: 5.5632\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 60.4294 - mae: 5.4898 - val_loss: 69.0492 - val_mae: 5.5676\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 60.5833 - mae: 5.4685 - val_loss: 68.6287 - val_mae: 5.5617\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 60.3180 - mae: 5.4548 - val_loss: 70.7118 - val_mae: 5.6819\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 60.4314 - mae: 5.5360 - val_loss: 69.3061 - val_mae: 5.5958\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 60.3665 - mae: 5.5116 - val_loss: 68.4648 - val_mae: 5.5481\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 60.1874 - mae: 5.4692 - val_loss: 68.6619 - val_mae: 5.5548\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 59.9927 - mae: 5.4635 - val_loss: 69.2787 - val_mae: 5.5752\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 60.0381 - mae: 5.5046 - val_loss: 68.0012 - val_mae: 5.5263\n",
      "Epoch 47: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.5, model__n_hidden=2, model__n_neurons=5, model__optimizer=sgd; total time=   4.6s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 25ms/step - loss: 308.7986 - mae: 13.8137 - val_loss: 204.5375 - val_mae: 11.9803\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 286.7713 - mae: 13.4971 - val_loss: 167.4830 - val_mae: 10.8317\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 266.9285 - mae: 13.1565 - val_loss: 156.6212 - val_mae: 10.5167\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 256.5597 - mae: 13.0033 - val_loss: 134.9056 - val_mae: 9.6648\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 251.7570 - mae: 12.7389 - val_loss: 135.3039 - val_mae: 9.8915\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 252.1862 - mae: 12.8164 - val_loss: 119.8921 - val_mae: 9.0200\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 246.6108 - mae: 12.6045 - val_loss: 120.3209 - val_mae: 9.3201\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 244.7738 - mae: 12.6781 - val_loss: 122.9342 - val_mae: 9.4279\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 236.0060 - mae: 12.2921 - val_loss: 150.0483 - val_mae: 10.3383\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 240.0403 - mae: 12.3901 - val_loss: 143.0628 - val_mae: 10.1493\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 238.3070 - mae: 12.2386 - val_loss: 123.0045 - val_mae: 9.4922\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 233.1576 - mae: 12.2274 - val_loss: 104.8256 - val_mae: 8.6864\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 226.5724 - mae: 12.0517 - val_loss: 139.3317 - val_mae: 10.0023\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 231.8103 - mae: 12.1080 - val_loss: 99.9306 - val_mae: 8.4111\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 218.5075 - mae: 11.8455 - val_loss: 98.4408 - val_mae: 7.9892\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 220.0506 - mae: 11.9066 - val_loss: 96.7738 - val_mae: 7.9353\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 224.1867 - mae: 11.9732 - val_loss: 96.3987 - val_mae: 8.2386\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 215.3422 - mae: 11.6677 - val_loss: 105.3905 - val_mae: 8.7349\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 211.5914 - mae: 11.5087 - val_loss: 116.2822 - val_mae: 9.1275\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 213.3321 - mae: 11.5428 - val_loss: 111.4239 - val_mae: 8.9469\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 208.7610 - mae: 11.4932 - val_loss: 92.1043 - val_mae: 8.0268\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 209.5027 - mae: 11.3703 - val_loss: 89.3109 - val_mae: 7.8766\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 205.5561 - mae: 11.3027 - val_loss: 134.5916 - val_mae: 9.7236\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 208.0058 - mae: 11.2637 - val_loss: 85.2275 - val_mae: 7.5787\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 198.7529 - mae: 10.9627 - val_loss: 84.7141 - val_mae: 7.6540\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 198.4944 - mae: 11.0586 - val_loss: 100.5402 - val_mae: 8.4507\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 200.1306 - mae: 11.0620 - val_loss: 91.0553 - val_mae: 8.0175\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 206.1629 - mae: 11.1370 - val_loss: 81.5904 - val_mae: 7.4486\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 192.6203 - mae: 10.7351 - val_loss: 102.4048 - val_mae: 8.4632\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 192.7881 - mae: 10.8313 - val_loss: 82.4062 - val_mae: 7.5329\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 190.4826 - mae: 10.7281 - val_loss: 80.1658 - val_mae: 7.1135\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 188.9574 - mae: 10.5635 - val_loss: 98.0194 - val_mae: 8.2508\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 193.6281 - mae: 10.8297 - val_loss: 123.1895 - val_mae: 9.2414\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 187.9716 - mae: 10.6754 - val_loss: 82.7988 - val_mae: 7.5386\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 184.0752 - mae: 10.4609 - val_loss: 76.5730 - val_mae: 7.2346\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 186.4876 - mae: 10.5077 - val_loss: 75.2932 - val_mae: 7.0150\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 182.1284 - mae: 10.2489 - val_loss: 77.0131 - val_mae: 7.2364\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 182.5122 - mae: 10.3431 - val_loss: 72.7250 - val_mae: 6.9595\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 175.8660 - mae: 10.1735 - val_loss: 89.5161 - val_mae: 7.7999\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 178.0005 - mae: 10.2440 - val_loss: 87.0692 - val_mae: 7.6857\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 174.5042 - mae: 10.1752 - val_loss: 70.1646 - val_mae: 6.8686\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 171.7487 - mae: 9.9454 - val_loss: 76.5717 - val_mae: 7.2187\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 170.6374 - mae: 10.0199 - val_loss: 68.9212 - val_mae: 6.7433\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 170.6857 - mae: 9.9725 - val_loss: 81.1282 - val_mae: 7.3856\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 168.9757 - mae: 9.8053 - val_loss: 67.7188 - val_mae: 6.7132\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 167.9968 - mae: 9.7791 - val_loss: 67.5335 - val_mae: 6.7320\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 169.3292 - mae: 9.7294 - val_loss: 82.7955 - val_mae: 7.4310\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 167.1302 - mae: 9.8383 - val_loss: 78.1096 - val_mae: 7.2235\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 165.3501 - mae: 9.6424 - val_loss: 78.0861 - val_mae: 7.2188\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 164.1903 - mae: 9.6247 - val_loss: 72.8845 - val_mae: 6.9865\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 165.5243 - mae: 9.6725 - val_loss: 68.2813 - val_mae: 6.7518\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 165.2525 - mae: 9.5290 - val_loss: 63.6133 - val_mae: 6.4802\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 163.5924 - mae: 9.5971 - val_loss: 67.2804 - val_mae: 6.6804\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 156.2694 - mae: 9.4026 - val_loss: 66.8445 - val_mae: 6.3190\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 157.4503 - mae: 9.1144 - val_loss: 87.1028 - val_mae: 7.5250\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 159.8429 - mae: 9.4360 - val_loss: 60.7234 - val_mae: 6.2718\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 154.8682 - mae: 9.1727 - val_loss: 65.4233 - val_mae: 6.5579\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 158.4357 - mae: 9.2852 - val_loss: 84.1845 - val_mae: 7.3881\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 156.4184 - mae: 9.3873 - val_loss: 71.8312 - val_mae: 6.3757\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 153.6009 - mae: 9.1185 - val_loss: 66.8313 - val_mae: 6.6186\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 156.6751 - mae: 9.2022 - val_loss: 58.2387 - val_mae: 6.0919\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 153.1641 - mae: 9.1405 - val_loss: 64.7585 - val_mae: 6.1811\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 154.7588 - mae: 9.0182 - val_loss: 60.2304 - val_mae: 6.2258\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 151.7229 - mae: 8.9905 - val_loss: 68.9591 - val_mae: 6.7228\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 153.9478 - mae: 9.1047 - val_loss: 58.0044 - val_mae: 6.0669\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 153.8163 - mae: 9.0079 - val_loss: 57.5446 - val_mae: 6.0397\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 150.2932 - mae: 8.9436 - val_loss: 57.8287 - val_mae: 6.0645\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 146.8115 - mae: 8.8945 - val_loss: 61.2698 - val_mae: 6.2667\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 146.7027 - mae: 8.8365 - val_loss: 69.6464 - val_mae: 6.7122\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 145.3932 - mae: 8.7995 - val_loss: 54.2777 - val_mae: 5.8732\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 145.5113 - mae: 8.7885 - val_loss: 66.3418 - val_mae: 6.1864\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 150.9853 - mae: 8.8282 - val_loss: 58.9666 - val_mae: 6.1104\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 144.8017 - mae: 8.7124 - val_loss: 54.1058 - val_mae: 5.8411\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 145.2940 - mae: 8.6604 - val_loss: 67.8383 - val_mae: 6.5929\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 147.4033 - mae: 8.7403 - val_loss: 54.1301 - val_mae: 5.8278\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 144.9049 - mae: 8.7630 - val_loss: 53.4107 - val_mae: 5.7919\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 141.2832 - mae: 8.5324 - val_loss: 52.3086 - val_mae: 5.7597\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 140.2770 - mae: 8.4546 - val_loss: 55.1615 - val_mae: 5.8724\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 139.1119 - mae: 8.5448 - val_loss: 52.2970 - val_mae: 5.7431\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 141.6681 - mae: 8.4896 - val_loss: 62.4742 - val_mae: 6.2927\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 141.5160 - mae: 8.5847 - val_loss: 51.7600 - val_mae: 5.6937\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 137.8652 - mae: 8.3581 - val_loss: 55.1653 - val_mae: 5.8744\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 136.3032 - mae: 8.3338 - val_loss: 59.9951 - val_mae: 6.1279\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 139.1145 - mae: 8.4457 - val_loss: 62.3991 - val_mae: 6.2392\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 138.8988 - mae: 8.3685 - val_loss: 59.1983 - val_mae: 6.0696\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 137.1174 - mae: 8.2946 - val_loss: 64.6884 - val_mae: 6.3358\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 133.7215 - mae: 8.3698 - val_loss: 50.9041 - val_mae: 5.6319\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 135.0651 - mae: 8.1753 - val_loss: 61.6863 - val_mae: 6.1751\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 138.8100 - mae: 8.3424 - val_loss: 57.5943 - val_mae: 5.9586\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 133.0209 - mae: 8.2554 - val_loss: 49.2458 - val_mae: 5.5635\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 133.8928 - mae: 8.2253 - val_loss: 59.0507 - val_mae: 6.0112\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 136.0338 - mae: 8.3616 - val_loss: 49.0655 - val_mae: 5.5350\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 134.7997 - mae: 8.2152 - val_loss: 47.6855 - val_mae: 5.4637\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 129.7247 - mae: 7.9880 - val_loss: 55.6078 - val_mae: 5.8426\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 131.2302 - mae: 8.1101 - val_loss: 54.6952 - val_mae: 5.7869\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 130.0842 - mae: 8.0969 - val_loss: 47.0476 - val_mae: 5.4171\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 129.6279 - mae: 8.0304 - val_loss: 53.3642 - val_mae: 5.7263\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 131.4892 - mae: 8.1297 - val_loss: 51.3504 - val_mae: 5.6577\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 132.2565 - mae: 8.2357 - val_loss: 47.0300 - val_mae: 5.4337\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 129.2479 - mae: 7.9345 - val_loss: 54.5130 - val_mae: 5.7640\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   9.0s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 30ms/step - loss: 11426.5947 - mae: 90.4282 - val_loss: 5484.6011 - val_mae: 60.6854\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 3689.5176 - mae: 50.7854 - val_loss: 2506.7478 - val_mae: 41.2313\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1704.2092 - mae: 34.4534 - val_loss: 1194.0872 - val_mae: 27.6597\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 816.4033 - mae: 23.4867 - val_loss: 581.2072 - val_mae: 19.1751\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 420.6663 - mae: 16.6182 - val_loss: 277.1396 - val_mae: 14.3954\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 261.8236 - mae: 13.1682 - val_loss: 155.5005 - val_mae: 10.3696\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 170.6475 - mae: 10.2422 - val_loss: 99.4250 - val_mae: 8.1365\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 139.8413 - mae: 8.9341 - val_loss: 88.4305 - val_mae: 7.2124\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 124.1160 - mae: 8.2300 - val_loss: 72.6848 - val_mae: 6.4287\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 121.1916 - mae: 8.2716 - val_loss: 56.5789 - val_mae: 5.8288\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 115.1937 - mae: 8.0150 - val_loss: 52.9098 - val_mae: 5.6401\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 114.4564 - mae: 7.9228 - val_loss: 50.8454 - val_mae: 5.5316\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 112.1456 - mae: 7.8945 - val_loss: 50.4690 - val_mae: 5.5059\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 115.6060 - mae: 8.0831 - val_loss: 47.2960 - val_mae: 5.3857\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 110.9533 - mae: 7.8906 - val_loss: 49.4142 - val_mae: 5.4367\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 111.3790 - mae: 7.9165 - val_loss: 40.2328 - val_mae: 5.1803\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 113.2513 - mae: 7.9115 - val_loss: 46.9039 - val_mae: 5.3344\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 113.9661 - mae: 8.0104 - val_loss: 42.6288 - val_mae: 5.1997\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 110.9982 - mae: 7.9372 - val_loss: 41.6329 - val_mae: 5.1749\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 113.9713 - mae: 8.0751 - val_loss: 45.0430 - val_mae: 5.2539\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 108.2139 - mae: 7.8673 - val_loss: 42.7690 - val_mae: 5.4806\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 117.8322 - mae: 8.1215 - val_loss: 38.0567 - val_mae: 5.0313\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 110.0801 - mae: 7.8481 - val_loss: 41.6350 - val_mae: 5.0985\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 111.0380 - mae: 7.9756 - val_loss: 43.5360 - val_mae: 5.1640\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 109.6320 - mae: 7.7527 - val_loss: 40.6175 - val_mae: 5.0778\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 115.2173 - mae: 8.0422 - val_loss: 48.0346 - val_mae: 5.3263\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 112.0937 - mae: 7.9106 - val_loss: 38.3377 - val_mae: 5.0518\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 117.0843 - mae: 8.3333 - val_loss: 38.6450 - val_mae: 5.0334\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 111.5386 - mae: 7.9530 - val_loss: 38.1403 - val_mae: 5.0163\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 108.5860 - mae: 7.7565 - val_loss: 45.1297 - val_mae: 5.2196\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 111.0415 - mae: 8.0757 - val_loss: 38.7227 - val_mae: 5.0898\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 114.1250 - mae: 8.0101 - val_loss: 52.5368 - val_mae: 5.5292\n",
      "Epoch 32: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   5.7s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 7439.6934 - mae: 59.7148 - val_loss: 807.4540 - val_mae: 23.6948\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1396.1224 - mae: 28.7016 - val_loss: 372.7760 - val_mae: 15.5685\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 680.9045 - mae: 20.4164 - val_loss: 186.9581 - val_mae: 11.4766\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 373.9305 - mae: 14.7878 - val_loss: 143.6359 - val_mae: 10.2394\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 243.4083 - mae: 12.1472 - val_loss: 122.7382 - val_mae: 9.1426\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 186.8910 - mae: 10.8181 - val_loss: 125.8804 - val_mae: 9.0834\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 166.8241 - mae: 10.1431 - val_loss: 147.0021 - val_mae: 9.8197\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 148.8837 - mae: 9.6159 - val_loss: 136.3970 - val_mae: 8.8051\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 145.4943 - mae: 9.1978 - val_loss: 143.8154 - val_mae: 9.4611\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 141.8091 - mae: 9.1368 - val_loss: 139.0262 - val_mae: 8.7280\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 140.1607 - mae: 8.9235 - val_loss: 143.9798 - val_mae: 9.3637\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 141.8916 - mae: 9.0224 - val_loss: 139.3365 - val_mae: 8.8705\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 137.5260 - mae: 8.6640 - val_loss: 143.0336 - val_mae: 9.2356\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 138.1390 - mae: 8.8160 - val_loss: 144.2845 - val_mae: 9.3129\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 135.6756 - mae: 8.7209 - val_loss: 138.7849 - val_mae: 8.8441\n",
      "Epoch 15: early stopping\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "[CV] END model__learning_rate=1e-06, model__momentum=0.1, model__n_hidden=0, model__n_neurons=125, model__optimizer=nesterov; total time=   1.8s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 28ms/step - loss: 398.5051 - mae: 14.5442 - val_loss: 31.1290 - val_mae: 4.5627\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 89.7510 - mae: 6.8090 - val_loss: 28.1176 - val_mae: 4.3905\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81.3126 - mae: 6.4216 - val_loss: 25.8948 - val_mae: 4.1013\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81.1840 - mae: 6.5029 - val_loss: 43.7763 - val_mae: 5.0934\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 82.2109 - mae: 6.5831 - val_loss: 23.7364 - val_mae: 3.8596\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 79.0773 - mae: 6.2534 - val_loss: 24.0397 - val_mae: 4.0123\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 77.4547 - mae: 6.1905 - val_loss: 22.1720 - val_mae: 3.8157\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 81.0665 - mae: 6.4280 - val_loss: 33.6487 - val_mae: 4.4629\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 72.4011 - mae: 6.0675 - val_loss: 39.6732 - val_mae: 4.9410\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71.7043 - mae: 6.0423 - val_loss: 28.6922 - val_mae: 4.1793\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 71.0279 - mae: 5.9897 - val_loss: 18.5567 - val_mae: 3.5590\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 72.3935 - mae: 5.9505 - val_loss: 22.5231 - val_mae: 3.9514\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 69.3759 - mae: 5.8287 - val_loss: 71.1130 - val_mae: 7.1268\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 80.6695 - mae: 6.6206 - val_loss: 15.9260 - val_mae: 3.3523\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 64.9273 - mae: 5.7348 - val_loss: 18.7203 - val_mae: 3.6850\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 67.6851 - mae: 5.8288 - val_loss: 15.9668 - val_mae: 3.3694\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 67.8631 - mae: 5.6067 - val_loss: 24.6417 - val_mae: 4.2329\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 68.0389 - mae: 5.7951 - val_loss: 15.5786 - val_mae: 3.3583\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 64.2471 - mae: 5.5599 - val_loss: 21.3870 - val_mae: 3.9592\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 65.8321 - mae: 5.7108 - val_loss: 39.2668 - val_mae: 5.2599\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 64.1923 - mae: 5.8185 - val_loss: 18.7063 - val_mae: 3.7207\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 68.8297 - mae: 5.8416 - val_loss: 19.1709 - val_mae: 3.3246\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 64.8415 - mae: 5.4721 - val_loss: 26.6846 - val_mae: 4.3961\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 63.6689 - mae: 5.6258 - val_loss: 19.3496 - val_mae: 3.7998\n",
      "Epoch 24: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   3.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 75ms/step - loss: 953.0062 - mae: 22.3825 - val_loss: 114.7660 - val_mae: 8.9880\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 135.7088 - mae: 8.6817 - val_loss: 27.9808 - val_mae: 4.4256\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 102.3378 - mae: 7.3411 - val_loss: 39.5484 - val_mae: 4.9924\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 94.2299 - mae: 7.1622 - val_loss: 30.8480 - val_mae: 4.4521\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 91.6822 - mae: 7.0754 - val_loss: 19.7084 - val_mae: 3.6688\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 96.1079 - mae: 7.1187 - val_loss: 16.9906 - val_mae: 3.3772\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 92.5271 - mae: 6.8382 - val_loss: 26.3209 - val_mae: 4.3394\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 103.8718 - mae: 7.2870 - val_loss: 30.9356 - val_mae: 4.7307\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 85.8257 - mae: 6.6420 - val_loss: 41.0049 - val_mae: 5.3952\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 86.3343 - mae: 6.8043 - val_loss: 62.0724 - val_mae: 6.8188\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 90.4039 - mae: 7.0879 - val_loss: 18.4511 - val_mae: 3.6005\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 86.4578 - mae: 6.5945 - val_loss: 47.6064 - val_mae: 5.9920\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 85.3735 - mae: 6.7145 - val_loss: 44.5262 - val_mae: 5.7001\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 90.7075 - mae: 7.0265 - val_loss: 21.2209 - val_mae: 3.9033\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 82.3689 - mae: 6.5522 - val_loss: 26.7601 - val_mae: 4.4331\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 82.3506 - mae: 6.5248 - val_loss: 22.7218 - val_mae: 4.0638\n",
      "Epoch 16: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   3.2s\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 32ms/step - loss: 1259.3644 - mae: 25.8502 - val_loss: 158.5182 - val_mae: 9.6228\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 102.7031 - mae: 7.3370 - val_loss: 71.7682 - val_mae: 5.7715\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 79.4721 - mae: 6.3758 - val_loss: 61.7594 - val_mae: 5.6400\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 65.3234 - mae: 5.6069 - val_loss: 49.8216 - val_mae: 5.1480\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 62.7692 - mae: 5.5370 - val_loss: 50.2450 - val_mae: 5.3478\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 64.4901 - mae: 5.7175 - val_loss: 44.6842 - val_mae: 4.8921\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 64.3050 - mae: 5.6675 - val_loss: 82.7177 - val_mae: 7.5673\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 64.3389 - mae: 5.8100 - val_loss: 44.2388 - val_mae: 4.9664\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 59.0099 - mae: 5.4093 - val_loss: 45.8690 - val_mae: 4.9957\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 59.8166 - mae: 5.5099 - val_loss: 42.6448 - val_mae: 4.9811\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 58.8428 - mae: 5.3732 - val_loss: 41.9613 - val_mae: 4.8887\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 62.1035 - mae: 5.5714 - val_loss: 79.8935 - val_mae: 7.3342\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 76.5011 - mae: 6.4272 - val_loss: 45.1425 - val_mae: 4.9660\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 66.2556 - mae: 5.7868 - val_loss: 44.9145 - val_mae: 4.9771\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 58.9902 - mae: 5.4457 - val_loss: 42.6612 - val_mae: 4.8532\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 60.9744 - mae: 5.3973 - val_loss: 45.7617 - val_mae: 5.0643\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 58.2422 - mae: 5.4919 - val_loss: 44.3540 - val_mae: 5.1216\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 60.2350 - mae: 5.5060 - val_loss: 41.3223 - val_mae: 4.8049\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 57.8268 - mae: 5.4369 - val_loss: 42.0275 - val_mae: 4.8960\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 57.7812 - mae: 5.3150 - val_loss: 42.4251 - val_mae: 4.9765\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 59.7361 - mae: 5.5054 - val_loss: 41.7494 - val_mae: 4.9356\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 56.9981 - mae: 5.2200 - val_loss: 41.4704 - val_mae: 4.8228\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 55.7229 - mae: 5.4062 - val_loss: 45.1923 - val_mae: 5.1609\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 59.7003 - mae: 5.2980 - val_loss: 66.0581 - val_mae: 6.5842\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 62.4972 - mae: 5.6569 - val_loss: 40.0336 - val_mae: 4.7876\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 56.6366 - mae: 5.3217 - val_loss: 51.0972 - val_mae: 5.5637\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 57.8390 - mae: 5.4130 - val_loss: 40.3407 - val_mae: 4.8343\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 59.7844 - mae: 5.5673 - val_loss: 40.2655 - val_mae: 4.8237\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 56.4475 - mae: 5.3074 - val_loss: 41.6201 - val_mae: 4.9891\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 57.3278 - mae: 5.3030 - val_loss: 51.1897 - val_mae: 5.5712\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 57.0309 - mae: 5.3143 - val_loss: 39.6545 - val_mae: 4.7771\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 56.2993 - mae: 5.2842 - val_loss: 47.5569 - val_mae: 5.3596\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 61.2671 - mae: 5.7308 - val_loss: 52.8503 - val_mae: 5.8705\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 61.4609 - mae: 5.5802 - val_loss: 44.3793 - val_mae: 5.1592\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 56.2683 - mae: 5.3681 - val_loss: 40.4694 - val_mae: 4.8404\n",
      "Epoch 35: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "[CV] END model__learning_rate=1e-05, model__momentum=0.9, model__n_hidden=3, model__n_neurons=25, model__optimizer=sgd; total time=   3.9s\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hania/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [            nan -1.92049806e+01  2.14922144e-01 -2.39302831e-01\n",
      "             nan  8.21757097e-02             nan             nan\n",
      " -3.65483113e+02             nan             nan  3.13113283e-01\n",
      " -3.31349732e+02             nan -6.47096265e+00  1.42239177e-01\n",
      "             nan  2.75028300e-03 -5.40451461e-01  2.14098850e-01]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 17ms/step - loss: 780.3152 - mae: 17.8700 - val_loss: 52.5324 - val_mae: 6.0175\n",
      "Epoch 2/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 87.6351 - mae: 6.8682 - val_loss: 43.6857 - val_mae: 5.2989\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 84.1127 - mae: 6.6473 - val_loss: 35.6155 - val_mae: 4.4513\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 79.1199 - mae: 6.3947 - val_loss: 33.1547 - val_mae: 4.1252\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 76.9004 - mae: 6.2778 - val_loss: 31.7412 - val_mae: 4.2325\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 74.3124 - mae: 6.2095 - val_loss: 34.4787 - val_mae: 4.5321\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 72.7393 - mae: 5.9828 - val_loss: 28.9629 - val_mae: 3.8538\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 72.2550 - mae: 6.1006 - val_loss: 27.3881 - val_mae: 3.7766\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 72.6225 - mae: 6.1269 - val_loss: 29.7682 - val_mae: 4.0593\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 71.5322 - mae: 5.9397 - val_loss: 26.0872 - val_mae: 3.6709\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 70.6447 - mae: 6.0504 - val_loss: 27.3529 - val_mae: 3.7573\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 73.8758 - mae: 6.1277 - val_loss: 45.7831 - val_mae: 5.4652\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 77.2022 - mae: 6.3738 - val_loss: 29.3629 - val_mae: 4.0969\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 69.8418 - mae: 5.9978 - val_loss: 28.5989 - val_mae: 4.0381\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 69.0847 - mae: 5.9649 - val_loss: 25.1322 - val_mae: 3.6873\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 68.4320 - mae: 5.8841 - val_loss: 24.2758 - val_mae: 3.5571\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 66.1821 - mae: 5.6235 - val_loss: 75.7583 - val_mae: 7.6202\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 73.0866 - mae: 6.3427 - val_loss: 25.3307 - val_mae: 3.6711\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 69.3441 - mae: 5.8134 - val_loss: 23.4348 - val_mae: 3.4447\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 66.5651 - mae: 5.8073 - val_loss: 23.0670 - val_mae: 3.4148\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 65.6792 - mae: 5.7568 - val_loss: 44.9817 - val_mae: 5.5352\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 73.6813 - mae: 6.1559 - val_loss: 27.7218 - val_mae: 4.0944\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 68.1809 - mae: 5.8826 - val_loss: 23.1052 - val_mae: 3.5091\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 67.9604 - mae: 5.8411 - val_loss: 23.3023 - val_mae: 3.5324\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 67.6279 - mae: 5.8274 - val_loss: 22.4582 - val_mae: 3.4212\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 63.5709 - mae: 5.5748 - val_loss: 22.0457 - val_mae: 3.3883\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 63.6044 - mae: 5.6656 - val_loss: 29.2824 - val_mae: 4.2059\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 66.5251 - mae: 5.5517 - val_loss: 23.9916 - val_mae: 3.6819\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 64.4092 - mae: 5.6970 - val_loss: 30.8727 - val_mae: 4.3752\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 64.2751 - mae: 5.5675 - val_loss: 24.7554 - val_mae: 3.8501\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 64.8692 - mae: 5.7239 - val_loss: 22.5340 - val_mae: 3.4966\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 61.6747 - mae: 5.3811 - val_loss: 21.0788 - val_mae: 3.3327\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 63.7927 - mae: 5.5919 - val_loss: 21.1501 - val_mae: 3.4022\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 63.7315 - mae: 5.5935 - val_loss: 24.5227 - val_mae: 3.8737\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 62.6423 - mae: 5.5492 - val_loss: 25.0048 - val_mae: 3.9430\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 64.9828 - mae: 5.8169 - val_loss: 20.4457 - val_mae: 3.3072\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 61.9375 - mae: 5.5531 - val_loss: 20.4988 - val_mae: 3.3399\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 61.9202 - mae: 5.5478 - val_loss: 24.2854 - val_mae: 3.7520\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 64.4539 - mae: 5.5437 - val_loss: 21.8639 - val_mae: 3.5636\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 62.3201 - mae: 5.6334 - val_loss: 28.9821 - val_mae: 4.2399\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 65.4163 - mae: 5.6716 - val_loss: 24.3069 - val_mae: 3.8482\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 61.5345 - mae: 5.6129 - val_loss: 20.7944 - val_mae: 3.4723\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 61.9311 - mae: 5.4958 - val_loss: 20.7524 - val_mae: 3.4416\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 61.6605 - mae: 5.6035 - val_loss: 23.2214 - val_mae: 3.6099\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 61.2488 - mae: 5.3853 - val_loss: 20.9028 - val_mae: 3.4483\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 60.1920 - mae: 5.5359 - val_loss: 20.6294 - val_mae: 3.4911\n",
      "Epoch 46: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=KerasRegressor(callbacks=[<keras.callbacks.EarlyStopping object at 0x7fb97d108c40>], model=<function build_model at 0x7fb9987dd310>),\n",
       "                   n_iter=20,\n",
       "                   param_distributions={'model__learning_rate': [1e-06, 1e-05,\n",
       "                                                                 0.0001],\n",
       "                                        'model__momentum': [0.1, 0.5, 0.9],\n",
       "                                        'model__n_hidden': [0, 1, 2, 3],\n",
       "                                        'model__n_neurons': [5, 25, 125],\n",
       "                                        'model__optimizer': ['sgd', 'nesterov',\n",
       "                                                             'momentum',\n",
       "                                                             'adam']},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=20, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c4891e9-6bc6-4caa-b471-d370622d978c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model__optimizer': 'momentum',\n",
       " 'model__n_neurons': 125,\n",
       " 'model__n_hidden': 1,\n",
       " 'model__momentum': 0.5,\n",
       " 'model__learning_rate': 1e-06}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbe11f3c-b121-44e1-8ae6-541b5b13dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnd_search.pkl', 'wb') as fp:\n",
    "    pickle.dump(rnd_search_cv.best_params_, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
